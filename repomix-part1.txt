This file is a merged representation of the entire codebase, combined into a single document by Repomix.
The content has been processed where comments have been removed, empty lines have been removed, content has been compressed (code blocks are separated by ⋮---- delimiter), security check has been disabled.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Code comments have been removed from supported file types
- Empty lines have been removed from all files
- Content has been compressed - code blocks are separated by ⋮---- delimiter
- Security check has been disabled - content may contain sensitive information
- Files are sorted by Git change count (files with more changes are at the bottom)

Additional Info:
----------------

================================================================
Directory Structure
================================================================
docs/
  comprehensive-logging-system/
    AI_MODEL_TRAINING.md
    API_REFERENCE.md
    COMPANY_CONTEXT_FEATURES_FIX.md
    COMPANY_MEMBERS_RECURSION_FIX.md
    COMPANY_MEMBERS_REMOVAL.md
    COMPLIANCE.md
    DATA_MODELING.md
    ENVIRONMENT_VARIABLES.md
    IMPLEMENTATION_GUIDE.md
    IMPLEMENTATION_PLAN.md
    INFINITE_RECURSION_FIX.md
    MODEL_TRAINING_GUIDE.md
    OVERVIEW.md
    PERMISSION_ISSUES_FIX.md
    README.md
    RECURSIVE_POLICY_FIX.md
    REQUIREMENTS.md
    SECURITY.md
    TECHNICAL_ARCHITECTURE.md
    USER_GUIDE.md
  enhanced-profile-system/
    IMPLEMENTATION_PLAN.md
    OVERVIEW.md
    README.md
    REQUIREMENTS.md
    TECHNICAL_ARCHITECTURE.md
    USER_STORIES.md
  examples/
    AdvancedVisualizationExample.tsx
    FeedbackSystemIntegration.tsx
    JourneyHooksExample.tsx
    StepAssistantIntegration.tsx
  huggingface-spaces-integration/
    API_KEY_SETUP_GUIDE.md
    IMPLEMENTATION_GUIDE.md
    README.md
    SETTINGS_FIX.md
    SETTINGS_TABLE_FIX.md
    SPACE_SETUP_GUIDE.md
    TECHNICAL_ARCHITECTURE.md
    TROUBLESHOOTING_ENDPOINTS.md
    TROUBLESHOOTING.md
    USER_GUIDE.md
  idea-playground/
    components/
      AI_SERVICE_LAYER.md
      DOMAIN_SERVICES.md
      PATHWAY1_INTEGRATION.md
      UI_INTEGRATION.md
    guides/
      CODE_STANDARDS.md
      TESTING_STRATEGY.md
    AI_FEATURES_FIX.md
    AI_STREAMING_IMPROVEMENTS.md
    ARCHITECTURE.md
    BACKEND_SERVICES.md
    CONTINUOUS_LEARNING_WITH_IP_PROTECTION.md
    DATA_MIGRATION.md
    DATABASE_SCHEMA.md
    DEPLOYMENT_AND_MONITORING.md
    ENHANCED_IDEA_HUB_FIX.md
    ENHANCED_IDEA_HUB_README.md
    ENHANCED_IDEA_HUB.md
    FRONTEND_COMPONENTS.md
    IDEA_GENERATOR_FIX.md
    IMPLEMENTATION_PLAN.md
    IMPLEMENTATION_SUMMARY.md
    IP_PROTECTION.md
    MODULAR_ARCHITECTURE.md
    MODULAR_IMPLEMENTATION_GUIDE.md
    MODULAR_IMPLEMENTATION_SUMMARY.md
    MODULAR_IMPLEMENTATION.md
    MODULAR_STRUCTURE.md
    OVERVIEW.md
    OWNERSHIP_FILTERING.md
    OWNERSHIP_MODEL_IMPLEMENTATION.md
    README.md
    REAL_AI_ONLY_MODE.md
    REBUILD_OVERVIEW.md
    REDIRECT_FIX.md
    REQUIREMENTS.md
    TRAINING_PIPELINE.md
    USER_STORIES.md
  idea-playground-pathway1/
    AI_INTEGRATION.md
    DATA_MODELING.md
    IMPLEMENTATION_PLAN.md
    JSON_PARSING_FIX.md
    LINTING_FIXES.md
    README.md
    REQUIREMENTS.md
    TECHNICAL_ARCHITECTURE.md
    TESTING_GUIDE.md
    USER_GUIDE.md
  multi-persona-profile/
    CONTINUE_SETUP_FIX.md
    CURRENT_STATUS.md
    IMPLEMENTATION_FIX.md
    MIGRATION_FIX.md
    ONBOARDING_COMPLETION_FIX.md
    ONBOARDING_FIX.md
    ONBOARDING_PROGRESS_FIX.md
    PERSONA_NAME_FIX.md
    README_FIX.md
  onboarding-wizard/
    BLINKING_FIX.md
    INITIAL_ONBOARDING.md
    REQUIREMENTS.md
    TECHNICAL_ARCHITECTURE.md
    TESTING_GUIDE.md
  AI_IMPLEMENTATION.md
  AI_SERVICES_FIX.md
  COMPANY_JOURNEY_PAGE_INTERACTIVE_AI.md
  COMPREHENSIVE_DOCUMENTATION.md
  CORE_FEATURES.md
  DASHBOARD_LOADING_FIX_DETAILS.md
  DASHBOARD_LOADING_FIX.md
  DISABLED_LOGGING_DOCUMENTATION.md
  huggingface-integration.md
  JOURNEY_EXPERIENCE_REDESIGN.md
  JOURNEY_REDESIGN_COMPONENTS_SUMMARY.md
  JOURNEY_REDESIGN_IMPLEMENTATION_PLAN.md
  JOURNEY_REDESIGN_SPRINT_PLAN.md
  JOURNEY_STEP_PAGE_IMPLEMENTATION.md
  JOURNEY_STEPS_IMPLEMENTATION_STATUS.md
  JOURNEY_STEPS_UX_IMPROVEMENTS.md
  JOURNEY_STEPS_UX_SPRINT_PLAN.md
  JOURNEY_SYSTEM_MIGRATION_GUIDE.md
  JOURNEY_SYSTEM_REDESIGN_STATUS.md
  JOURNEY_SYSTEM_SPRINT1_DATA_FOUNDATION.md
  JOURNEY_SYSTEM_SPRINT1_IMPLEMENTATION_SUMMARY.md
  JOURNEY_SYSTEM_SPRINT1_SUMMARY.md
  JOURNEY_SYSTEM_SPRINT1_WEEK2_PLAN.md
  JOURNEY_SYSTEM_SPRINT2_PLAN.md
  JOURNEY_SYSTEM_SPRINT3_PLAN.md
  JOURNEY_SYSTEM_UNIFIED_REDESIGN.md
  JOURNEY_UI_SPRINT3_IMPLEMENTATION_PREVIEW.md
  JOURNEY_UI_SPRINT3_IMPLEMENTATION_STATUS.md
  JOURNEY_UI_SPRINT3_IMPLEMENTATION.md
  JOURNEY_UI_SPRINT3_MOCKUPS.md
  JOURNEY_UI_SPRINT3_PLAN.md
  JOURNEY_UI_SPRINT4_COMPLETION.md
  JOURNEY_UI_SPRINT4_PLAN.md
  JOURNEY_UI_SPRINT5_PLAN.md
  JOURNEY_UI_UX_REFRESH_SUMMARY.md
  JOURNEY_UX_REDESIGN_CONCEPTS.md
  JOURNEY_UX_REFRESH_IMPLEMENTATION.md
  MULTI_TENANT_TERMINOLOGY_SYSTEM.md
  recommendation-system-guide.md
  ROADMAP.md
  STANDUP_BOT_CONVERSATION.md
  TECHNICAL_ARCHITECTURE.md
  TERMINOLOGY_SERVICE_DIRECT_APPROACH.md
  TERMINOLOGY_SPRINT3_SUMMARY.md
  THE_WHEEL_ADVANCED_SIMULATION.md
  THE_WHEEL_AI_AGENT_ECOSYSTEM.md
  THE_WHEEL_API_DOCUMENTATION.md
  THE_WHEEL_COMMUNITY_INFRASTRUCTURE.md
  THE_WHEEL_DATA_MODEL.md
  THE_WHEEL_DYNAMIC_PROGRESS_TRACKER.md
  THE_WHEEL_EXECUTIVE_SUMMARY.md
  THE_WHEEL_FUTURE_FEATURES.md
  THE_WHEEL_IMPLEMENTATION_GUIDE.md
  THE_WHEEL_IMPLEMENTATION_PLAN.md
  THE_WHEEL_IMPLEMENTATION_ROADMAP.md
  THE_WHEEL_JOURNEY_USER_GUIDE.md
  THE_WHEEL_KNOWLEDGE_HUB.md
  THE_WHEEL_MARKETPLACE_INTEGRATION.md
  THE_WHEEL_MIGRATION_STRATEGY.md
  THE_WHEEL_PROGRESS_ANALYTICS.md
  THE_WHEEL_REQUIREMENTS_ANALYSIS.md
  THE_WHEEL_ROADMAP.md
  THE_WHEEL_SECURITY_ARCHITECTURE.md
  THE_WHEEL_USER_FLOWS.md
  THE_WHEEL_USER_STORIES.md
  THE_WHEEL_WIREFRAMES.md
  TOOL_SELECTION_AND_EVALUATION_FLOW.md
  TOOL_SELECTION_API_DOCUMENTATION.md
  TOOL_SELECTION_EVALUATION_ADVANCED.md
  USER_STORIES.md
public/
  force-logout.html
  test-disabled-services.html
  test.html
scripts/
  activate-all-features.js
  add-huggingface-auth-token.cjs
  analyze-excel.cjs
  apply-json-parsing-fix.js
  apply-logging-fix.js
  archive-huggingface-integration.cjs
  archive-huggingface-spaces.cjs
  archive-huggingface-spaces.js
  check-and-set-feature-flags.mjs
  check-huggingface-db-settings.js
  check-huggingface-space-direct.cjs
  check-huggingface-space-status.js
  check-migration.cjs
  check-null-handling.js
  debug-idea-pathway-ai.js
  diagnose-dashboard-issues.ts
  diagnose-huggingface-key.js
  direct-schema-check.js
  enable-mock-auth.js
  enable-real-ai.js
  enhance-tool-data.js
  fix-ai-feature-flags.mjs
  fix-json-parsing-issues.js.backup
  force-logout-all-users.js
  generate-hierarchical-test-data.js
  generate-tool-pathways.js
  generate-tool-relationships.js
  import-journey-excel.cjs
  init-profile-sections.js
  initialize-feature-flags.js
  install-supabase-deps.sh
  key-validator.js
  make-user-admin.ts
  mark-onboarding-complete.js
  method-fixes-log.md
  migrate-journey-data.cjs
  migrate-journey-data.ts
  migrate-journey-tools.cjs
  migrate-to-modular-idea-playground.js
  migrate-tools-correctly.cjs
  migrate-tools-direct.cjs
  migrate-tools-final.cjs
  migrate-tools-one-by-one.cjs
  migrate.js
  README-HUGGINGFACE-TEST.md
  reset-llm-service.js
  run-company-context-and-features-fix.js
  run-company-members-fix.js
  run-comprehensive-logging-migration.js
  run-consolidated-migration.js
  run-dashboard-diagnostics.js
  run-enhanced-idea-hub-fix.js
  run-enhanced-idea-hub-migration.js
  run-enhanced-profile-migration.js
  run-fix-system-logs-recursion.js
  run-full-typecheck.js
  run-huggingface-fix.cjs
  run-huggingface-fix.js
  run-huggingface-settings-fix.js
  run-idea-ownership-migration.js
  run-idea-pathway1-complete-fix.js
  run-idea-playground-pathway1-migration.js
  run-journey-step-migration.js
  run-journey-transformation.js
  run-logging-test.js
  run-model-feedback-update.js
  run-profile-permissions-fix.js
  run-recommendation-system-migration.js
  run-service-role-api-migration.js
  seed-alternative-terminology-sets.js
  seed-terminology-defaults.js
  seed-tool-recommendations.js
  simple-recursion-verify.js
  summary.js
  supabase-client.cjs
  test-company-context-features-fix.js
  test-dashboard-load.js
  test-disabled-logging.js
  test-huggingface-api-key-direct.js
  test-huggingface-api-key.js
  test-huggingface-integration.js
  test-huggingface-spaces-connection.cjs
  test-huggingface-spaces-connection.js
  test-huggingface-spaces-integration.js
  test-huggingface-spaces.js
  test-idea-ownership-filter.js
  test-idea-pathway1-ai.js
  test-idea-pathway1-fixed.js
  test-idea-pathway1-fixes-final.js
  test-idea-pathway1-fixes.js
  test-idea-pathway1-json-fix.js
  test-idea-playground-pathway1-fixed.js
  test-idea-playground-pathway1.js
  test-idea-playground-redirect.js
  test-idea-refinement.js
  test-initial-onboarding.js
  test-journey-migration.js
  test-journey-steps-service.js
  test-json-parsing-fix.js
  test-json-parsing.js
  test-logging-permissions-fix.js
  test-logging-system.js
  test-modular-idea-playground.js
  test-onboarding-flow.js
  test-profile-completion.js
  test-progress-analytics.js
  test-real-ai-mode.js
  test-service-role-api-function-fixed.js
  test-service-role-api-function-modified.js
  test-service-role-api-function.js
  test-standup-bot-fix.js
  test-standup-bot.js
  test-standup-conversation.js
  test-standup-fix.js
  test-step-card-components.js
  test-streaming-suggestions.js
  test-system-logs-fix.js
  test-terminology-service.js
  test-tool-selection-service.js
  toggle-ai-mode.js
  type-check-json-parsing-fix.js
  typecheck-summary.md
  verify-huggingface-model-access.js
  verify-idea-pathway1-fix.js
  verify-recursion-and-features-fix.js
src/
  __tests__/
    marketAnalysis.test.ts
  components/
    admin/
      AppCredentialsSettings.tsx
      ExcelImportMapper.tsx
      FeatureFlagsSettings.tsx
      HuggingFaceSettings.tsx
      ModelManagementPanel.tsx
      OpenAISettings.tsx
      TerminologyManagement.tsx
      UserManagement.tsx
    common/
      Toast.tsx
      Tooltip.tsx
    company/
      dashboard/
        FinancialSnapshotWidget.tsx
        JourneyMapView.tsx
        JourneyProgressWidget.tsx
        MyTasksWidget.tsx
      journey/
        Analytics/
          index.ts
          JourneyAnalyticsDashboard.tsx
        ChallengeCard/
          ChallengeCard.tsx
          DifficultyIndicator.tsx
          EstimatedTime.tsx
          index.ts
          StatusBadge.tsx
        ChallengeEditor/
          ChallengeEditor.tsx
          index.ts
        ChallengeList/
          ChallengeList.tsx
          index.ts
        ListView/
          index.ts
          ListView.tsx
        PhaseProgress/
          SimplePhaseProgress/
            index.ts
          SimplePhaseProgressList/
            index.ts
          index.ts
          PhaseProgress.tsx
          SimplePhaseProgress.tsx
          SimplePhaseProgressList.tsx
        StepAssistant/
          index.ts
          StepAssistant.tsx
        StepCard/
          DifficultyIndicator.tsx
          DraggableStepCard.tsx
          EstimatedTime.tsx
          index.ts
          StatusBadge.tsx
          StepCard.tsx
          StepCardProps.ts
        StepDetails/
          ActionSection.tsx
          AdviceCard.tsx
          ChecklistSection.tsx
          DescriptionSection.tsx
          FeedbackSection.tsx
          GuidanceSection.tsx
          NotesSection.tsx
          OptionsSection.tsx
          ResourcesSection.tsx
          TipsSection.tsx
        StepList/
          DraggableStepList.tsx
          index.ts
          StepList.tsx
        StepRecommendations/
          index.ts
          NextBestSteps.tsx
          RecommendationsPanel.tsx
          StepRelationshipMap.tsx
        TimelineView/
          index.ts
          TimelineView.tsx
        ToolSelector/
          CustomToolForm.tsx
          DocumentUploader.tsx
          EvaluationHistory.tsx
          ScorecardBuilder.tsx
          ToolComparisonTable.tsx
          ToolDetailsModal.tsx
          ToolEvaluationForm.tsx
          ToolList.tsx
          ToolRecommendationList.tsx
          ToolSelector.tsx
        ViewToggle/
          index.ts
          ViewToggle.tsx
        JourneyOverview.tsx
        JourneyStepDetails.tsx
      CompanyStages.tsx
      DocumentStore.tsx
    feedback/
      index.ts
      InlineRatingComponent.tsx
      StepImprovementSuggestionForm.tsx
    idea-exploration/
      IdeaCard.tsx
      IdeaComparison.tsx
      IdeaDetail.tsx
      IdeaExplorer.tsx
      IdeaMerger.tsx
    idea-playground/
      enhanced/
        components/
          stages/
            BusinessModelStage.tsx
            CompanyFormationStage.tsx
            DetailedRefinementStage.tsx
            GoToMarketStage.tsx
            IdeaGenerationStage.tsx
            InitialAssessmentStage.tsx
            MarketValidationStage.tsx
          Dashboard.tsx
          EnhancedWorkspace.tsx
          NavigationSidebar.tsx
        context/
          IdeaPlaygroundContext.tsx
        services/
          ai-service.factory.ts
          ai-service.interface.ts
          mock-ai.service.ts
          multi-tiered-ai.service.ts
        state/
          idea-workflow.machine.ts
      landing/
        LandingPage.tsx
      pathway/
        IdeaPathwayWorkflow.tsx
        IdeaVariationCard.tsx
        IdeaVariationList.tsx
        MergedIdeaCard.tsx
        MergedIdeaList.tsx
        PathwayNavigation.tsx
        PathwayStepIndicator.tsx
      pathway1/
        BusinessModelScreen.tsx
        GoToMarketScreen.tsx
        IdeaCaptureScreen.tsx
        ProblemSolutionScreen.tsx
        SuggestionCard.tsx
        SuggestionEditor.tsx
        SuggestionEditor.tsx.backup
        SuggestionMerger.tsx
        SuggestionMerger.tsx.backup
        SuggestionsScreen.tsx
        SuggestionsScreen.tsx.backup
        TargetValueScreen.tsx
      pathway2/
        IdeaComparisonScreen.tsx
        IdeaRefinementScreen.tsx
        IndustrySelectionScreen.tsx
      pathway3/
        IdeaAnalysisScreen.tsx
        IdeaLibraryScreen.tsx
        IdeaRefinementScreen.tsx
      shared/
        ContextualAIPanel.tsx
        ExternalToolsIntegration.tsx
        IdeaDashboard.tsx
        IdeaExportModal.tsx
        OnboardingContent.tsx
        OnboardingTutorial.tsx
        SmartSuggestionButton.tsx
        TeamCollaboration.tsx
      CanvasSelector.tsx
      CreateCanvasModal.tsx
      IdeaGenerationForm.tsx
      IdeaList.tsx
      IdeaPlaygroundWorkspace.tsx
      IdeaPlaygroundWorkspaceWithPathway.tsx
      IdeaRefinementForm.tsx
      SavedIdeasList.tsx
      SaveIdeaModal.tsx
    idea-refinement/
      BasicIdeaInfo.tsx
      BusinessModelGenerator.tsx
      ComponentVariations.tsx
      ConceptVariations.tsx
      DetailedRefinement.tsx
      IdeaRefinementWorkflow.tsx
      StepIndicator.tsx
      StepNavigation.tsx
    onboarding/
      steps/
        CompanyStageStep.tsx
        EnhancedCompanyStageStep.tsx
        EnhancedInviteCodeStep.tsx
        EnhancedRoleSelectionStep.tsx
        EnhancedServiceCategoriesStep.tsx
        FeatureRecommendations.tsx
        FounderCompanyStageStep.tsx
        GoalsSelectionStep.tsx
        IndustrySelectionStep.tsx
        InitialRoleStep.tsx
        JoinCompanyStep.tsx
        NotificationPreferencesStep.tsx
        OnboardingCompletion.tsx
        OnboardingWelcome.tsx
        RoleSelectionStep.tsx
        ServiceProviderCategoriesStep.tsx
        SkillLevelStep.tsx
        ThemePreferencesStep.tsx
      EnhancedOnboardingWizard.tsx
      InitialOnboardingWizard.tsx
      OnboardingController.tsx
      OnboardingProgress.tsx
      OnboardingWizard.tsx
      ProgressTrackingDemo.tsx
      SimpleProgressBar.tsx
    profile/
      CreatePersonaForm.tsx
      PersonaSelector.tsx
      ProfileSetupDemo.tsx
    settings/
      LLMProviderSettings.tsx
    shared/
      idea/
        BaseIdeaCard.tsx
        BaseSuggestionCard.tsx
        IdeaCard.tsx
        SuggestionCard.tsx
      ui/
        forms/
          AIAssistedInput.tsx
          AIAssistedTextArea.tsx
          BaseAIAssistedInput.tsx
        README.md
      README.md
    tasks/
      AITaskGenerator.tsx
      CreateTaskDialog.tsx
      ManualTaskCreation.tsx
      TaskCreation.tsx
      TaskForm.tsx
      TaskItem.tsx
      TaskList.tsx
      TaskManager.tsx
    terminology/
      DynamicText.tsx
      index.ts
      SimpleTerminologyEditor.tsx
      Term.tsx
      TerminologyEditor.tsx
      TerminologyProvider.tsx
      TerminologyShowcase.tsx
    ui/
      forms/
        AIAssistedInput.tsx
        AIAssistedTextArea.tsx
      alert.tsx
      button.tsx
      card.tsx
      input.tsx
      label.tsx
      OnboardingWizard.tsx
      progress.tsx
      select.tsx
      SignOutButton.tsx
      textarea.tsx
    unified-idea/
      IdeaComparisonPanel.tsx
      IdeaExplorationPanel.tsx
      IdeaMergePanel.tsx
      IdeaRefinementPanel.tsx
      UnifiedBasicIdeaInfo.tsx
      UnifiedBusinessModelGenerator.tsx
      UnifiedComponentVariations.tsx
      UnifiedConceptVariations.tsx
      UnifiedDetailedRefinement.tsx
      UnifiedIdeaWorkspace.tsx
      UnifiedStepIndicator.tsx
      UnifiedStepNavigation.tsx
      WorkspaceHeader.tsx
    visualization/
      index.ts
      InteractiveJourneyMap.tsx
      MilestoneCelebrationAnimation.tsx
    BusinessGenerator.tsx
    CloudStorageSettings.tsx
    CloudStorageSetup.tsx
    CreateCommunityModal.tsx
    DragDropProvider.tsx
    ErrorBoundary.tsx
    ExportToGoogleSlides.tsx
    IdeaComponentVariations.tsx
    IdeaRefinement.tsx
    JoinCompanyDialog.tsx
    Layout.tsx
    LLMProviderTest.tsx
    LoggingProvider.tsx
    MarketAnalysis.tsx
    MarketSuggestions.tsx
    MarketValidationQuestions.tsx
    OnboardingProgressCard.tsx
    StandupHistory.tsx
    TaskCreation.tsx
    TaskItem.tsx
    TaskList.tsx
    TaskManager.tsx
    TaskPromptDialog.tsx
    TeamManagement.tsx
  enhanced-idea-hub/
    components/
      common/
        IdeaCard.tsx
      views/
        CardGridView.tsx
      ViewManager.tsx
    services/
      api/
        idea-hub-api.ts
      supabaseClient.ts
    store/
      idea-hub-store.ts
    types/
      index.ts
  examples/
    OnboardingWizardExamples.tsx
  lib/
    contexts/
      IdeaContext.tsx
      IdeaPlaygroundContext.tsx
      UnifiedIdeaContext.tsx
    errors/
      journey-errors.ts
    hooks/
      index.ts
      useAuth.ts
      useCanvas.ts
      useCentralizedLogging.ts
      useCompany.ts
      useCompanyJourney.ts
      useEnhancedLogging.ts
      useIdeaGeneration.ts
      useIdeaRefinement.ts
      useJourneyPageData.ts
      useJourneySteps.ts
      useJourneyTools.ts
      useLogging.ts
      usePersona.ts
      useRecommendationAnalytics.ts
      useStepProgress.ts
      useTasks.ts
    services/
      ai/
        shared/
          BaseAIContextProvider.tsx
          README.md
        ai-context-provider.tsx
        ai-context.provider.tsx
        standup-context-provider.tsx
      idea-playground/
        ai/
          idea-merger.service.ts
          index.ts
          sequential-generation.service.ts
        llm/
          adapters/
            interface.ts
            openai.adapter.ts
          context/
            abstraction-context.provider.ts
            base-context.provider.ts
            company-context.provider.ts
            context-manager.ts
            interface.ts
          pathway1/
            ai.service.ts
          orchestrator.ts
        utils/
          response-parsing.ts
        canvas.service.ts
        component.service.ts
        feedback.service.ts
        idea-generation.service.ts
        idea-management.service.adapter.ts
        idea-management.service.ts
        index.ts
        pathway1-adapter.ts
        refinement.service.ts
        service-adapter.ts
        type-compatibility.ts
      ai.service.ts
      app-settings.service.ts
      appSettings.service.ts
      askWheel.service.ts
      auth.service.ts
      cache.service.ts
      company-access.service.ts
      company-data.service.ts
      company-model.service.ts
      company.service.ts
      companyJourney.service.ts
      companyToolEvaluation.service.ts
      companyTools.service.ts
      conversation-memory.service.ts
      enhanced-onboarding.service.ts
      enhanced-profile.service.ts
      feature-activator.ts
      feature-flags.service.ts
      feedback.service.ts
      financialAnalytics.service.ts
      financialHub.service.ts
      general-llm.service.ts
      huggingface-llm.service.ts
      idea-exploration.service.ts
      idea-generation.service.ts
      idea-memory.service.ts
      idea-pathway1-ai.service.ts
      idea-pathway1-ai.service.ts.backup
      idea-pathway1-ai.service.ts.backup2
      idea-playground-pathway.service.ts
      idea-playground-service-extension.ts
      idea-playground.service.extended.ts
      idea-playground.service.facade.ts
      idea-playground.service.ts
      journey-unified.service.ts
      journeyAnalytics.service.ts
      journeyBoard.service.ts
      journeyChallenges.service.ts
      journeyContent.service.ts
      journeySteps.service.ts
      localStorage-cleaner.ts
      logging.service.enhanced.ts
      logging.service.ts
      mock-auth.service.ts
      mock-general-llm.service.ts
      mock-idea-playground.service.ts
      mock-profile.service.ts
      model-abstraction.service.ts
      model-manager.service.ts
      model-training.service.ts
      multi-persona-profile.service.ts
      onboarding.service.ts
      privacy.service.ts
      profile.service.ts
      recommendation.service.ts
      settings.ts
      standup-ai.service.ts
      task.service.ts
      terminology.service.ts
      toolSelection.service.ts
      toolSubmission.service.ts
      unified-idea.service.ts
    types/
      ai.types.ts
      enhanced-profile.types.ts
      extended-profile.types.ts
      idea-exploration.types.ts
      idea-generation.types.ts
      idea-pathway.types.ts
      idea-playground.types.ts
      journey-challenges.types.ts
      journey-steps.types.ts
      journey-unified.types.ts
      journey.types.ts
      logging.types.ts
      multi-persona-profile.types.ts
      profile.types.ts
      task.types.ts
      terminology.types.ts
      unified-idea.types.ts
    utils/
      journey-validators.ts
      terminology-utils.ts
      time-utils.ts
    cloud-storage.ts
    database.types.ts
    google.ts
    huggingface-client.ts
    openai-client.ts
    openai.ts
    slides.ts
    store.ts
    supabase.ts
    supabaseClient.ts
    utils.ts
  pages/
    admin/
      AskWheelRequestsPage.tsx
    auth/
      GoogleCallback.tsx
      MicrosoftCallback.tsx
    community/
      CommunityPage.tsx
      NewPost.tsx
      Post.tsx
    company/
      JourneyBoard/
        AIRecommendationPanel.tsx
        FilterBar.tsx
        JourneyBoard.tsx
        ListView.tsx
        PhaseColumn.tsx
        StepCard.tsx
      CompanyBudgetPage.tsx
      CompanyDashboard.tsx
      CompanySettings.tsx
      CompanySetup.tsx
      CompanyToolEvaluationPage.tsx
      CompanyToolsPage.tsx
      JourneyChallengeDetailPage.tsx
      JourneyChallengesPage.tsx
      JourneyMapPage.tsx
      JourneyOverviewPage.tsx
      JourneyPage.tsx
      JourneyStepPage.tsx
      JourneyStepsPage.tsx
      JourneyStepsRedirect.tsx
    idea-hub/
      AIDiscussion.tsx
      BusinessGenerator.tsx
      BusinessModel.tsx
      CofounderBot.tsx
      EnhancedIdeaHub.tsx
      ExplorationHub.tsx
      IdeaCanvas.tsx
      IdeaComparisonPage.tsx
      IdeaDetailPage.tsx
      IdeaFlow.tsx
      IdeaMergerPage.tsx
      IdeaRefinement.tsx
      MarketResearch.tsx
      MarketValidation.tsx
      PitchDeck.tsx
      QuickGeneration.tsx
      Refinement.tsx
      ResourceLibrary.tsx
      SavedIdeasPage.tsx
      TestComponent.tsx
      UnifiedWorkflow.tsx
    idea-playground/
      IdeaPlaygroundPage.tsx
      IdeaPlaygroundPageWrapper.tsx
      PathwayRouter.tsx
    profile/
      PersonaManagementPage.tsx
    AdminAppSettingsPage.tsx
    AdminJourneyContentPage.tsx
    AdminPanel.tsx
    AdminToolModerationPage.tsx
    Community.tsx
    Dashboard.tsx
    Directory.tsx
    EnhancedIdeaHubPage.tsx
    EnhancedOnboardingPage.tsx
    FinancialHubPage.tsx
    IdeaHub.tsx
    InitialOnboardingPage.tsx
    Login.tsx
    Messages.tsx
    OnboardingDemoPage.tsx
    OnboardingPage.tsx
    OnboardingWizardPage.tsx
    Profile.tsx
    ProfileSetup.tsx
    SettingsPage.tsx
    SimplifiedDashboard.tsx
    StandupTestPage.tsx
    StandupTestPageWrapper.tsx
    TerminologyDemoPage.tsx
    TestPage.tsx
    ToolsMarketplacePage.tsx
  tests/
    journey-unified-tools.test.ts
    journey-unified.service.test.ts
  types/
    global.d.ts
  ui/
    components/
      Dashboard/
        DashboardLayout.tsx
      DragDrop/
        DraggableList.tsx
      Navigation/
        Sidebar.tsx
      Card.tsx
    styles/
      components.css
    design-system.ts
  App.tsx
  index.css
  Layout.tsx
  main.tsx
  setupTests.ts
  test.tsx
  vite-env.d.ts
supabase/
  migrations/
    20250316131100_multi_persona_profile_system.sql
    20250316145000_enhanced_profile_completion.sql
    20250429_add_multi_tenant_terminology_system.sql
    20250430_create_test_hierarchy.sql
    20250430_seed_terminology_defaults.sql
    20250430_seed_terminology_templates.sql
    20250501000000_create_step_enhanced_views.sql
    20250501000000_transform_journey_steps_to_challenges.sql
    20250502000000_add_recommendation_functions.sql
    20250503000000_add_feedback_system_tables.sql
    20250504000000_sprint5_journey_analytics_foundation.sql
    20250504000001_fix_sprint5_analytics_foundation.sql
    20250505000000_journey_system_unification.sql
    20250505010000_fix_tool_migration.sql
    20250505020000_fix_tool_migration_schema.sql
    20250505030000_expand_tools_table.sql
    20250505040000_add_compatibility_layer.sql
  sql/
    20250430_complete_recommendation_system_migration.sql
    20250430_query_recommendation_functions.sql
    20250430_seed_tool_recommendations.sql
  schema.sql
.gitignore
CLOUD-INTEGRATION-GUIDE.md
components.json
eslint.config.js
index.html
jest.config.js
journey_map_implementation_plan.md
journey_map_mvp_status.md
journey_map_task_integration_plan.md
journey_map_track_manually_implementation.md
LICENSE
mvp-feature-review.md
netlify.toml
package.json
postcss.config.js
pull_request_template_track_manually.md
README-HUGGINGFACE-REMOVAL.md
README-HUGGINGFACE-SPACES-FIX.md
README-HUGGINGFACE-SPACES-REMOVAL.md
README-ONBOARDING-FIX.md
README.md
steps1.csv
tailwind.config.js
the_wheel_adaptation_plan.md
the_wheel_data_migration_plan.md
the_wheel_design_system_guidelines.md
the_wheel_implementation_master_plan.md
the_wheel_mvp_implementation_plan.md
the_wheel_mvp_implementation_summary.md
tsconfig.app.json
tsconfig.json
tsconfig.node.json
update-ai-components.sh
update-components.sh
vite.config.ts

================================================================
Files
================================================================

================
File: docs/comprehensive-logging-system/AI_MODEL_TRAINING.md
================
# AI Model Training with Logging Data

## Overview

This document provides detailed guidance and examples of how to use the data collected by the Comprehensive Logging System to train AI models. It expands on the conceptual framework defined in the [Data Modeling](./DATA_MODELING.md) document by providing concrete implementation examples, processing pipelines, and best practices.

## Table of Contents

1. [Data Processing Pipeline](#data-processing-pipeline)
2. [Feature Engineering Examples](#feature-engineering-examples)
3. [Model Training Workflows](#model-training-workflows)
4. [Evaluation and Validation](#evaluation-and-validation)
5. [Deployment and Feedback Loop](#deployment-and-feedback-loop)
6. [Case Studies](#case-studies)
7. [Best Practices](#best-practices)

## Data Processing Pipeline

### Raw Data Extraction

The first step is extracting relevant data from the logging system:

```typescript
// src/lib/training/data-extraction.ts
import { supabase } from '../supabase';
import { privacyService } from '../services/privacy.service';

export async function extractTrainingData(
  startDate: Date,
  endDate: Date,
  consentFilter = true
): Promise<any[]> {
  // Only include users who have consented to AI training
  let query = supabase
    .from('system_logs')
    .select(`
      id,
      user_id,
      persona_id,
      company_id,
      event_type,
      event_source,
      action,
      data,
      metadata,
      created_at
    `)
    .gte('created_at', startDate.toISOString())
    .lte('created_at', endDate.toISOString())
    // Filter out sensitive data classifications
    .not('data_classification', 'eq', 'sensitive');
    
  if (consentFilter) {
    // Join with user_consent to check AI training consent
    query = query.in(
      'user_id', 
      supabase
        .from('user_consent')
        .select('user_id')
        .eq('ai_training', true)
    );
  }
  
  const { data, error } = await query;
  
  if (error) {
    throw new Error(`Failed to extract training data: ${error.message}`);
  }
  
  // Apply additional privacy transformations
  const transformedData = [];
  for (const log of data) {
    // Convert personal data to pseudonymized form
    if (log.data_classification === 'personal') {
      const anonymizedData = await privacyService.anonymizeData(
        log.data,
        'pseudonymize'
      );
      log.data = anonymizedData;
    }
    
    transformedData.push(log);
  }
  
  return transformedData;
}
```

### Data Transformation

Next, we transform raw logs into a format suitable for model training:

```typescript
// src/lib/training/data-transformation.ts
import { LogEvent } from '../types/logging.types';

// Transform logs into feature vectors
export function transformLogsToFeatures(logs: LogEvent[]): any[] {
  // Group logs by user
  const userLogs = groupBy(logs, 'user_id');
  
  return Object.entries(userLogs).map(([userId, userEvents]) => {
    // Sort by timestamp
    const sortedEvents = userEvents.sort(
      (a, b) => new Date(a.created_at).getTime() - new Date(b.created_at).getTime()
    );
    
    // Extract user features
    return {
      user_id: userId,
      
      // Activity patterns
      event_count: userEvents.length,
      active_days: countUniqueDays(userEvents),
      last_active: sortedEvents[sortedEvents.length - 1].created_at,
      first_active: sortedEvents[0].created_at,
      
      // Engagement features
      feature_usage: extractFeatureUsage(userEvents),
      workflow_completions: extractWorkflowCompletions(userEvents),
      session_duration: calculateAverageSessionDuration(userEvents),
      
      // Behavioral features
      preferred_time: extractPreferredTimeOfDay(userEvents),
      response_patterns: extractResponsePatterns(userEvents),
      
      // Content features
      content_interactions: extractContentInteractions(userEvents),
      
      // Company context (if applicable)
      company_id: userEvents[0].company_id || null,
      company_features: extractCompanyFeatures(userEvents)
    };
  });
}

// Helper functions for feature extraction
function groupBy(array: any[], key: string): Record<string, any[]> {
  return array.reduce((result, item) => {
    (result[item[key]] = result[item[key]] || []).push(item);
    return result;
  }, {});
}

function countUniqueDays(events: LogEvent[]): number {
  const uniqueDays = new Set();
  for (const event of events) {
    const date = new Date(event.created_at).toISOString().split('T')[0];
    uniqueDays.add(date);
  }
  return uniqueDays.size;
}

function extractFeatureUsage(events: LogEvent[]): Record<string, number> {
  const featureUsage: Record<string, number> = {};
  
  for (const event of events) {
    if (event.event_type === 'user_action' && event.component) {
      featureUsage[event.component] = (featureUsage[event.component] || 0) + 1;
    }
  }
  
  return featureUsage;
}

function extractWorkflowCompletions(events: LogEvent[]): Record<string, number> {
  const workflows: Record<string, number> = {};
  
  // Identify workflow completion events
  for (const event of events) {
    if (event.action && event.action.endsWith('_complete')) {
      const workflow = event.action.replace('_complete', '');
      workflows[workflow] = (workflows[workflow] || 0) + 1;
    }
  }
  
  return workflows;
}

// Additional feature extraction helper functions...
```

### Sequence Processing

For many AI models, event sequences are more valuable than individual events:

```typescript
// src/lib/training/sequence-processing.ts
import { LogEvent } from '../types/logging.types';

// Create sequence data for sequential models (LSTM, Transformers)
export function createSequenceData(logs: LogEvent[], windowSize = 10): any[] {
  const sequences = [];
  
  // Group by user and session
  const userSessions = groupByUserAndSession(logs);
  
  for (const [userId, sessions] of Object.entries(userSessions)) {
    for (const sessionEvents of Object.values(sessions)) {
      // Sort by timestamp
      const sortedEvents = sessionEvents.sort(
        (a, b) => new Date(a.created_at).getTime() - new Date(b.created_at).getTime()
      );
      
      // Create sliding windows of events
      for (let i = 0; i <= sortedEvents.length - windowSize; i++) {
        const windowEvents = sortedEvents.slice(i, i + windowSize);
        const sequenceFeatures = extractSequenceFeatures(windowEvents);
        
        // The next event is the target to predict
        const targetEvent = i + windowSize < sortedEvents.length 
          ? sortedEvents[i + windowSize] 
          : null;
          
        if (targetEvent) {
          sequences.push({
            features: sequenceFeatures,
            target: extractTargetFeatures(targetEvent)
          });
        }
      }
    }
  }
  
  return sequences;
}

function groupByUserAndSession(logs: LogEvent[]): Record<string, Record<string, LogEvent[]>> {
  const result: Record<string, Record<string, LogEvent[]>> = {};
  
  for (const log of logs) {
    if (!log.user_id || !log.session_id) continue;
    
    if (!result[log.user_id]) {
      result[log.user_id] = {};
    }
    
    if (!result[log.user_id][log.session_id]) {
      result[log.user_id][log.session_id] = [];
    }
    
    result[log.user_id][log.session_id].push(log);
  }
  
  return result;
}

function extractSequenceFeatures(events: LogEvent[]): any {
  return events.map(event => ({
    event_type: event.event_type,
    action: event.action,
    component: event.component,
    // Extract relevant features for sequence prediction
    timestamp_delta: event.metadata?.timestamp_delta || 0,
    // Encode event-specific features
    data_features: extractDataFeatures(event.data)
  }));
}

function extractTargetFeatures(event: LogEvent): any {
  // Target features to predict
  return {
    event_type: event.event_type,
    action: event.action,
    component: event.component
  };
}

function extractDataFeatures(data: any): any {
  // Extract numeric features from data
  // This is domain-specific and depends on the data structure
  const features: Record<string, number> = {};
  
  // Example feature extraction
  if (data) {
    if (typeof data.duration === 'number') features.duration = data.duration;
    if (typeof data.count === 'number') features.count = data.count;
    if (data.items && Array.isArray(data.items)) features.item_count = data.items.length;
    // Add more data feature extraction logic...
  }
  
  return features;
}
```

## Feature Engineering Examples

### User Behavior Modeling

Extract features that help predict user behavior:

```typescript
// src/lib/training/behavior-features.ts
import { LogEvent } from '../types/logging.types';

export function extractBehaviorFeatures(userLogs: LogEvent[]): any {
  // User engagement features
  const engagementScore = calculateEngagementScore(userLogs);
  const retentionFeatures = calculateRetentionFeatures(userLogs);
  const usagePatterns = extractUsagePatterns(userLogs);
  
  // Workflow patterns
  const workflowEfficiency = calculateWorkflowEfficiency(userLogs);
  const preferredFeatures = extractPreferredFeatures(userLogs);
  
  // Collaboration patterns (for team products)
  const collaborationFeatures = extractCollaborationFeatures(userLogs);
  
  return {
    engagement: engagementScore,
    retention: retentionFeatures,
    usage_patterns: usagePatterns,
    workflow_efficiency: workflowEfficiency,
    preferred_features: preferredFeatures,
    collaboration: collaborationFeatures
  };
}

function calculateEngagementScore(logs: LogEvent[]): number {
  // Define weights for different event types
  const weights = {
    page_view: 1,
    feature_interaction: 2,
    content_creation: 3,
    sharing: 4,
    feedback: 5
  };
  
  // Calculate weighted sum of events
  let totalScore = 0;
  let weightedCount = 0;
  
  for (const log of logs) {
    const category = categorizeEvent(log);
    if (weights[category]) {
      totalScore += weights[category];
      weightedCount += 1;
    }
  }
  
  return weightedCount > 0 ? totalScore / weightedCount : 0;
}

function categorizeEvent(log: LogEvent): string {
  // Categorize events based on type and action
  if (log.event_type === 'navigation') return 'page_view';
  if (log.event_type === 'user_action' && log.action?.includes('create')) return 'content_creation';
  if (log.event_type === 'user_action' && log.action?.includes('share')) return 'sharing';
  if (log.event_type === 'feedback') return 'feedback';
  return 'feature_interaction'; // Default
}

function calculateRetentionFeatures(logs: LogEvent[]): any {
  // Sort logs by date
  const sortedLogs = [...logs].sort(
    (a, b) => new Date(a.created_at).getTime() - new Date(b.created_at).getTime()
  );
  
  if (sortedLogs.length === 0) return { days_active: 0, retention_days: [] };
  
  const firstDate = new Date(sortedLogs[0].created_at);
  const lastDate = new Date(sortedLogs[sortedLogs.length - 1].created_at);
  
  // Calculate days between first and last activity
  const totalDays = Math.ceil(
    (lastDate.getTime() - firstDate.getTime()) / (1000 * 60 * 60 * 24)
  );
  
  // Get all dates with activity
  const activeDays = new Set();
  for (const log of sortedLogs) {
    const date = new Date(log.created_at).toISOString().split('T')[0];
    activeDays.add(date);
  }
  
  // Calculate day-by-day retention
  const retentionDays = [];
  const startDay = new Date(firstDate);
  startDay.setHours(0, 0, 0, 0);
  
  for (let i = 0; i <= totalDays; i++) {
    const currentDate = new Date(startDay);
    currentDate.setDate(startDay.getDate() + i);
    const dateString = currentDate.toISOString().split('T')[0];
    
    retentionDays.push({
      day: i,
      active: activeDays.has(dateString)
    });
  }
  
  return {
    days_active: activeDays.size,
    retention_days: retentionDays
  };
}

// Additional feature extraction functions...
```

### Content Interaction Features

Analyze how users interact with content:

```typescript
// src/lib/training/content-features.ts
import { LogEvent } from '../types/logging.types';

export function extractContentFeatures(logs: LogEvent[]): any {
  // Filter to content-related events
  const contentEvents = logs.filter(log => 
    log.event_type === 'content_interaction' || 
    (log.event_type === 'user_action' && log.action?.includes('content'))
  );
  
  // Extract features related to content consumption
  const contentConsumption = analyzeContentConsumption(contentEvents);
  
  // Extract features related to content creation
  const contentCreation = analyzeContentCreation(contentEvents);
  
  // Extract content preferences
  const contentPreferences = analyzeContentPreferences(contentEvents);
  
  return {
    consumption: contentConsumption,
    creation: contentCreation,
    preferences: contentPreferences
  };
}

function analyzeContentConsumption(events: LogEvent[]): any {
  // Identify view events
  const viewEvents = events.filter(e => 
    e.action?.includes('view') || 
    e.action?.includes('read') || 
    e.action?.includes('open')
  );
  
  // Calculate time spent on content
  const timeSpentByContentType: Record<string, number> = {};
  const viewCountByContentType: Record<string, number> = {};
  
  for (const event of viewEvents) {
    const contentType = event.data?.content_type || 'unknown';
    const duration = event.data?.duration || 0;
    
    timeSpentByContentType[contentType] = (timeSpentByContentType[contentType] || 0) + duration;
    viewCountByContentType[contentType] = (viewCountByContentType[contentType] || 0) + 1;
  }
  
  // Calculate completion rates
  const completionEvents = events.filter(e => e.action?.includes('complete'));
  const completionRates: Record<string, number> = {};
  
  for (const contentType of Object.keys(viewCountByContentType)) {
    const viewCount = viewCountByContentType[contentType] || 0;
    const completeCount = completionEvents.filter(
      e => e.data?.content_type === contentType
    ).length;
    
    completionRates[contentType] = viewCount > 0 ? completeCount / viewCount : 0;
  }
  
  return {
    view_counts: viewCountByContentType,
    time_spent: timeSpentByContentType,
    completion_rates: completionRates
  };
}

// Additional content analysis functions...
```

### AI Interaction Features

Analyze user interactions with AI features:

```typescript
// src/lib/training/ai-interaction-features.ts
import { LogEvent } from '../types/logging.types';

export function extractAIInteractionFeatures(logs: LogEvent[]): any {
  // Filter to AI interaction events
  const aiEvents = logs.filter(log => log.event_type === 'ai_interaction');
  
  if (aiEvents.length === 0) {
    return { has_ai_interactions: false };
  }
  
  // Calculate usage frequency
  const usageFrequency = calculateAIUsageFrequency(aiEvents);
  
  // Analyze prompt patterns
  const promptPatterns = analyzePromptPatterns(aiEvents);
  
  // Calculate satisfaction metrics
  const satisfactionMetrics = calculateSatisfactionMetrics(aiEvents);
  
  // Analyze feedback patterns
  const feedbackPatterns = analyzeFeedbackPatterns(aiEvents);
  
  return {
    has_ai_interactions: true,
    usage_frequency: usageFrequency,
    prompt_patterns: promptPatterns,
    satisfaction: satisfactionMetrics,
    feedback: feedbackPatterns
  };
}

function calculateAIUsageFrequency(events: LogEvent[]): any {
  // Group events by date
  const eventsByDate: Record<string, LogEvent[]> = {};
  
  for (const event of events) {
    const date = new Date(event.created_at).toISOString().split('T')[0];
    eventsByDate[date] = eventsByDate[date] || [];
    eventsByDate[date].push(event);
  }
  
  // Calculate usage metrics
  const dailyCount = Object.values(eventsByDate).map(events => events.length);
  
  return {
    total_interactions: events.length,
    unique_days: Object.keys(eventsByDate).length,
    avg_daily_interactions: dailyCount.length > 0 
      ? dailyCount.reduce((sum, count) => sum + count, 0) / dailyCount.length 
      : 0,
    max_daily_interactions: Math.max(...dailyCount, 0)
  };
}

function analyzePromptPatterns(events: LogEvent[]): any {
  // Extract prompt-related events
  const promptEvents = events.filter(e => 
    e.action?.includes('prompt') || 
    e.action?.includes('request') || 
    e.action?.includes('query')
  );
  
  // Calculate prompt length statistics
  const promptLengths = promptEvents
    .map(e => e.data?.prompt_length || e.data?.query_length || 0)
    .filter(length => length > 0);
  
  // Identify common prompt types
  const promptTypes: Record<string, number> = {};
  for (const event of promptEvents) {
    const promptType = event.data?.prompt_type || 'unknown';
    promptTypes[promptType] = (promptTypes[promptType] || 0) + 1;
  }
  
  return {
    count: promptEvents.length,
    avg_length: promptLengths.length > 0 
      ? promptLengths.reduce((sum, length) => sum + length, 0) / promptLengths.length 
      : 0,
    types: promptTypes
  };
}

// Additional AI interaction analysis functions...
```

## Model Training Workflows

### User Behavior Prediction Model

Train a model to predict user behavior:

```typescript
// src/lib/training/behavior-model.ts
import * as tf from '@tensorflow/tfjs';
import { extractTrainingData } from './data-extraction';
import { extractBehaviorFeatures } from './behavior-features';

export async function trainBehaviorModel(
  startDate: Date, 
  endDate: Date
): Promise<tf.LayersModel> {
  // Extract and transform the data
  const logs = await extractTrainingData(startDate, endDate);
  
  // Group logs by user
  const userLogs = groupBy(logs, 'user_id');
  
  // Prepare training data
  const trainingData = [];
  const trainingLabels = [];
  
  for (const [userId, userEvents] of Object.entries(userLogs)) {
    // Split data into training period and label period
    const splitDate = new Date(endDate);
    splitDate.setDate(splitDate.getDate() - 7); // Use last 7 days for labels
    
    const trainingPeriodEvents = userEvents.filter(
      e => new Date(e.created_at) < splitDate
    );
    
    const labelPeriodEvents = userEvents.filter(
      e => new Date(e.created_at) >= splitDate
    );
    
    if (trainingPeriodEvents.length === 0 || labelPeriodEvents.length === 0) {
      continue; // Skip users with insufficient data
    }
    
    // Extract features from training period
    const features = extractBehaviorFeatures(trainingPeriodEvents);
    
    // Calculate labels from label period
    const labels = calculateBehaviorLabels(labelPeriodEvents);
    
    // Add to training data
    trainingData.push(flattenFeatures(features));
    trainingLabels.push(labels);
  }
  
  // Convert to tensors
  const xs = tf.tensor2d(trainingData);
  const ys = tf.tensor2d(trainingLabels);
  
  // Create and train the model
  const model = createBehaviorModel(xs.shape[1], ys.shape[1]);
  
  await model.fit(xs, ys, {
    epochs: 50,
    batchSize: 32,
    validationSplit: 0.2,
    callbacks: tf.callbacks.earlyStopping({ patience: 5 })
  });
  
  return model;
}

function flattenFeatures(features: any): number[] {
  // Convert nested feature object to flat array
  const result = [];
  
  // Engagement score
  result.push(features.engagement);
  
  // Retention features
  result.push(features.retention.days_active);
  
  // Usage patterns
  const usagePatterns = features.usage_patterns;
  result.push(usagePatterns.morning_activity || 0);
  result.push(usagePatterns.afternoon_activity || 0);
  result.push(usagePatterns.evening_activity || 0);
  result.push(usagePatterns.weekend_activity || 0);
  
  // Workflow efficiency
  result.push(features.workflow_efficiency.completion_rate || 0);
  result.push(features.workflow_efficiency.avg_completion_time || 0);
  
  // Add more feature flattening...
  
  return result;
}

function calculateBehaviorLabels(events: LogEvent[]): number[] {
  // Calculate label values to predict
  return [
    events.length > 0 ? 1 : 0, // Active user
    calculateFeatureUsageIntensity(events), // Usage intensity
    calculateWorkflowCompletionRate(events), // Workflow completion
    events.some(e => e.action?.includes('upgrade')) ? 1 : 0, // Conversion
    calculateContentEngagement(events) // Content engagement
  ];
}

function createBehaviorModel(inputSize: number, outputSize: number): tf.LayersModel {
  const model = tf.sequential();
  
  // Add layers
  model.add(tf.layers.dense({
    units: 32,
    activation: 'relu',
    inputShape: [inputSize]
  }));
  
  model.add(tf.layers.dropout({ rate: 0.2 }));
  
  model.add(tf.layers.dense({
    units: 16,
    activation: 'relu'
  }));
  
  model.add(tf.layers.dense({
    units: outputSize,
    activation: 'sigmoid'
  }));
  
  // Compile the model
  model.compile({
    optimizer: tf.train.adam(0.001),
    loss: 'binaryCrossentropy',
    metrics: ['accuracy']
  });
  
  return model;
}

// Helper functions...
```

### Content Recommendation Model

Train a collaborative filtering model for content recommendations:

```typescript
// src/lib/training/recommendation-model.ts
import * as tf from '@tensorflow/tfjs';
import { extractTrainingData } from './data-extraction';

export async function trainContentRecommendationModel(
  startDate: Date,
  endDate: Date
): Promise<tf.LayersModel> {
  // Extract content interaction data
  const logs = await extractTrainingData(startDate, endDate);
  
  // Filter to content view events
  const contentViews = logs.filter(log => 
    log.event_type === 'content_interaction' && 
    log.action === 'view_content'
  );
  
  // Create user-content interaction matrix
  const { userIndices, contentIndices, interactions } = processInteractions(contentViews);
  
  // Extract metadata features
  const contentFeatures = extractContentFeatures(logs, contentIndices);
  const userFeatures = extractUserFeatures(logs, userIndices);
  
  // Create and train the model
  const model = createRecommendationModel(
    userIndices.size,
    contentIndices.size,
    userFeatures.shape[1],
    contentFeatures.shape[1]
  );
  
  // Prepare training data
  const userIdTensor = tf.tensor1d(interactions.map(i => i.userIndex), 'int32');
  const contentIdTensor = tf.tensor1d(interactions.map(i => i.contentIndex), 'int32');
  const labelsTensor = tf.tensor1d(interactions.map(i => i.score));
  
  // Train the model
  await model.fit(
    [userIdTensor, contentIdTensor, userFeatures, contentFeatures],
    labelsTensor,
    {
      epochs: 20,
      batchSize: 64,
      validationSplit: 0.2,
      callbacks: tf.callbacks.earlyStopping({ patience: 3 })
    }
  );
  
  return model;
}

function processInteractions(contentViews: LogEvent[]): any {
  // Create maps for indexing
  const userIndices = new Map<string, number>();
  const contentIndices = new Map<string, number>();
  const interactions = [];
  
  // Create indices and interaction records
  for (const view of contentViews) {
    const userId = view.user_id;
    const contentId = view.data?.content_id;
    
    if (!userId || !contentId) continue;
    
    // Create indices if needed
    if (!userIndices.has(userId)) {
      userIndices.set(userId, userIndices.size);
    }
    
    if (!contentIndices.has(contentId)) {
      contentIndices.set(contentId, contentIndices.size);
    }
    
    // Add interaction with derived engagement score
    const engagementScore = calculateEngagementScore(view);
    
    interactions.push({
      userIndex: userIndices.get(userId),
      contentIndex: contentIndices.get(contentId),
      score: engagementScore
    });
  }
  
  return { userIndices, contentIndices, interactions };
}

function calculateEngagementScore(view: LogEvent): number {
  // Calculate engagement score from 0 to 1
  let score = 0.5; // Default value
  
  // Adjust based on view duration
  if (view.data?.duration) {
    const duration = view.data.duration;
    const contentLength = view.data?.content_length || 1000; // Default if unknown
    
    // Calculate read percentage with a cap
    const readPercentage = Math.min(duration / contentLength, 1);
    score += readPercentage * 0.3; // Weight for read percentage
  }
  
  // Adjust based on interactions
  if (view.data?.interactions) {
    const interactionCount = view.data.interactions.length;
    score += Math.min(interactionCount / 5, 0.2); // Weight for interactions
  }
  
  // Cap at 0-1 range
  return Math.max(0, Math.min(1, score));
}

function extractContentFeatures(logs: LogEvent[], contentIndices: Map<string, number>): tf.Tensor2d {
  // Create feature array
  const features = Array(contentIndices.size).fill(null).map(() => {
    return Array(10).fill(0); // 10 features per content item
  });
  
  // Collect content metadata
  const contentMetadata = new Map<string, any>();
  
  for (const log of logs) {
    if (log.event_type === 'content_metadata' && log.data?.content_id) {
      contentMetadata.set(log.data.content_id, log.data);
    }
  }
  
  // Fill in features
  for (const [contentId, index] of contentIndices.entries()) {
    const metadata = contentMetadata.get(contentId) || {};
    
    // Set feature values based on metadata
    features[index][0] = metadata.type === 'article' ? 1 : 0;
    features[index][1] = metadata.type === 'video' ? 1 : 0;
    features[index][2] = metadata.type === 'interactive' ? 1 : 0;
    features[index][3] = metadata.difficulty || 0.5;
    features[index][4] = metadata.word_count ? Math.min(metadata.word_count / 2000, 1) : 0.5;
    // Add more features...
  }
  
  return tf.tensor2d(features);
}

// More helper functions...

function createRecommendationModel(
  numUsers: number,
  numContent: number,
  userFeatureDim: number,
  contentFeatureDim: number
): tf.LayersModel {
  // User input and embedding
  const userInput = tf.input({ shape: [1], name: 'user_id', dtype: 'int32' });
  const userEmbedding = tf.layers.embedding({
    inputDim: numUsers,
    outputDim: 16,
    inputLength: 1
  }).apply(userInput);
  const userFeatureInput = tf.input({ shape: [userFeatureDim], name: 'user_features' });
  
  // Content input and embedding
  const contentInput = tf.input({ shape: [1], name: 'content_id', dtype: 'int32' });
  const contentEmbedding = tf.layers.embedding({
    inputDim: numContent,
    outputDim: 16,
    inputLength: 1
  }).apply(contentInput);
  const contentFeatureInput = tf.input({ shape: [contentFeatureDim], name: 'content_features' });
  
  // Reshape embeddings
  const reshapeLayer = tf.layers.flatten();
  const userVector = reshapeLayer.apply(userEmbedding);
  const contentVector = reshapeLayer.apply(contentEmbedding);
  
  // Combine embeddings and features
  const userCombined = tf.layers.concatenate().apply([
    userVector, user

================
File: docs/comprehensive-logging-system/API_REFERENCE.md
================
# API Reference: Comprehensive Logging System

## Overview

This document provides a complete reference for the Comprehensive Logging System API. It details all available service methods, their parameters, return values, and usage examples. This reference is intended for developers who need to integrate with or extend the logging system.

## Table of Contents

1. [Core Logging Service](#core-logging-service)
2. [Privacy Service](#privacy-service)
3. [Analytics Service](#analytics-service)
4. [React Hooks](#react-hooks)
5. [Helper Utilities](#helper-utilities)
6. [TypeScript Interfaces](#typescript-interfaces)
7. [Error Handling](#error-handling)

## Core Logging Service

The `LoggingService` class is the main entry point for the logging system. It provides methods for logging various types of events.

### Class: `LoggingService`

```typescript
import { LoggingService } from '../lib/services/logging.service';

// Singleton instance
export const loggingService = new LoggingService();
```

#### Constructor

```typescript
constructor(options?: LoggingServiceOptions)
```

**Parameters:**
- `options` (optional): Configuration options for the logging service
  - `enabled`: Boolean to enable/disable logging
  - `verbosity`: Log level ('debug', 'info', 'warn', 'error')
  - `batchingEnabled`: Enable batch logging
  - `maxBatchSize`: Maximum events in a batch
  - `batchIntervalMs`: Batch flush interval in milliseconds

#### Method: `logEvent`

Logs a generic event to the system.

```typescript
async logEvent(event: LogEvent): Promise<string | null>
```

**Parameters:**
- `event`: The event to log
  - `user_id?`: Optional user ID
  - `persona_id?`: Optional persona ID
  - `company_id?`: Optional company ID
  - `event_type`: Type of event ('user_action', 'system_event', etc.)
  - `event_source`: Source of the event ('ui', 'api', etc.)
  - `component?`: Optional component name
  - `action`: The action that occurred
  - `data`: Event data (will be stored as JSONB)
  - `metadata?`: Optional additional metadata
  - `session_id?`: Optional session ID

**Returns:**
- A Promise that resolves to the event ID if successful, or null if logging failed or was disabled

**Example:**
```typescript
await loggingService.logEvent({
  user_id: currentUser.id,
  event_type: 'user_action',
  event_source: 'profile_page',
  component: 'EditProfileForm',
  action: 'save_profile',
  data: {
    fields_updated: ['name', 'bio'],
    has_avatar: true
  },
  metadata: {
    ui_version: '2.3.0',
    feature_flags: ['new_profile_ui', 'enhanced_validation']
  }
});
```

#### Method: `logUserAction`

Convenience method to log user-initiated actions.

```typescript
async logUserAction(action: string, data: any, metadata?: any): Promise<string | null>
```

**Parameters:**
- `action`: The user action (e.g., 'button_click', 'form_submit')
- `data`: Data associated with the action
- `metadata`: Optional additional context

**Returns:**
- A Promise that resolves to the event ID if successful, or null if logging failed

**Example:**
```typescript
const handleSubmit = async (formData) => {
  await loggingService.logUserAction(
    'submit_idea',
    {
      idea_type: formData.type,
      word_count: formData.description.split(' ').length,
      has_attachments: formData.attachments.length > 0
    },
    { form_version: '3.1.2' }
  );
  
  // Process form submission
};
```

#### Method: `logAIInteraction`

Logs interactions with AI models.

```typescript
async logAIInteraction(action: string, data: any, metadata?: any): Promise<string | null>
```

**Parameters:**
- `action`: The AI interaction type (e.g., 'generate_text', 'classify_content')
- `data`: Data associated with the interaction
- `metadata`: Optional additional context

**Returns:**
- A Promise that resolves to the event ID if successful, or null if logging failed

**Example:**
```typescript
// Before making the AI request
const interactionId = await loggingService.logAIInteraction(
  'generate_ideas',
  {
    model: 'gpt-4',
    input_length: prompt.length,
    parameters: {
      temperature: 0.7,
      max_tokens: 500
    }
  }
);

try {
  const result = await openAI.createCompletion({/*...*/});
  
  // Log the result
  await loggingService.logAIInteraction(
    'generate_ideas_result',
    {
      interaction_id: interactionId,
      output_length: result.choices[0].text.length,
      token_usage: result.usage,
      success: true
    }
  );
} catch (error) {
  // Log the error
  await loggingService.logAIInteraction(
    'generate_ideas_error',
    {
      interaction_id: interactionId,
      error: error.message,
      success: false
    }
  );
}
```

#### Method: `logError`

Logs an error event.

```typescript
async logError(error: Error, source: string, context?: any): Promise<string | null>
```

**Parameters:**
- `error`: The Error object
- `source`: Where the error occurred (component or service name)
- `context`: Optional additional context about the error

**Returns:**
- A Promise that resolves to the event ID if successful, or null if logging failed

**Example:**
```typescript
try {
  await fetchData();
} catch (error) {
  await loggingService.logError(error, 'DataService', {
    method: 'fetchUserProfile',
    userId: currentUser.id,
    attempt: retryCount
  });
  
  // Handle the error
}
```

#### Method: `logPerformance`

Logs performance-related metrics.

```typescript
async logPerformance(
  operation: string,
  duration: number,
  unit: 'ms' | 's',
  context?: any
): Promise<string | null>
```

**Parameters:**
- `operation`: The operation being measured
- `duration`: The time it took
- `unit`: Time unit ('ms' or 's')
- `context`: Optional additional context

**Returns:**
- A Promise that resolves to the event ID if successful, or null if logging failed

**Example:**
```typescript
const startTime = performance.now();

// Some operation
const result = await expensiveOperation();

const duration = performance.now() - startTime;
await loggingService.logPerformance(
  'render_dashboard',
  duration,
  'ms',
  {
    component_count: dashboardComponents.length,
    data_size: JSON.stringify(dashboardData).length,
    user_tier: currentUser.tier
  }
);
```

#### Method: `batchLog`

Logs multiple events in a batch for efficiency.

```typescript
async batchLog(events: LogEvent[]): Promise<(string | null)[]>
```

**Parameters:**
- `events`: Array of events to log

**Returns:**
- A Promise that resolves to an array of event IDs or nulls

**Example:**
```typescript
// Collect events during a complex operation
const events = [];

// Step 1
events.push({
  event_type: 'workflow',
  event_source: 'import_wizard',
  action: 'step_1_complete',
  data: { validated_entries: 150 }
});

// Step 2
events.push({
  event_type: 'workflow',
  event_source: 'import_wizard',
  action: 'step_2_complete',
  data: { processed_entries: 130 }
});

// Log all events at once
await loggingService.batchLog(events);
```

#### Method: `enableLogging`

Enables or disables logging.

```typescript
enableLogging(enabled: boolean): void
```

**Parameters:**
- `enabled`: Boolean to enable or disable logging

**Example:**
```typescript
// Disable logging during sensitive operations
loggingService.enableLogging(false);

// Perform sensitive operation
await processSensitiveData();

// Re-enable logging
loggingService.enableLogging(true);
```

#### Method: `setVerbosity`

Sets the logging verbosity level.

```typescript
setVerbosity(level: 'debug' | 'info' | 'warn' | 'error'): void
```

**Parameters:**
- `level`: The verbosity level

**Example:**
```typescript
// Set higher verbosity during development
if (process.env.NODE_ENV === 'development') {
  loggingService.setVerbosity('debug');
} else {
  loggingService.setVerbosity('warn');
}
```

#### Method: `flush`

Manually flushes any batched logs.

```typescript
async flush(): Promise<void>
```

**Example:**
```typescript
// Ensure all logs are sent before the user leaves
window.addEventListener('beforeunload', () => {
  loggingService.flush();
});
```

## Privacy Service

The `PrivacyService` handles consent checking and data protection.

### Class: `PrivacyService`

```typescript
import { privacyService } from '../lib/services/privacy.service';
```

#### Method: `checkConsent`

Checks if a user has given consent for a specific logging purpose.

```typescript
async checkConsent(userId: string | undefined, consentType: string): Promise<boolean>
```

**Parameters:**
- `userId`: The user's ID, or undefined for anonymous users
- `consentType`: The type of consent to check ('analytics', 'ai_training', etc.)

**Returns:**
- A Promise that resolves to a boolean indicating consent status

**Example:**
```typescript
// Before logging analytics data
const canLogAnalytics = await privacyService.checkConsent(
  currentUser?.id,
  'analytics'
);

if (canLogAnalytics) {
  await loggingService.logUserAction('page_view', { page: 'dashboard' });
}
```

#### Method: `classifyData`

Classifies data according to privacy sensitivity.

```typescript
async classifyData(data: any): Promise<{ 
  classification: string; 
  retentionPolicy: string 
}>
```

**Parameters:**
- `data`: The data to classify

**Returns:**
- A Promise that resolves to an object with classification and retention policy

**Example:**
```typescript
// Classify data before logging
const { classification, retentionPolicy } = await privacyService.classifyData(userData);

// Only log if not sensitive
if (classification !== 'sensitive') {
  await loggingService.logEvent({
    event_type: 'user_profile',
    event_source: 'profile_service',
    action: 'profile_update',
    data: userData,
    data_classification: classification,
    retention_policy: retentionPolicy
  });
}
```

#### Method: `anonymizeData`

Anonymizes sensitive data for logging or analysis.

```typescript
async anonymizeData(
  data: any, 
  level: 'pseudonymize' | 'anonymize' = 'anonymize'
): Promise<any>
```

**Parameters:**
- `data`: The data to anonymize
- `level`: The anonymization level

**Returns:**
- A Promise that resolves to the anonymized data

**Example:**
```typescript
// Anonymize user data for AI training
const anonymizedData = await privacyService.anonymizeData(
  userData,
  'pseudonymize' // Preserve patterns but remove identifiers
);

await loggingService.logAIInteraction(
  'train_model',
  {
    input_data: anonymizedData,
    model_type: 'recommendation'
  }
);
```

#### Method: `handlePrivacyRequest`

Creates and processes a privacy request (e.g., data export or deletion).

```typescript
async handlePrivacyRequest(
  userId: string, 
  requestType: 'export' | 'deletion' | 'correction' | 'restriction'
): Promise<string>
```

**Parameters:**
- `userId`: The user's ID
- `requestType`: The type of privacy request

**Returns:**
- A Promise that resolves to the request ID

**Example:**
```typescript
// When user requests their data
async function handleExportRequest(userId: string) {
  try {
    const requestId = await privacyService.handlePrivacyRequest(
      userId,
      'export'
    );
    
    return {
      success: true,
      message: 'Your request is being processed. You will be notified when completed.',
      requestId
    };
  } catch (error) {
    loggingService.logError(error, 'PrivacyController', { userId });
    return {
      success: false,
      message: 'Failed to process your request. Please try again later.'
    };
  }
}
```

## Analytics Service

The `AnalyticsService` processes log data for insights and model training.

### Class: `AnalyticsService`

```typescript
import { analyticsService } from '../lib/services/analytics.service';
```

#### Method: `processLogData`

Processes log data for analysis.

```typescript
async processLogData(startDate?: Date, endDate?: Date): Promise<any>
```

**Parameters:**
- `startDate`: Optional start date for the data range
- `endDate`: Optional end date for the data range

**Returns:**
- A Promise that resolves to the processed data

**Example:**
```typescript
// Process last week's data
const oneWeekAgo = new Date();
oneWeekAgo.setDate(oneWeekAgo.getDate() - 7);

const processedData = await analyticsService.processLogData(oneWeekAgo);
```

#### Method: `trainModels`

Trains machine learning models based on log data.

```typescript
async trainModels(): Promise<void>
```

**Example:**
```typescript
// Schedule regular model training
setInterval(async () => {
  try {
    await analyticsService.trainModels();
    console.log('Models trained successfully');
  } catch (error) {
    console.error('Model training failed:', error);
  }
}, 24 * 60 * 60 * 1000); // Daily
```

#### Method: `generateInsights`

Generates insights from processed data.

```typescript
async generateInsights(): Promise<any>
```

**Returns:**
- A Promise that resolves to the generated insights

**Example:**
```typescript
async function generateDashboardInsights() {
  const insights = await analyticsService.generateInsights();
  return insights.filter(insight => insight.confidence > 0.7);
}
```

#### Method: `getCompanyContext`

Builds context for a specific company based on their logs.

```typescript
async getCompanyContext(companyId: string): Promise<any>
```

**Parameters:**
- `companyId`: The company's ID

**Returns:**
- A Promise that resolves to the company context

**Example:**
```typescript
// Get context before generating company-specific recommendations
const companyContext = await analyticsService.getCompanyContext(currentCompany.id);

// Use context to personalize recommendations
const recommendations = await generateRecommendations(companyContext);
```

## React Hooks

### Hook: `useLogging`

A React hook that provides access to logging functionality within components.

```typescript
import { useLogging } from '../lib/hooks/useLogging';

function MyComponent() {
  const { 
    logUIEvent, 
    logFormSubmission, 
    logNavigation 
  } = useLogging();
  
  // Use logging methods in component
}
```

#### Method: `logUIEvent`

Logs a UI interaction.

```typescript
const logUIEvent: (
  component: string,
  action: string,
  data: any,
  metadata?: any
) => Promise<void>;
```

**Example:**
```typescript
function FeatureCard({ feature }) {
  const { logUIEvent } = useLogging();
  
  const handleClick = () => {
    logUIEvent(
      'FeatureCard',
      'feature_click',
      { featureId: feature.id, featureType: feature.type },
      { cardPosition: 'main_dashboard' }
    );
    
    // Handle the click
  };
  
  return (
    <div onClick={handleClick}>
      {/* Card content */}
    </div>
  );
}
```

#### Method: `logFormSubmission`

Logs form submissions.

```typescript
const logFormSubmission: (
  form: string,
  data: any,
  metadata?: any
) => Promise<void>;
```

**Example:**
```typescript
function ContactForm() {
  const { logFormSubmission } = useLogging();
  const [formData, setFormData] = useState({/*...*/});
  
  const handleSubmit = async (e) => {
    e.preventDefault();
    
    // Log the submission
    await logFormSubmission(
      'contact_form',
      { 
        has_email: !!formData.email,
        message_length: formData.message.length,
        topic: formData.topic
      }
    );
    
    // Submit the form
    await submitContactForm(formData);
  };
  
  return (
    <form onSubmit={handleSubmit}>
      {/* Form fields */}
    </form>
  );
}
```

#### Method: `logNavigation`

Logs navigation events.

```typescript
const logNavigation: (
  from: string,
  to: string,
  metadata?: any
) => Promise<void>;
```

**Example:**
```typescript
function NavigationLink({ to, children }) {
  const { logNavigation } = useLogging();
  const location = useLocation();
  
  const handleClick = () => {
    logNavigation(
      location.pathname,
      to,
      { navSource: 'sidebar_menu' }
    );
  };
  
  return (
    <Link to={to} onClick={handleClick}>
      {children}
    </Link>
  );
}
```

### Hook: `usePerformanceLogging`

A React hook that measures and logs component performance.

```typescript
import { usePerformanceLogging } from '../lib/hooks/usePerformanceLogging';

function DataTable({ data }) {
  usePerformanceLogging('DataTable', { rowCount: data.length });
  
  // Component implementation
}
```

## Helper Utilities

### Utility: `createLogContext`

Creates a context object to add to logs.

```typescript
import { createLogContext } from '../lib/utils/logging-utils';

const context = createLogContext(user, {
  page: 'dashboard',
  view: 'summary'
});
```

### Utility: `sanitizeLogData`

Sanitizes data to remove sensitive information before logging.

```typescript
import { sanitizeLogData } from '../lib/utils/logging-utils';

const safeData = sanitizeLogData(userData);
await loggingService.logEvent({
  // Other properties
  data: safeData
});
```

### Utility: `withLogging`

Higher-order function that adds logging to any function.

```typescript
import { withLogging } from '../lib/utils/logging-utils';

const fetchUserWithLogging = withLogging(
  fetchUser,
  'UserService',
  'fetchUser'
);

// When called, will automatically log start, completion, and errors
const user = await fetchUserWithLogging(userId);
```

## TypeScript Interfaces

### Interface: `LogEvent`

Represents a log event.

```typescript
interface LogEvent {
  id?: string;
  user_id?: string;
  persona_id?: string;
  company_id?: string;
  event_type: string;
  event_source: string;
  component?: string;
  action: string;
  data: any; // JSONB in database
  metadata?: any; // JSONB in database
  data_classification?: 'non_personal' | 'pseudonymized' | 'personal' | 'sensitive';
  retention_policy?: 'transient' | 'short_term' | 'medium_term' | 'long_term';
  session_id?: string;
  client_info?: any;
  created_at?: string;
}
```

### Interface: `ConsentSettings`

Represents user consent preferences.

```typescript
interface ConsentSettings {
  user_id: string;
  essential: boolean; // Cannot be disabled
  analytics: boolean;
  product_improvement: boolean;
  ai_training: boolean; 
  cross_company_insights: boolean;
  personalization: boolean;
  last_updated: string;
  verified: boolean;
}
```

### Interface: `LoggingServiceOptions`

Configuration options for the logging service.

```typescript
interface LoggingServiceOptions {
  enabled?: boolean;
  verbosity?: 'debug' | 'info' | 'warn' | 'error';
  batchingEnabled?: boolean;
  maxBatchSize?: number;
  batchIntervalMs?: number;
  samplingRate?: number;
}
```

## Error Handling

### LoggingError Class

```typescript
class LoggingError extends Error {
  code: string;
  details?: any;
  
  constructor(message: string, code: string, details?: any) {
    super(message);
    this.name = 'LoggingError';
    this.code = code;
    this.details = details;
  }
}
```

### Common Error Codes

- `CONSENT_REQUIRED`: User hasn't provided required consent
- `CLASSIFICATION_FAILED`: Failed to classify data sensitivity
- `RATE_LIMIT_EXCEEDED`: Too many log events in a short period
- `INVALID_DATA`: Log data doesn't meet schema requirements
- `SERVICE_UNAVAILABLE`: Logging service is unavailable

### Error Handling Example

```typescript
try {
  await loggingService.logEvent({/*...*/});
} catch (error) {
  if (error instanceof LoggingError) {
    switch (error.code) {
      case 'CONSENT_REQUIRED':
        // Prompt user for consent
        showConsentDialog();
        break;
      case 'RATE_LIMIT_EXCEEDED':
        // Implement backoff strategy
        scheduleRetry(error.details.retryAfter);
        break;
      default:
        // Handle other errors
        console.error('Logging error:', error.message);
    }
  } else {
    // Handle unexpected errors
    console.error('Unexpected error:', error);
  }
}
```

This API Reference provides a comprehensive guide to the Comprehensive Logging System. For implementation examples and best practices, see the [Implementation Guide](./IMPLEMENTATION_GUIDE.md).

================
File: docs/comprehensive-logging-system/COMPANY_CONTEXT_FEATURES_FIX.md
================
# Company Context and Features Fix

## Problem Summary

The application is experiencing two critical issues:

1. **Infinite Recursion in Company Members Policy**: 
   ```
   Error checking company access: {code: '42P17', details: null, hint: null, message: 'infinite recursion detected in policy for relation "company_members"'}
   ```
   This occurs when Row Level Security (RLS) policies create circular references between tables.

2. **Duplicate Key Constraint in Extracted Features**:
   ```
   Error saving extracted feature: {code: '23505', details: 'Key (feature_set, feature_name)=(user_interaction, user_interaction_feature) already exists.', hint: null, message: 'duplicate key value violates unique constraint "extracted_features_feature_set_feature_name_key"'}
   ```
   This prevents the system from saving multiple instances of the same feature type.

## Solution Overview

Our solution consists of three components:

1. **Database Migration**: SQL changes to implement a secure function and fix constraints
2. **Enhanced Logging Hook Update**: Modifications to use the new secure function
3. **Model Training Service Update**: Changes to prevent duplicate key violations

## Database Changes Required

The following SQL need to be executed in your Supabase instance with admin privileges:

```sql
-- Create a secure function to fetch company context
CREATE OR REPLACE FUNCTION public.fetch_company_context_securely(
  p_user_id UUID,
  p_company_id UUID
)
RETURNS BOOLEAN
SECURITY DEFINER
SET search_path = public
LANGUAGE plpgsql
AS $$
BEGIN
  -- Direct SQL query bypassing RLS
  RETURN EXISTS (
    SELECT 1 
    FROM company_members 
    WHERE user_id = p_user_id 
    AND company_id = p_company_id
  );
END;
$$;

COMMENT ON FUNCTION public.fetch_company_context_securely IS 'Securely check if a user is a member of a company without triggering recursive RLS policies';

-- Grant execute permission to authenticated users
GRANT EXECUTE ON FUNCTION public.fetch_company_context_securely TO authenticated;

-- Fix for duplicate key constraint in extracted_features
-- First alter the table to drop the existing unique constraint
ALTER TABLE public.extracted_features 
DROP CONSTRAINT IF EXISTS extracted_features_feature_set_feature_name_key;

-- Add a new unique constraint that includes the created_at timestamp
-- This ensures that multiple features with the same set and name can exist 
-- if they were created at different times
ALTER TABLE public.extracted_features 
ADD CONSTRAINT extracted_features_unique_with_time 
UNIQUE (feature_set, feature_name, created_at);

-- Update row level security on extracted_features
-- Drop existing policies if any
DROP POLICY IF EXISTS "extracted_features_policy" ON public.extracted_features;

-- Create new policy that allows insertion with user context
CREATE POLICY "extracted_features_policy" ON public.extracted_features
USING (true)
WITH CHECK (true);

-- Update public permissions for extracted_features
GRANT SELECT, INSERT, UPDATE, DELETE ON public.extracted_features TO authenticated;

-- Ensure sequence permissions are granted
GRANT USAGE, SELECT ON ALL SEQUENCES IN SCHEMA public TO authenticated;
```

## Client-side Code Changes

1. The `useEnhancedLogging.ts` hook has been updated to use the new secure function
2. The `model-training.service.ts` has been updated with:
   - Unique feature name generation with timestamp suffixes
   - Better error handling for feature extraction
   - Using upsert instead of insert to handle potential conflicts

## How to Apply the Fix

1. Execute the SQL commands above on your Supabase instance
2. Deploy the updated client-side code to your application
3. Test with the provided test script:
   ```bash
   # Set environment variables first
   export SUPABASE_URL='your-supabase-url'
   export SUPABASE_SERVICE_ROLE_KEY='your-service-role-key'
   
   # Run the test
   node scripts/test-system-logs-fix.js
   ```

## Validation

After applying the fix, the application should be able to:
1. Check company membership without infinite recursion
2. Extract features without duplicate key violations
3. Properly log user interactions and system events

## Technical Background

The infinite recursion issue occurs because RLS policies on `company_members` table are checking other tables that in turn try to access `company_members`, creating a circular dependency. Our solution uses a SECURITY DEFINER function that bypasses RLS completely when checking company membership.

The duplicate key constraint is resolved by modifying the uniqueness requirements to include the creation timestamp, allowing features with the same name and set to exist if created at different times.

================
File: docs/comprehensive-logging-system/COMPANY_MEMBERS_RECURSION_FIX.md
================
# Company Members Recursion Fix

## Issue Description

The application was experiencing an infinite recursion error in PostgreSQL's Row Level Security (RLS) policy for the `company_members` relation. The specific error was:

```
App.tsx:67 Error fetching logs for feature extraction: {
  code: '42P17', 
  details: null, 
  hint: null, 
  message: 'infinite recursion detected in policy for relation "company_members"'
}
```

This error occurred because the logging system was attempting to capture company context information while processing logs, which triggered a recursive database query pattern that PostgreSQL detected and prevented.

## Root Cause Analysis

1. The application's `LoggingProvider` component was configured to capture company context by default.
2. When logging events occurred, the `useCentralizedLogging` hook would pass this configuration to `useEnhancedLogging`.
3. The `useEnhancedLogging` hook would then attempt to query the `company_members` table to get company context.
4. This query would trigger the RLS policy, which might have been checking permissions based on the user's company membership.
5. The policy check would then generate additional log events, creating an infinite recursion.

## Solution

We implemented a full configuration chain that properly disables company context capturing:

1. **Default Configuration**: Changed the default value for `captureCompanyContext` to `false` at all levels of the logging system.

2. **LoggingProvider Component**: Ensured the LoggingProvider properly passes configuration options:
   ```tsx
   // Initialize the logging hook with configuration options
   const loggingFunctions = useCentralizedLogging({
     enableDetailedLogging,
     captureUserContext,
     captureCompanyContext,  // Now properly passed
     captureSystemContext,
     featureSets
   });
   ```

3. **useCentralizedLogging Hook**: Updated to accept and forward configuration:
   ```tsx
   export const useCentralizedLogging = (options?: CentralizedLoggingOptions) => {
     // ...
     const enhancedLogging = useEnhancedLogging({
       detailLevel: 'extensive',
       captureUserContext: options?.captureUserContext ?? true,
       captureCompanyContext: options?.captureCompanyContext ?? false, // Default to false
       // ...
     });
     // ...
   };
   ```

4. **App Component**: Already had the correct configuration in place:
   ```tsx
   <LoggingProvider
     enableDetailedLogging={true}
     captureUserContext={true}
     captureCompanyContext={false} // Already disabled for company functionality
     captureSystemContext={true}
     featureSets={['user_behavior', 'system_interactions', 'business_logic', 'ai_conversations', 'idea_generation']}
   >
   ```

5. **Stubbed Company Access Service**: As an additional safeguard, the application has a stubbed company access service that doesn't query the database:
   ```tsx
   async checkUserCompanyAccess(userId: string) {
     // ...
     // Return default empty values with error property to maintain API compatibility
     return {
       hasCompany: false,
       companyData: [],
       accessType: null,
       stubbed: true,
       error: null  // Include error property but set to null
     };
   }
   ```

## Impact and Limitations

- **Resolved Issue**: The infinite recursion error is resolved, allowing the application to function properly.
- **Limited Functionality**: Company-related features may have limited functionality since company context is not being captured for logs.
- **Temporary Solution**: This is a temporary solution until the company functionality and its database schema are properly rebuilt.

## Future Improvements

When rebuilding the company functionality, consider the following:

1. Redesign the RLS policies to avoid circular dependencies.
2. Use a different approach for capturing company context in logs that doesn't rely on querying the same tables that are being logged.
3. Implement a more granular control system for logging contexts to prevent cascading queries.

================
File: docs/comprehensive-logging-system/COMPANY_MEMBERS_REMOVAL.md
================
# Company Members Functionality Removal

## Overview

This document describes the changes made to remove the company members functionality from the application. This is a temporary measure until the company functionality can be completely rebuilt.

## Issue

We encountered the following error when accessing the dashboard:
```
App.tsx:67 Error fetching logs for feature extraction: {
  code: '42P17', 
  details: null, 
  hint: null, 
  message: 'infinite recursion detected in policy for relation "company_members"'
}
```

The infinite recursion was happening in the PostgreSQL Row Level Security (RLS) policies for the `company_members` table. Instead of fixing the recursion directly, we've chosen to temporarily remove the company members functionality and rebuild it from scratch later.

## Changes Made

### 1. Stubbed Company Access Service

The `company-access.service.ts` file was modified to return a stubbed version that doesn't query the `company_members` table at all. The service now:

- Returns empty company data with a `stubbed: true` flag
- Logs that the stubbed service was used
- Maintains the same API interface to avoid breaking code that depends on it

### 2. Disabled Company Context in Logging

Modified `App.tsx` to disable company context capturing in the `LoggingProvider`:

```typescript
<LoggingProvider
  enableDetailedLogging={true}
  captureUserContext={true}
  captureCompanyContext={false} // Disabled company context to prevent recursion issues
  captureSystemContext={true}
  featureSets={['user_behavior', 'system_interactions', 'business_logic', 'ai_conversations', 'idea_generation']}
>
```

## Affected Functionality

The following functionality is temporarily unavailable:

- Company membership management
- Company role permissions
- Company member lists
- Company context in logs

## Next Steps

When rebuilding the company functionality, consider these improvements:

1. Redesign the database schema to avoid circular dependencies
2. Implement RLS policies that prevent recursive checks
3. Use service-role API functions for complex permission checks
4. Add better error handling for database permission issues

Until the functionality is rebuilt, the application will continue to operate with stubbed company data.

================
File: docs/comprehensive-logging-system/COMPLIANCE.md
================
# Compliance: Comprehensive Logging System

## Overview

This document outlines how the Comprehensive Logging System complies with key data protection and privacy regulations, particularly the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA).

## GDPR Compliance

### Legal Basis for Processing

Our logging system processes personal data under the following legal bases:

1. **Consent**: Where users have explicitly consented to specific data processing purposes
2. **Legitimate Interest**: For essential system functions and security
3. **Contractual Necessity**: For features required to fulfill our service

### Data Subject Rights Support

The system supports all GDPR-mandated data subject rights:

#### Right to Access

Users can access their data through the Privacy Dashboard, which provides:
- Categories of collected data
- Specific data points with timestamps
- Purpose of collection for each data category
- Data retention periods

#### Right to Rectification

While log data is generally not editable (for integrity purposes), users can:
- Flag incorrect personal information
- Add clarifying notes to their data records
- Update their profile information which will be reflected in future logs

#### Right to Erasure (Right to be Forgotten)

The system supports complete and partial deletion options:
- Users can request deletion of specific data categories
- Users can request deletion of data from specific time periods
- Full account deletion is supported

Exceptions apply only to:
- Data required to comply with legal obligations
- Data necessary to protect against fraud or security threats

#### Right to Restriction of Processing

Users can:
- Temporarily pause all non-essential logging
- Restrict processing to specific purposes
- Restrict cross-company data sharing

#### Right to Data Portability

The Privacy Dashboard supports:
- Data exports in machine-readable formats (JSON, CSV)
- Structured data that can be transferred to other systems
- Complete export of all user data

#### Right to Object

Users can:
- Object to specific processing purposes
- Opt out of particular data collection categories
- Manage granular consent preferences

### Technical Compliance Measures

#### Data Protection by Design and Default

- Privacy settings default to the most restrictive options
- Data minimization is applied to all collection processes
- Classification engine identifies and protects sensitive data
- Retention periods are enforced automatically

#### Record of Processing Activities

The system maintains detailed records of:
- Data collection purposes
- Categories of personal data
- Data retention periods
- Security measures
- Cross-border data transfers (if applicable)

#### Data Protection Impact Assessment

A comprehensive DPIA was conducted during system design to:
- Identify privacy risks
- Implement mitigating measures
- Document legal compliance
- Establish ongoing monitoring

## CCPA Compliance

### Notice at Collection

The system provides clear notice regarding:
- Categories of personal information collected
- Purposes for which information is used
- Whether information is sold or shared
- Retention periods

### Right to Know

California residents can access:
- Specific pieces of personal information collected
- Categories of sources from which information is collected
- Business purpose for collecting information
- Third parties with whom information is shared

### Right to Delete

California residents can request deletion of personal information with exceptions for:
- Completing transactions
- Security purposes
- Debugging
- Legal compliance

### Right to Opt-Out

California residents can opt out of:
- Sale of personal information
- Sharing of personal information for cross-context behavioral advertising
- Certain uses of sensitive personal information

### Limit on Sensitive Personal Information

The system:
- Clearly identifies sensitive personal information
- Provides additional controls for such information
- Limits use to what is necessary for service function

## Compliance Implementation Details

### Data Inventory & Classification

The system automatically classifies data into:

| Classification | Description | Example | Retention | Special Handling |
|---------------|-------------|---------|-----------|------------------|
| Non-Personal | Cannot identify individuals | Aggregate metrics | Long-term | Standard security |
| Pseudonymized | Indirect identifiers | Hashed user IDs | Medium-term | Enhanced security |
| Personal | Directly identifiable | Email addresses | Short-term | Strict access control |
| Sensitive | Special category data | Health information | Transient | Encryption at rest/transit |

### Consent Management

The system implements a tiered consent model:

1. **Essential**: Required for core functionality (cannot be disabled)
2. **Analytics**: Usage statistics and patterns
3. **Product Improvement**: Feature enhancement data
4. **AI Training**: Data used for training AI models
5. **Cross-Company Insights**: Anonymized cross-organization data
6. **Personalization**: User-specific customization data

Each tier:
- Has clear purpose descriptions
- Records consent timestamps
- Maintains consent history
- Implements immediate consent changes

### Data Access Controls

Access to logged data is restricted by:

1. **Role-Based Access Control**:
   - Regular users: Access only to their own data
   - Team admins: Access to team data
   - System admins: Limited access for maintenance

2. **Row-Level Security**:
   - Database-enforced access controls
   - Context-aware permission checks
   - Policy-based restrictions

3. **Data Minimization**:
   - Service-specific data views
   - Field-level access control
   - Purpose limitation enforcement

### International Data Transfers

For international operations, the system:
- Maps data flow across borders
- Implements Standard Contractual Clauses
- Conducts Transfer Impact Assessments
- Enforces regional data storage where required

### Privacy Documentation

The system generates and maintains:
- Privacy notices and policies
- Consent records
- Processing records
- Data access logs
- Breach notification procedures

## Compliance Monitoring & Maintenance

### Regular Audits

The system is audited:
- Quarterly for general compliance
- Annually by external privacy experts
- After significant system updates
- In response to regulatory changes

### Staff Training

Team members receive:
- Initial privacy training
- Annual refresher courses
- Role-specific compliance education
- Updates when regulations change

### Incident Response

The system includes:
- Data breach detection mechanisms
- 72-hour notification procedures
- Documentation templates for authorities
- Remediation protocols

## Third-Party Processors

For any third-party data processors, we:
- Conduct vendor privacy assessments
- Implement Data Processing Agreements
- Verify security measures
- Require breach notification commitments

## Conclusion

The Comprehensive Logging System is designed with privacy compliance as a foundational principle. This approach ensures that data collection and processing activities respect user privacy while still enabling valuable insights and improvements.

For specific compliance questions or concerns, please contact the Data Protection Officer at dpo@example.com.

================
File: docs/comprehensive-logging-system/DATA_MODELING.md
================
# Data Modeling: Comprehensive Logging System

## Overview

This document describes how logged data is processed, transformed, and used for model training while maintaining privacy. It outlines the data structures, processing pipelines, and modeling approaches utilized by the Comprehensive Logging System.

## Data Collection & Structure

### Data Categories

The system collects the following categories of data:

1. **User Interactions**: UI events, navigation patterns, feature usage
2. **Service Operations**: API calls, function execution, performance metrics
3. **AI Interactions**: Prompts, responses, feedback loops
4. **System Events**: Errors, warnings, diagnostics
5. **Business Events**: Task creation, idea generation, company activities

### Log Structure

Logs are stored with a consistent schema:

```typescript
interface LogEvent {
  id: string;
  user_id?: string;
  persona_id?: string;
  company_id?: string;
  event_type: string;
  event_source: string;
  component?: string;
  action: string;
  data: any; // JSONB in database
  metadata?: any; // JSONB in database
  data_classification: 'non_personal' | 'pseudonymized' | 'personal' | 'sensitive';
  retention_policy: 'transient' | 'short_term' | 'medium_term' | 'long_term';
  session_id?: string;
  client_info?: any;
  created_at: string;
}
```

### Data Relationships

The system maintains relationships between:

- Users and their actions
- Companies and their members
- Sessions and events
- Related actions within workflows

These relationships allow for contextual understanding of the data when used in model training.

## Data Processing Pipelines

### 1. Privacy Transformation Pipeline

Before data can be used for model training, it passes through privacy transformations:

```mermaid
graph TD
    A[Raw Logs] --> B[Consent Filtering]
    B --> C[Classification]
    C --> D[Privacy Transform]
    D -->|Non-personal| E[Aggregation]
    D -->|Pseudonymized| F[ID Replacement]
    D -->|Personal| G[Anonymization]
    D -->|Sensitive| H[Exclusion]
    E --> I[Training-Ready Data]
    F --> I
    G --> I
```

#### Privacy Transformation Steps

1. **Consent Filtering**: Exclude data for users who haven't consented to relevant processing
2. **Classification**: Apply automatic classification to identify data sensitivity
3. **Privacy Transform**: Apply appropriate transformation based on classification level
4. **Aggregation/Anonymization**: Remove identifiers while preserving patterns

### 2. Feature Extraction Pipeline

```mermaid
graph TD
    A[Privacy-Processed Logs] --> B[Temporal Grouping]
    B --> C[Pattern Recognition]
    C --> D[Feature Extraction]
    D --> E[Feature Vectorization]
    E --> F[Feature Storage]
    F --> G[Model Training Pipeline]
```

#### Feature Extraction Steps

1. **Temporal Grouping**: Organize data by time windows (sessions, days, weeks)
2. **Pattern Recognition**: Identify recurring patterns and sequences
3. **Feature Extraction**: Transform raw data into meaningful features
4. **Vectorization**: Convert features to numerical representations for models

### 3. Model Training Pipeline

```mermaid
graph TD
    A[Feature Vectors] --> B[Dataset Creation]
    B --> C[Dataset Splitting]
    C --> D[Training]
    C --> E[Validation]
    C --> F[Testing]
    D --> G[Model Evaluation]
    E --> G
    F --> G
    G --> H[Model Registration]
    H --> I[Deployment]
```

## Modeling Approaches

The system employs several modeling approaches depending on the use case:

### 1. Behavior Prediction Models

These models predict user behavior to enhance the application experience.

**Input Features**:
- Historical user actions
- Session context
- Feature engagement metrics
- Temporal patterns

**Model Types**:
- Sequence models (LSTM/GRU)
- Gradient boosting models
- Collaborative filtering

**Application**:
- Feature recommendation
- Usage pattern prediction
- Churn prediction
- Engagement optimization

### 2. Company Similarity Models

These models identify similarities between companies for cross-company insights.

**Input Features**:
- Company attributes (anonymized)
- Aggregate usage patterns
- Feature adoption timelines
- Growth metrics

**Model Types**:
- Clustering algorithms
- Similarity matrices
- Graph embedding models

**Application**:
- Benchmarking
- Best practice recommendations
- Industry trend analysis
- Growth pattern identification

### 3. Context Enhancement Models

These models build contextual understanding for improved personalization.

**Input Features**:
- User interaction history (pseudonymized)
- Content engagement metrics
- Session progression patterns
- Workflow completions

**Model Types**:
- Attention-based models
- Context-aware embeddings
- Temporal convolutional networks

**Application**:
- Content personalization
- Interface adaptation
- Contextual assistance
- Smart defaults

### 4. Resource Generation Models

These models create resources based on usage patterns.

**Input Features**:
- Successful workflow patterns
- Document templates and usage
- Common process sequences
- User feedback on resources

**Model Types**:
- Generative models
- Template-based generation
- Pattern recognition

**Application**:
- Template recommendation
- Process optimization
- Resource personalization
- Content generation

## Privacy-Preserving Machine Learning Techniques

The system implements several privacy-preserving techniques in its modeling:

### Differential Privacy

Applied to aggregate statistics to ensure individual users cannot be identified from results:

```typescript
function addDifferentialPrivacy(statistic: number, sensitivity: number, epsilon: number): number {
  // Add calibrated noise to protect privacy while maintaining statistical utility
  const scale = sensitivity / epsilon;
  const noise = laplacianNoise(scale);
  return statistic + noise;
}
```

### Federated Learning

Where appropriate, models are trained across distributed data without centralizing raw data:

1. Initial model is distributed to client instances
2. Local updates occur on client data
3. Model gradients (not data) are sent back to central system
4. Gradients are aggregated to improve the shared model

### K-Anonymity Enforcement

Ensures that any pattern in the data applies to at least k different users:

```typescript
function enforceKAnonymity(dataset: any[], k: number): any[] {
  // Group by quasi-identifiers
  const groups = groupByQuasiIdentifiers(dataset);
  
  // Filter out groups with fewer than k records
  return groups.filter(group => group.length >= k)
    .flatMap(group => group);
}
```

### Secure Multi-Party Computation

For sensitive operations across organizational boundaries:

1. Computation is split into multiple shares
2. Each party computes on encrypted shares
3. Results are combined without revealing inputs

## Model Evaluation & Improvement

### Metrics

Models are evaluated using:

1. **Performance Metrics**:
   - Accuracy, precision, recall for classification
   - RMSE, MAE for prediction
   - BLEU, ROUGE for generation

2. **Privacy Metrics**:
   - Privacy budget consumption
   - Membership inference resistance
   - Model inversion resistance

3. **Business Impact Metrics**:
   - User engagement lift
   - Task completion improvements
   - Resource utilization

### Feedback Loops

Models improve through several feedback mechanisms:

1. **Explicit Feedback**: User ratings and surveys
2. **Implicit Feedback**: Engagement with recommendations
3. **A/B Testing**: Controlled experiments
4. **Performance Monitoring**: Continuous evaluation

### Retraining Strategy

Models are retrained based on:

1. **Schedule-based**: Regular interval retraining
2. **Drift-based**: When input distribution changes
3. **Performance-based**: When metrics degrade
4. **Feature-based**: When new features are added

## Data Retention for Models

### Training Data Lifecycle

1. **Raw Data**: Retained according to classification-based retention policies
2. **Feature Vectors**: Retained with pseudonymized IDs
3. **Model Artifacts**: Retained for model lineage tracking
4. **Evaluation Results**: Retained for performance benchmarking

### Versioning

All data transformations and models are versioned to ensure:

1. Reproducibility of training
2. Audit trails for compliance
3. Rollback capabilities
4. Evolution tracking

## Integration with the Application

### Model Serving

Models are served through:

1. **Real-time API**: For interactive features
2. **Batch Processing**: For periodic insights
3. **Embedded Models**: For client-side predictions

### Feature Flagging

Model-driven features are controlled through feature flags:

1. **Gradual Rollout**: Staged enablement for users
2. **Fallback Mechanisms**: Safety nets for model failures
3. **A/B Testing**: Comparative analysis of model versions

## Monitoring & Observability

### Model Monitoring

Models are monitored for:

1. **Performance Drift**: Changes in accuracy or metrics
2. **Data Drift**: Changes in input distributions
3. **Prediction Monitoring**: Unusual patterns or outliers
4. **Resource Consumption**: CPU, memory, latency

### Alerts & Interventions

The system provides alerts for:

1. **Model Degradation**: Performance below thresholds
2. **Data Quality Issues**: Missing features or skewed distributions
3. **Privacy Concerns**: Potential exposure of sensitive patterns
4. **System Overloads**: High resource consumption

## Conclusion

The data modeling approach of the Comprehensive Logging System balances powerful insights with privacy protection. By carefully processing data through privacy-preserving pipelines before using it for model training, the system can deliver valuable features and insights without compromising user privacy.

For technical details on the implementation of these models, refer to the model specification documents in the developer documentation.

================
File: docs/comprehensive-logging-system/ENVIRONMENT_VARIABLES.md
================
# Environment Variables: Comprehensive Logging System

## Overview

This document describes the environment variables used by the Comprehensive Logging System, how they are configured, and best practices for managing them securely. The logging system relies on several environment variables to connect to external services and configure its behavior.

## Table of Contents

1. [Required Environment Variables](#required-environment-variables)
2. [Configuration Variables](#configuration-variables)
3. [Security Best Practices](#security-best-practices)
4. [Key Rotation Procedures](#key-rotation-procedures)
5. [Environment-Specific Configuration](#environment-specific-configuration)

## Required Environment Variables

### Supabase Configuration

The logging system uses Supabase as its primary data store. The following variables are required for Supabase integration:

```
VITE_SUPABASE_URL=https://your-project.supabase.co
VITE_SUPABASE_ANON_KEY=your-anon-key
```

- **VITE_SUPABASE_URL**: The URL of your Supabase project
- **VITE_SUPABASE_ANON_KEY**: The anonymous key for your Supabase project, used for authenticated operations

### OpenAI Configuration

For AI interactions that need to be logged, the system uses OpenAI's API:

```
VITE_OPENAI_API_KEY=your-openai-api-key
```

- **VITE_OPENAI_API_KEY**: Your OpenAI API key for making requests to OpenAI's services

### Feature Flag Service

Optional but recommended for controlling logging behaviors dynamically:

```
VITE_FEATURE_FLAGS_API_KEY=your-feature-flags-api-key
```

## Configuration Variables

These variables control how the logging system behaves:

### General Logging Configuration

```
VITE_LOGGING_ENABLED=true
VITE_LOGGING_VERBOSITY=info
VITE_LOGGING_SAMPLE_RATE=1.0
```

- **VITE_LOGGING_ENABLED**: Master switch to enable/disable all logging (`true`/`false`)
- **VITE_LOGGING_VERBOSITY**: Controls the level of detail in logs (`debug`, `info`, `warn`, `error`)
- **VITE_LOGGING_SAMPLE_RATE**: Controls what fraction of events get logged (0.0-1.0)

### Performance Monitoring

```
VITE_PERFORMANCE_MONITORING_ENABLED=true
VITE_PERFORMANCE_SAMPLE_RATE=0.1
```

- **VITE_PERFORMANCE_MONITORING_ENABLED**: Enable/disable performance monitoring
- **VITE_PERFORMANCE_SAMPLE_RATE**: What fraction of performance events to log (0.0-1.0)

### Privacy Settings

```
VITE_DEFAULT_CONSENT_ANALYTICS=false
VITE_DEFAULT_CONSENT_AI_TRAINING=false
VITE_PRIVACY_MODE=strict
```

- **VITE_DEFAULT_CONSENT_ANALYTICS**: Default setting for analytics consent
- **VITE_DEFAULT_CONSENT_AI_TRAINING**: Default setting for AI training consent
- **VITE_PRIVACY_MODE**: Controls privacy strictness (`relaxed`, `standard`, `strict`)

### Batch Configuration

```
VITE_BATCH_LOGGING_ENABLED=true
VITE_BATCH_SIZE=50
VITE_BATCH_INTERVAL_MS=5000
```

- **VITE_BATCH_LOGGING_ENABLED**: Enable/disable batch logging
- **VITE_BATCH_SIZE**: Maximum number of events in a batch
- **VITE_BATCH_INTERVAL_MS**: How often to flush the batch (milliseconds)

## Security Best Practices

### API Key Storage

#### Development Environment

For local development, store keys in a `.env` file that is not committed to source control:

1. Create a `.env` file in the project root
2. Add your environment variables in the format `KEY=value`
3. Add `.env` to your `.gitignore` file to prevent accidental commits

```bash
# .env example
VITE_SUPABASE_URL=https://your-project.supabase.co
VITE_SUPABASE_ANON_KEY=your-anon-key
VITE_OPENAI_API_KEY=your-openai-api-key
```

#### Production Environment

For production, use your cloud provider's secrets management service:

- **AWS**: Use AWS Secrets Manager or Parameter Store
- **GCP**: Use Google Secret Manager
- **Azure**: Use Azure Key Vault
- **Netlify/Vercel**: Use the environment variables configuration in your dashboard

Never store production API keys directly in:
- Source code
- Docker images
- CI/CD logs
- Shared team documents

### Least Privilege Principle

1. **Supabase**: Create dedicated service accounts with minimal permissions needed:
   ```sql
   -- Example of creating a restricted role in Supabase
   CREATE ROLE logging_service;
   GRANT INSERT ON system_logs TO logging_service;
   GRANT SELECT ON user_consent TO logging_service;
   ```

2. **OpenAI**: If possible, create specific API keys for logging purposes with appropriate rate limits

3. **Enforce scopes**: If the API supports scopes, request only the minimal scopes needed

### Encryption

1. **In-transit encryption**: All API communication should use HTTPS
2. **At-rest encryption**: Ensure your Supabase database has encryption at rest enabled
3. **Client-side encryption**: Consider encrypting extremely sensitive data before logging

## Key Rotation Procedures

Regular key rotation is a security best practice. Follow these steps to safely rotate keys:

### Supabase Key Rotation

1. Generate a new API key in the Supabase dashboard
2. Update the key in your environment variable storage (secrets manager)
3. Deploy the application with the new key
4. Monitor for errors after deployment
5. Once confirmed working, revoke the old key

### OpenAI Key Rotation

1. Create a new API key in the OpenAI dashboard
2. Update the key in your environment variables
3. Deploy the application
4. After confirming successful operation, revoke the old key

### Automated Rotation

For production environments, consider implementing automated key rotation:

```typescript
// Pseudocode for automated key rotation
async function rotateKeys() {
  try {
    // 1. Fetch new key from secret manager
    const newKey = await secretManager.getLatestKey('openai');
    
    // 2. Update application configuration (without restart)
    globalConfig.set('OPENAI_API_KEY', newKey);
    
    // 3. Log successful rotation (use old key, as new one is not verified yet)
    await loggingService.logSystemEvent(
      'key_rotation',
      'system',
      'key_updated',
      { service: 'openai', success: true }
    );
    
    // 4. Verify new key works
    const testResult = await testKey(newKey);
    if (!testResult.success) {
      // Rollback to previous key if test fails
      await rollbackKey('openai');
    }
  } catch (error) {
    // Handle rotation errors
    console.error('Key rotation failed:', error);
  }
}
```

## Environment-Specific Configuration

### Development

```bash
# .env.development
VITE_LOGGING_ENABLED=true
VITE_LOGGING_VERBOSITY=debug
VITE_LOGGING_DESTINATION=console
VITE_BATCH_LOGGING_ENABLED=false
VITE_PRIVACY_MODE=relaxed
```

Development environments should prioritize debugging and developer productivity:
- More verbose logging
- Console output for quick debugging
- Relaxed privacy for easier troubleshooting
- Real-time logging (no batching)

### Staging

```bash
# .env.staging
VITE_LOGGING_ENABLED=true
VITE_LOGGING_VERBOSITY=info
VITE_LOGGING_DESTINATION=database
VITE_BATCH_LOGGING_ENABLED=true
VITE_BATCH_SIZE=20
VITE_PRIVACY_MODE=standard
```

Staging environments should closely mirror production:
- Moderate verbosity
- Database storage for logs
- Batching enabled for performance testing
- Standard privacy controls

### Production

```bash
# .env.production
VITE_LOGGING_ENABLED=true
VITE_LOGGING_VERBOSITY=warn
VITE_LOGGING_DESTINATION=database
VITE_BATCH_LOGGING_ENABLED=true
VITE_BATCH_SIZE=50
VITE_PRIVACY_MODE=strict
VITE_PERFORMANCE_SAMPLE_RATE=0.1
```

Production environments prioritize performance and privacy:
- Limited verbosity (warnings and errors)
- Efficient batch processing
- Strict privacy enforcement
- Performance monitoring with sampling

### Testing

```bash
# .env.test
VITE_LOGGING_ENABLED=false
VITE_MOCK_LOGGING=true
```

Test environments should avoid external dependencies:
- Disable actual logging
- Use mocked logging services
- Prevent test data from polluting analytics

## Environment Variable Loading

The application uses Vite's environment variable handling:

```typescript
// Example of accessing environment variables
const isLoggingEnabled = import.meta.env.VITE_LOGGING_ENABLED === 'true';
const verbosity = import.meta.env.VITE_LOGGING_VERBOSITY || 'info';
const sampleRate = parseFloat(import.meta.env.VITE_LOGGING_SAMPLE_RATE || '1.0');

// Fallback pattern
const batchSize = parseInt(import.meta.env.VITE_BATCH_SIZE || '50', 10);
```

### Runtime Configuration

Some configuration can be adjusted at runtime via app settings:

```typescript
// Example of runtime configuration
class LoggingConfig {
  private settings: Map<string, any> = new Map();
  
  async initialize() {
    // Load from environment first
    this.settings.set('enabled', import.meta.env.VITE_LOGGING_ENABLED === 'true');
    this.settings.set('verbosity', import.meta.env.VITE_LOGGING_VERBOSITY || 'info');
    
    // Then try to load from app settings (database)
    try {
      const { data } = await supabase
        .from('app_settings')
        .select('key, value')
        .in('key', ['logging_enabled', 'logging_verbosity']);
        
      if (data) {
        data.forEach(setting => {
          this.settings.set(setting.key.replace('logging_', ''), setting.value);
        });
      }
    } catch (error) {
      console.error('Failed to load logging settings:', error);
      // Fall back to environment variables
    }
  }
  
  get(key: string, defaultValue?: any): any {
    return this.settings.has(key) ? this.settings.get(key) : defaultValue;
  }
  
  set(key: string, value: any): void {
    this.settings.set(key, value);
    
    // Optionally persist the setting
    supabase
      .from('app_settings')
      .upsert({ key: `logging_${key}`, value })
      .catch(error => console.error(`Failed to update ${key}:`, error));
  }
}

export const loggingConfig = new LoggingConfig();
```

## Troubleshooting Environment Variables

### Missing Variables

If logging isn't working as expected, check if environment variables are properly loaded:

```typescript
function debugEnvironmentVariables() {
  console.log('Environment Variables:');
  console.log('VITE_LOGGING_ENABLED:', import.meta.env.VITE_LOGGING_ENABLED);
  console.log('VITE_SUPABASE_URL:', import.meta.env.VITE_SUPABASE_URL?.substring(0, 10) + '...');
  console.log('VITE_SUPABASE_ANON_KEY:', import.meta.env.VITE_SUPABASE_ANON_KEY ? '[SET]' : '[NOT SET]');
  console.log('VITE_OPENAI_API_KEY:', import.meta.env.VITE_OPENAI_API_KEY ? '[SET]' : '[NOT SET]');
  console.log('VITE_LOGGING_VERBOSITY:', import.meta.env.VITE_LOGGING_VERBOSITY);
}
```

### Common Issues

1. **Variables not loading**: 
   - Ensure your `.env` file is in the correct location
   - For Vite, ensure variables are prefixed with `VITE_`
   - Restart the development server after changing `.env`

2. **Incorrect variable format**:
   - Boolean values should be strings: `VITE_FEATURE=true`
   - Check for typos and trailing spaces

3. **Production build not using variables**:
   - Ensure your CI/CD pipeline sets the environment variables
   - For static hosting, environment variables must be set at build time

By properly managing environment variables for your logging system, you ensure secure, configurable operation across all environments.

================
File: docs/comprehensive-logging-system/IMPLEMENTATION_GUIDE.md
================
# Implementation Guide: Comprehensive Logging System

## Overview

This guide provides practical instructions for developers to implement and integrate the Comprehensive Logging System into application components. It covers common integration patterns, best practices, and solutions to typical challenges.

## Table of Contents

1. [Integration Patterns](#integration-patterns)
2. [Common Use Cases](#common-use-cases)
3. [Best Practices](#best-practices)
4. [Troubleshooting](#troubleshooting)
5. [Performance Considerations](#performance-considerations)

## Integration Patterns

### Component-Level Integration

For React components, use the `useLogging` hook to access logging functionality:

```typescript
import React from 'react';
import { useLogging } from '../../lib/hooks/useLogging';

const MyComponent: React.FC = () => {
  const { logUIEvent } = useLogging();
  
  const handleButtonClick = () => {
    // Log the interaction
    logUIEvent('MyComponent', 'button_click', { buttonType: 'primary' });
    
    // Proceed with the action
    // ...
  };
  
  return (
    <button onClick={handleButtonClick}>
      Click Me
    </button>
  );
};
```

### Service-Level Integration

For services, import the logging service directly:

```typescript
import { loggingService } from '../logging.service';

class DataService {
  async fetchData(params: any) {
    try {
      // Log the data request
      await loggingService.logEvent({
        event_type: 'service_operation',
        event_source: 'data_service',
        action: 'fetch_data',
        data: { params }
      });
      
      // Proceed with data fetching
      const result = await apiClient.get('/data', { params });
      
      // Log successful response
      await loggingService.logEvent({
        event_type: 'service_operation',
        event_source: 'data_service',
        action: 'fetch_data_success',
        data: { 
          params,
          resultCount: result.data.length,
          responseTime: performance.now() - startTime
        }
      });
      
      return result.data;
    } catch (error) {
      // Log error
      await loggingService.logError(
        error,
        'DataService',
        { method: 'fetchData', params }
      );
      throw error;
    }
  }
}
```

### Router-Level Integration

To log navigation events, integrate with your router:

```typescript
// For React Router
import { useEffect } from 'react';
import { useLocation } from 'react-router-dom';
import { loggingService } from '../services/logging.service';

export const NavigationLogger = () => {
  const location = useLocation();
  
  useEffect(() => {
    const logNavigation = async () => {
      await loggingService.logEvent({
        event_type: 'navigation',
        event_source: 'router',
        action: 'page_view',
        data: {
          path: location.pathname,
          search: location.search,
          referrer: document.referrer
        }
      });
    };
    
    logNavigation();
  }, [location]);
  
  return null; // This is a utility component with no UI
};

// Then use it in your app
function App() {
  return (
    <Router>
      <NavigationLogger />
      {/* Rest of your app */}
    </Router>
  );
}
```

### API Client Integration

Wrap your API clients to automatically log all requests and responses:

```typescript
import axios from 'axios';
import { loggingService } from '../services/logging.service';

// Create a logged version of axios
const loggedAxios = axios.create({
  baseURL: 'https://api.example.com'
});

// Add request interceptor
loggedAxios.interceptors.request.use(
  async (config) => {
    const requestId = uuidv4();
    
    // Attach the request ID to the request for correlation
    config.headers['X-Request-ID'] = requestId;
    
    // Log the request
    await loggingService.logEvent({
      event_type: 'api',
      event_source: 'api_client',
      action: 'request',
      data: {
        request_id: requestId,
        method: config.method,
        url: config.url,
        params: config.params,
        // Don't log sensitive data in request body
        has_body: !!config.data
      }
    });
    
    // Store the start time for performance measurement
    config.metadata = {
      startTime: performance.now(),
      requestId
    };
    
    return config;
  },
  (error) => {
    loggingService.logError(error, 'ApiClient', { stage: 'request' });
    return Promise.reject(error);
  }
);

// Add response interceptor
loggedAxios.interceptors.response.use(
  async (response) => {
    const { config } = response;
    const duration = performance.now() - config.metadata.startTime;
    
    // Log the successful response
    await loggingService.logEvent({
      event_type: 'api',
      event_source: 'api_client',
      action: 'response',
      data: {
        request_id: config.metadata.requestId,
        method: config.method,
        url: config.url,
        status: response.status,
        duration_ms: duration,
        response_size: JSON.stringify(response.data).length
      }
    });
    
    return response;
  },
  async (error) => {
    if (error.config) {
      const { config } = error;
      const duration = performance.now() - config.metadata.startTime;
      
      // Log the error response
      await loggingService.logEvent({
        event_type: 'api',
        event_source: 'api_client',
        action: 'response_error',
        data: {
          request_id: config.metadata.requestId,
          method: config.method,
          url: config.url,
          status: error.response?.status,
          error_message: error.message,
          duration_ms: duration
        }
      });
    } else {
      // Network error or other error without config
      await loggingService.logError(error, 'ApiClient', { stage: 'response' });
    }
    
    return Promise.reject(error);
  }
);

export default loggedAxios;
```

## Common Use Cases

### Logging Form Submissions

```typescript
import { useLogging } from '../../lib/hooks/useLogging';

const ContactForm = () => {
  const { logFormSubmission } = useLogging();
  const [formData, setFormData] = useState({
    name: '',
    email: '',
    message: ''
  });
  
  const handleSubmit = async (e) => {
    e.preventDefault();
    
    // Log the form submission, excluding sensitive data
    await logFormSubmission('contact_form', {
      has_name: !!formData.name,
      has_email: !!formData.email,
      message_length: formData.message.length
    });
    
    // Process the form submission
    // ...
  };
  
  return (
    <form onSubmit={handleSubmit}>
      {/* Form fields */}
    </form>
  );
};
```

### Logging AI/LLM Interactions

```typescript
import { loggingService } from '../services/logging.service';
import { openaiClient } from '../openai-client';

async function generateSuggestions(prompt: string, options = {}) {
  // Log the AI request
  const interactionId = await loggingService.logAIInteraction(
    'generate_suggestions_request',
    {
      model: options.model || 'gpt-4',
      prompt_length: prompt.length,
      prompt_type: 'suggestion_generation',
      options
    }
  );
  
  try {
    const startTime = performance.now();
    
    // Make the AI request
    const response = await openaiClient.chat.completions.create({
      model: options.model || 'gpt-4',
      messages: [{ role: 'user', content: prompt }],
      temperature: options.temperature || 0.7,
      max_tokens: options.maxTokens || 500
    });
    
    const duration = performance.now() - startTime;
    
    // Log the successful AI response
    await loggingService.logAIInteraction(
      'generate_suggestions_response',
      {
        interaction_id: interactionId,
        duration_ms: duration,
        token_usage: response.usage,
        suggestion_count: response.choices.length,
        content_length: response.choices[0]?.message?.content?.length || 0,
        status: 'success'
      }
    );
    
    return response.choices[0]?.message?.content || '';
  } catch (error) {
    // Log the AI error
    await loggingService.logAIInteraction(
      'generate_suggestions_error',
      {
        interaction_id: interactionId,
        error_message: error.message,
        status: 'error'
      }
    );
    
    throw error;
  }
}
```

### Logging User Preferences

```typescript
import { loggingService } from '../services/logging.service';

class PreferenceService {
  async updatePreferences(userId: string, preferences: UserPreferences) {
    try {
      // Log the preference update
      await loggingService.logEvent({
        event_type: 'user_preference',
        event_source: 'preference_service',
        user_id: userId,
        action: 'update_preferences',
        data: {
          preference_types: Object.keys(preferences),
          has_theme: 'theme' in preferences,
          has_notifications: 'notifications' in preferences,
          // Don't log actual values for privacy
        }
      });
      
      // Update preferences in database
      // ...
      
      return { success: true };
    } catch (error) {
      await loggingService.logError(error, 'PreferenceService', {
        method: 'updatePreferences',
        user_id: userId
      });
      
      throw error;
    }
  }
}
```

### Logging Performance Metrics

```typescript
import { loggingService } from '../services/logging.service';

// Create a performance observer
const performanceObserver = new PerformanceObserver((list) => {
  list.getEntries().forEach(async (entry) => {
    // Log the performance entry
    await loggingService.logPerformance(
      entry.name,
      entry.duration,
      'ms',
      {
        entry_type: entry.entryType,
        start_time: entry.startTime,
        // Add additional context for specific metrics
        ...(entry.name.startsWith('api-') ? {
          api_endpoint: entry.name.substring(4)
        } : {})
      }
    );
  });
});

// Start observing different performance metrics
performanceObserver.observe({ entryTypes: ['measure'] });

// Example of measuring and logging performance
export function measurePerformance(operation: string, callback: () => any) {
  const startMark = `${operation}-start`;
  const endMark = `${operation}-end`;
  
  performance.mark(startMark);
  const result = callback();
  performance.mark(endMark);
  
  // Create the performance measure
  performance.measure(operation, startMark, endMark);
  
  return result;
}

// Usage example
measurePerformance('render-component', () => {
  // Component rendering logic
});
```

## Best Practices

### Sensitive Data Handling

1. **Never log sensitive data directly**:
   - Passwords, authentication tokens, API keys
   - Personal identifiable information (PII)
   - Financial information, health records
   - User-generated content without consent

2. **Use data masking techniques**:
   ```typescript
   // Instead of this
   logger.log({userId, email, address, password});
   
   // Do this
   logger.log({
     userId, 
     hasEmail: !!email,
     addressCountry: address?.country,
     // Never log passwords
   });
   ```

3. **Filter sensitive fields**:
   ```typescript
   const sensitiveFields = ['password', 'token', 'key', 'secret', 'ssn', 'credit'];
   
   function filterSensitiveData(data) {
     if (!data) return data;
     
     const filtered = {...data};
     
     Object.keys(filtered).forEach(key => {
       if (sensitiveFields.some(field => key.toLowerCase().includes(field))) {
         filtered[key] = '[REDACTED]';
       } else if (typeof filtered[key] === 'object') {
         filtered[key] = filterSensitiveData(filtered[key]);
       }
     });
     
     return filtered;
   }
   
   // Usage
   logger.log(filterSensitiveData(userData));
   ```

### Log Categorization

Correctly categorize logs to enable better filtering and analysis:

1. **Use consistent event types**:
   - `user_action`: User-initiated interactions
   - `system_event`: System-level operations
   - `api`: API calls (both internal and external)
   - `error`: Errors and exceptions
   - `performance`: Performance measurements
   - `ai_interaction`: AI model interactions

2. **Include context information**:
   ```typescript
   await loggingService.logEvent({
     event_type: 'user_action',
     event_source: 'idea_generator',
     component: 'IdeaForm',
     action: 'submit',
     data: { /* action data */ },
     metadata: {
       workflow_step: 'ideation',
       user_role: userRole,
       feature_flags: activeFeatures
     }
   });
   ```

3. **Use hierarchical naming for actions**:
   - `resource.action`: e.g., `idea.create`, `profile.update`
   - `workflow.step`: e.g., `onboarding.step1`, `checkout.payment`

### Batch Logging

For high-frequency events, use batch logging to reduce overhead:

```typescript
class BatchLogger {
  private queue: LogEvent[] = [];
  private maxBatchSize: number;
  private flushInterval: number;
  private timer: NodeJS.Timeout | null = null;
  
  constructor(maxBatchSize = 50, flushIntervalMs = 5000) {
    this.maxBatchSize = maxBatchSize;
    this.flushInterval = flushIntervalMs;
    this.startTimer();
  }
  
  private startTimer() {
    this.timer = setInterval(() => this.flush(), this.flushInterval);
  }
  
  async log(event: LogEvent) {
    this.queue.push(event);
    
    if (this.queue.length >= this.maxBatchSize) {
      await this.flush();
    }
  }
  
  async flush() {
    if (this.queue.length === 0) return;
    
    const batch = [...this.queue];
    this.queue = [];
    
    try {
      await loggingService.batchLog(batch);
    } catch (error) {
      console.error('Error flushing log batch:', error);
      // Re-add critical logs back to the queue
      const criticalLogs = batch.filter(log => log.event_type === 'error' || log.metadata?.critical);
      this.queue.unshift(...criticalLogs);
    }
  }
  
  destroy() {
    if (this.timer) {
      clearInterval(this.timer);
      this.timer = null;
    }
    this.flush(); // Flush remaining logs
  }
}

// Usage
const batchLogger = new BatchLogger();

// In high-frequency component
function TrackMouseMovement() {
  useEffect(() => {
    const handleMouseMove = (e) => {
      batchLogger.log({
        event_type: 'user_action',
        event_source: 'ui',
        action: 'mouse_move',
        data: { x: e.clientX, y: e.clientY }
      });
    };
    
    window.addEventListener('mousemove', handleMouseMove);
    return () => {
      window.removeEventListener('mousemove', handleMouseMove);
    };
  }, []);
  
  return null;
}
```

## Troubleshooting

### Missing Logs

If logs are not appearing as expected:

1. **Check Consent Settings**:
   ```typescript
   // Check if the user has consented to this type of logging
   const hasConsent = await privacyService.checkConsent(userId, 'analytics');
   console.debug('Analytics consent status:', hasConsent);
   ```

2. **Verify Classification**:
   ```typescript
   // Check how data is being classified
   const { classification, retentionPolicy } = await privacyService.classifyData(data);
   console.debug('Data classification:', classification, retentionPolicy);
   ```

3. **Inspect Network Requests**:
   - Check for errors in network requests to logging endpoints
   - Verify batch logging flush events are completing

### Handling Rate Limits

If hitting rate limits with logging services:

```typescript
// Implement exponential backoff
async function logWithBackoff(event, maxRetries = 3) {
  let retries = 0;
  let delay = 1000; // Start with 1 second delay
  
  while (retries < maxRetries) {
    try {
      return await loggingService.logEvent(event);
    } catch (error) {
      if (error.code === 'RATE_LIMIT_EXCEEDED') {
        retries++;
        
        if (retries >= maxRetries) throw error;
        
        // Exponential backoff with jitter
        const jitter = Math.random() * 0.3 + 0.85; // 0.85-1.15
        await sleep(delay * jitter);
        
        delay *= 2; // Double the delay for next attempt
      } else {
        throw error; // Not a rate limit error, rethrow
      }
    }
  }
}
```

### Debugging Log Processing

For issues with log processing or model training:

1. **Enable debug mode**:
   ```typescript
   // Set debug mode in your environment variables
   // LOGGING_DEBUG=true
   
   // In your code
   if (process.env.LOGGING_DEBUG === 'true') {
     console.debug('Logging event:', event);
   }
   ```

2. **Add correlation IDs**:
   ```typescript
   const correlationId = uuidv4();
   
   // Add correlation ID to related logs
   await loggingService.logEvent({
     // Other fields...
     metadata: {
       correlation_id: correlationId
     }
   });
   
   // In another part of the code
   await loggingService.logEvent({
     // Other fields...
     metadata: {
       correlation_id: correlationId
     }
   });
   ```

## Performance Considerations

### Minimize Blocking

Use asynchronous, non-blocking logging:

```typescript
// Don't block the UI thread for logging
const handleClick = () => {
  // Trigger logging but don't await it
  loggingService.logUserAction('button_click', { id: buttonId })
    .catch(err => console.error('Logging error:', err));
  
  // Continue with user action immediately
  navigate('/next-page');
};
```

### Use Sampling for High-Volume Events

For high-volume events, implement sampling:

```typescript
function shouldSampleEvent(eventName, samplingRate = 0.1) {
  // Always log errors and critical events
  if (eventName.includes('error') || eventName.includes('critical')) {
    return true;
  }
  
  // Sample other high-volume events
  return Math.random() < samplingRate;
}

// Usage
if (shouldSampleEvent('mouse_move', 0.05)) {
  loggingService.logEvent({
    event_type: 'user_action',
    action: 'mouse_move',
    // ...
  });
}
```

### Optimize Payload Size

Minimize the size of logged data:

```typescript
// Trim large objects before logging
function trimLargeObject(obj, maxDepth = 3, currentDepth = 0) {
  if (currentDepth >= maxDepth) return '[Truncated]';
  
  if (Array.isArray(obj)) {
    if (obj.length > 10) {
      return obj.slice(0, 10).map(item => 
        typeof item === 'object' && item !== null
          ? trimLargeObject(item, maxDepth, currentDepth + 1)
          : item
      ).concat([`... and ${obj.length - 10} more items`]);
    }
    
    return obj.map(item => 
      typeof item === 'object' && item !== null
        ? trimLargeObject(item, maxDepth, currentDepth + 1)
        : item
    );
  }
  
  if (typeof obj === 'object' && obj !== null) {
    const result = {};
    for (const [key, value] of Object.entries(obj)) {
      result[key] = typeof value === 'object' && value !== null
        ? trimLargeObject(value, maxDepth, currentDepth + 1)
        : value;
    }
    return result;
  }
  
  return obj;
}

// Usage
loggingService.logEvent({
  // ...
  data: trimLargeObject(largeData)
});
```

### Load Testing

Test your logging system under load:

```typescript
// Simple load test function
async function loadTestLogging(events = 1000, concurrency = 10) {
  console.time('loadTestLogging');
  
  const batches = [];
  for (let i = 0; i < events / concurrency; i++) {
    const batch = [];
    for (let j = 0; j < concurrency; j++) {
      batch.push(loggingService.logEvent({
        event_type: 'test',
        event_source: 'load_test',
        action: 'test_event',
        data: {
          test_id: i * concurrency + j,
          timestamp: Date.now()
        }
      }));
    }
    batches.push(Promise.all(batch));
  }
  
  await Promise.all(batches);
  console.timeEnd('loadTestLogging');
}
```

### Offline Logging

Implement offline logging for mobile or unreliable connections:

```typescript
class OfflineLogger {
  private storageKey = 'offline_logs';
  private syncing = false;
  
  constructor() {
    // Try to sync logs when coming back online
    window.addEventListener('online', () => this.syncLogs());
  }
  
  async log(event) {
    if (navigator.onLine) {
      // Online, log directly
      return loggingService.logEvent(event);
    } else {
      // Offline, store the log
      this.storeLog(event);
      return Promise.resolve();
    }
  }
  
  private storeLog(event) {
    try {
      const storedLogs = JSON.parse(localStorage.getItem(this.storageKey) || '[]');
      storedLogs.push({
        ...event,
        stored_at: Date.now()
      });
      
      // Cap the number of stored logs to prevent storage overflow
      if (storedLogs.length > 1000) {
        storedLogs.shift(); // Remove oldest log
      }
      
      localStorage.setItem(this.storageKey, JSON.stringify(storedLogs));
    } catch (error) {
      console.error('Error storing offline log:', error);
    }
  }
  
  async syncLogs() {
    if (this.syncing || !navigator.onLine) return;
    
    this.syncing = true;
    
    try {
      const storedLogs = JSON.parse(localStorage.getItem(this.storageKey) || '[]');
      
      if (storedLogs.length === 0) {
        this.syncing = false;
        return;
      }
      
      // Process logs in batches
      while (storedLogs.length > 0) {
        const batch = storedLogs.splice(0, 50);
        await loggingService.batchLog(batch);
      }
      
      // Clear the stored logs
      localStorage.setItem(this.storageKey, '[]');
    } catch (error) {
      console.error('Error syncing offline logs:', error);
    } finally {
      this.syncing = false;
    }
  }
}

// Usage
const offlineLogger = new OfflineLogger();
offlineLogger.log({
  event_type: 'user_action',
  action: 'button_click',
  // ...
});
```

By following these implementation guidelines, you can effectively integrate the Comprehensive Logging System throughout your application while maintaining high performance and respecting user privacy.

================
File: docs/comprehensive-logging-system/IMPLEMENTATION_PLAN.md
================
# Implementation Plan: Comprehensive Logging System

## Overview

This document outlines the phased approach for implementing the Comprehensive Logging System. The plan is designed to deliver value incrementally while maintaining compatibility with existing systems and ensuring proper privacy compliance.

## Phase 1: Foundation (Weeks 1-2)

### 1.1 Database Setup

- Create Supabase migration for logging tables
- Set up indices for performance optimization
- Implement Row-Level Security policies
- Create initial data classification rules

### 1.2 Core Services

- Implement LoggingService
- Implement PrivacyService
- Add database initialization scripts
- Set up automated testing framework

### 1.3 Basic Integration

- Create LoggingContext provider
- Add basic wiring to App.tsx
- Implement essential logging for critical paths
- Configure production error handling

### 1.4 Deployment Pipeline

- Set up CI/CD for logging system
- Implement feature flags for gradual rollout
- Create monitoring dashboard
- Configure alerts for system health

## Phase 2: Privacy Infrastructure (Weeks 3-4)

### 2.1 Consent Management

- Implement ConsentManager component
- Create consent database operations
- Build UI for consent settings
- Add consent verification to logging flow

### 2.2 Data Classification Engine

- Implement classification algorithms
- Create pattern matching for PII detection
- Build retention policy enforcement
- Set up automated classification testing

### 2.3 Privacy Dashboard

- Create PrivacyDashboard component
- Implement data export functionality
- Add deletion request handling
- Build data summary visualizations

### 2.4 Privacy Compliance

- Implement GDPR-compliant processes
- Add CCPA-specific features
- Create privacy policy documentation
- Set up compliance monitoring

## Phase 3: Comprehensive Integration (Weeks 5-6)

### 3.1 UI Component Integration

- Add logging to all form components
- Implement navigation tracking
- Add interaction logging to UI elements
- Create specialized logging hooks

### 3.2 Service Layer Integration

- Wrap all API calls with logging
- Add context preservation across service calls
- Implement error tracking
- Add performance metrics logging

### 3.3 OpenAI Integration

- Create OpenAI client wrapper
- Add prompt and response logging
- Implement feedback loop tracking
- Build model performance metrics

### 3.4 Authentication Integration

- Add session tracking
- Implement persona-based logging
- Create company context integration
- Build user journey tracking

## Phase 4: Data Processing Pipeline (Weeks 7-8)

### 4.1 Data Aggregation

- Implement ETL processes for log data
- Create aggregation jobs
- Build anonymization pipeline
- Set up scheduled processing

### 4.2 Analytics Infrastructure

- Implement AnalyticsService
- Create data transformation functions
- Build model training infrastructure
- Implement insight generation algorithms

### 4.3 Reporting

- Create admin analytics dashboard
- Build report generation functionality
- Implement export capabilities
- Add visualization components

### 4.4 Resource Generation

- Create template generation based on patterns
- Implement recommendation algorithms
- Build feedback mechanisms
- Add generated resource management

## Phase 5: Testing & Refinement (Weeks 9-10)

### 5.1 Performance Testing

- Conduct load testing
- Optimize database operations
- Refine batching strategies
- Implement scaling improvements

### 5.2 Security Audit

- Conduct privacy impact assessment
- Perform penetration testing
- Verify compliance requirements
- Address security findings

### 5.3 User Testing

- Conduct usability testing for privacy controls
- Gather feedback on consent interfaces
- Test data dashboard with users
- Refine UX based on feedback

### 5.4 Final Deployment

- Release logging system to production
- Monitor system performance
- Gather initial metrics
- Address any operational issues

## Dependencies & Risks

### Dependencies

1. **Supabase Availability**: System relies on Supabase for data storage
2. **OpenAI API Stability**: Changes to OpenAI's API may require updates
3. **React Component Integration**: Requires coordination across the frontend codebase
4. **Legal Review**: Privacy features should be reviewed by legal counsel

### Risks

1. **Performance Impact**: Logging may impact application performance
   - Mitigation: Asynchronous logging, batching, and performance optimization
   
2. **Data Volume Growth**: Log data may grow rapidly
   - Mitigation: Implement retention policies and archiving strategies
   
3. **Privacy Compliance Changes**: Regulations may change
   - Mitigation: Design flexible architecture that can adapt to new requirements
   
4. **User Resistance**: Users may disable consent options
   - Mitigation: Clear communication about benefits and transparent data usage

## Success Metrics

The implementation will be considered successful when:

1. **Complete Coverage**: All system interactions are properly logged
2. **Performance Goals**: Logging adds <50ms overhead to operations
3. **Privacy Compliance**: System meets all GDPR and CCPA requirements
4. **User Control**: Users can easily manage their privacy settings
5. **Data Utility**: Captured data successfully drives model training and insights

## Maintenance Plan

After initial implementation, the following ongoing activities will be required:

1. **Regular Audits**: Quarterly privacy compliance reviews
2. **Data Quality Monitoring**: Ongoing validation of log data quality
3. **Pattern Updates**: Regular updates to data classification patterns
4. **Performance Monitoring**: Continuous monitoring of system performance
5. **User Feedback**: Collection and incorporation of user feedback

================
File: docs/comprehensive-logging-system/INFINITE_RECURSION_FIX.md
================
# Infinite Recursion Fix

This document explains the fix for the infinite recursion issue that was occurring in the company_members policy and in feature extraction.

## Problem Overview

We were encountering two related issues:

1. **Company Members Recursion**: When attempting to access the Dashboard, we encountered the error:
```
App.tsx:67 Error fetching logs for feature extraction: {code: '42P17', details: null, hint: null, message: 'infinite recursion detected in policy for relation "company_members"'}
```

2. **Feature Extraction Duplicate Key Conflicts**: Extraction of features for model training was encountering conflicts due to non-unique timestamps being used as part of feature names.

## Root Causes

### Company Members Recursion

The Row Level Security (RLS) policy for `company_members` was causing infinite recursion because:

1. The policy was checking if the user belongs to a company.
2. To check company membership, it was querying the `company_members` table.
3. This query triggered the same policy again, leading to infinite recursion.

### Feature Extraction Conflicts

The feature extraction process was:
1. Using timestamps for feature name uniqueness
2. But multiple features could be created within the same millisecond
3. This would trigger unique constraint violations because the timestamp wasn't granular enough

## Solutions Implemented

### 1. Company Access Service

We created a new service (`company-access.service.ts`) that:

- Uses a secure RPC function to get company memberships without triggering recursion
- Provides multiple fallback strategies
- Implements proper error handling and timeouts

```typescript
// Key implementation: Using the RPC instead of direct query
const secureCheckPromise = supabase.rpc(
  'fetch_company_context_securely',
  { 
    p_user_id: userId,
    p_company_id: null // null will return all companies
  }
);
```

### 2. Dashboard Integration

Updated `Dashboard.tsx` to:
- Use the company access service instead of direct queries
- Import the service dynamically to avoid circular imports
- Handle edge cases and errors gracefully

### 3. Feature Extraction Fix

Enhanced `model-training.service.ts` with multiple uniqueness factors:
- Timestamp with milliseconds
- Random UUID segments 
- Small random delays between operations (1-15ms)
- Consistent creation timestamps

```typescript
// Key improvements
const now = new Date();
const timestamp = now.getTime();
const uniqueSuffix = uuidv4().substring(0, 8);
const featureName = `${featureSet}_feature_${timestamp}_${uniqueSuffix}`;
```

### 4. Database Changes

A migration script was created (`20250320062500_fix_company_members_recursion.sql`) that:
- Creates a secure RPC function that bypasses RLS
- Modifies the policy to prevent recursion

## Verification

We've created verification scripts to test both fixes:

1. `simple-recursion-verify.js` - Tests the company members recursion fix
2. `verify-recursion-and-features-fix.js` - Tests both fixes comprehensively

## How to Apply & Test the Fix

### Apply the Fix

Run the database migration:

```bash
node scripts/run-company-members-fix.js
```

### Test the Fix

Run the verification script:

```bash
node scripts/simple-recursion-verify.js
```

## Additional Notes

- The fixes have been implemented in a way that provides fallbacks if the primary methods fail
- Error handling has been added at multiple levels to prevent cascading failures
- Timing measurements have been added to help diagnose any performance issues
- The feature extraction fix introduces a small random delay, but this is negligible compared to the reliability improvement

## Contributors

This fix was implemented as part of addressing the critical error in the logging and company access systems.

================
File: docs/comprehensive-logging-system/MODEL_TRAINING_GUIDE.md
================
# Model Training Data Collection Guide

This guide describes how to utilize the comprehensive logging system for gathering high-quality training data for ML models.

## Overview

Our application now incorporates an enhanced logging system designed specifically for capturing detailed data that can be used to train machine learning models. The system logs user interactions, AI responses, system events, and business logic operations with rich contextual information, enabling better model training and evaluation.

## Key Components

1. **Enhanced Type Definitions**
   - Updated `logging.types.ts` with all necessary interfaces
   - Added support for automatic and neutral feedback types
   - Added data classification and retention policy fields
   
2. **Database Schema**
   - Created comprehensive tables for storing log data
   - Enhanced model_feedback table with additional feedback types
   - Implemented data retention policies and privacy controls

3. **Logging Hooks**
   - `useLogging`: Basic logging capabilities
   - `useEnhancedLogging`: Detailed contextual logging for model training
   - `useCentralizedLogging`: Central hook that integrates both systems

4. **LoggingProvider Component**
   - React context provider for app-wide logging access
   - Automatic session tracking
   - Component lifecycle logging

## Using the Logging System for Model Training

### 1. Capturing User Interactions with Context

```typescript
const { logAction } = useCentralizedLogging();

// Log a user action with detailed context
logAction(
  'button_click',
  'IdeaGenerator',
  {
    idea_id: idea.id,
    idea_title: idea.title,
    element: 'generate_button',
    step: 'initialization'
  },
  {
    training_priority: 'high',
    workflow: 'idea_generation'
  }
);
```

### 2. Logging AI Interactions

```typescript
const { logAIInteraction } = useCentralizedLogging();

// Log AI interaction for model training
logAIInteraction(
  'idea_generation',
  {
    model: 'gpt-4',
    prompt: userPrompt,
    response: aiResponse,
    tokens: 1250,
    idea_id: generatedIdea.id
  },
  {
    training_priority: 'high',
    domain: 'business_ideas'
  }
);
```

### 3. Recording Model Feedback

```typescript
const { recordModelFeedback } = useCentralizedLogging();

// Record explicit user feedback
recordModelFeedback(
  'idea-generation-model-v2',
  promptData,
  responseData,
  'positive', // or 'negative', 'correction', 'suggestion', 'automatic', 'neutral'
  { quality: 4.5, relevance: 5, creativity: 4 }
);

// Record automatic feedback based on user behavior
recordModelFeedback(
  'idea-generation-model-v2',
  promptData,
  responseData,
  'automatic',
  { 
    timeSpentViewing: 45,
    savedToFavorites: true,
    furtherRefined: true
  }
);
```

### 4. Logging Business Logic

```typescript
const { logBusinessLogic } = useCentralizedLogging();

// Log business decision points and rule applications
logBusinessLogic(
  'apply_rules',
  'idea_validation',
  {
    rules_applied: ['market_validation', 'feasibility_check'],
    decision_points: {
      market_size: 'large',
      competition: 'medium',
      feasibility: 'high'
    },
    outcome: 'approved'
  }
);
```

### 5. Logging Data Operations

```typescript
const { logDataOperation } = useCentralizedLogging();

// Log data operations for tracking data provenance
logDataOperation(
  'create',
  'ideas',
  {
    entity_id: idea.id,
    fields: ['title', 'description', 'market_size'],
    user_generated: true
  }
);
```

## Data Classification and Retention

All logged data includes classification and retention policy information:

- **Data Classification**:
  - `non_personal`: No personally identifying information
  - `pseudonymized`: Contains indirect identifiers
  - `personal`: Contains personal information
  - `sensitive`: Contains sensitive personal information

- **Retention Policies**:
  - `transient`: Deleted after processing (max 24 hours)
  - `short_term`: Kept for 30 days
  - `medium_term`: Kept for 6 months
  - `long_term`: Kept for 2 years

## Automatic Anonymization

The system automatically applies anonymization or pseudonymization based on retention policies. This ensures:

1. Model training data is properly sanitized
2. User privacy is protected
3. Regulatory compliance requirements are met

## New Database Migration Changes

We recently added support for additional feedback types to allow more nuanced model training:

- `automatic`: Feedback generated by the system based on user behavior
- `neutral`: Neither positive nor negative feedback

The migration script `20250316212000_update_model_feedback_types.sql` updates the database schema to support these new feedback types. Run it using:

```bash
node scripts/run-model-feedback-update.js
```

## Best Practices

1. **Always include context**: Add relevant business and user context to logs
2. **Respect privacy settings**: Check user consent before logging for AI training
3. **Classify data properly**: Set appropriate classification and retention policies
4. **Capture both positive and negative examples**: Log both successful and failed interactions
5. **Include timestamps**: Always include timestamps for temporal analysis

================
File: docs/comprehensive-logging-system/OVERVIEW.md
================
# Overview: Comprehensive Logging System

## Introduction

The Comprehensive Logging System is designed to capture all relevant system interactions, user behaviors, and data points across the application. This data serves multiple purposes:

1. Training machine learning models to predict user behavior
2. Creating personalized experiences through context awareness
3. Identifying cross-company patterns and insights
4. Generating resources and recommendations based on usage patterns
5. Improving the application through data-driven decisions

All of these objectives are achieved while maintaining strict privacy compliance with regulations like GDPR and CCPA.

## System Philosophy

The system is built on three core principles:

1. **Privacy by Design**: Privacy considerations are embedded into every aspect of the system, not added as an afterthought.
2. **Maximum Utility, Minimum Exposure**: The system extracts maximum value from data while minimizing exposure of personal information.
3. **User Control**: Users maintain control over their data through granular consent options and privacy tools.

## Core Components

### 1. Data Collection Layer

The data collection layer captures events from multiple sources:

- **UI Interactions**: User clicks, form submissions, navigation patterns
- **Service Calls**: API requests, service usage, function calls
- **AI Interactions**: Prompts, responses, feedback loops
- **System Events**: Errors, performance metrics, service health

Each event is captured with appropriate context, including user information (when permitted), session data, and environmental factors.

### 2. Privacy Management Layer

The privacy management layer ensures all data is handled according to regulations and user preferences:

- **Consent Management**: Tracking and enforcing user consent preferences
- **Data Classification**: Categorizing data based on sensitivity
- **Anonymization**: Removing or obscuring identifiable information
- **Retention Policies**: Enforcing appropriate data lifespans

This layer acts as a gateway, ensuring that only properly consented, appropriately classified data moves through the system.

### 3. Data Processing Layer

The data processing layer transforms raw logs into valuable insights:

- **Aggregation**: Combining data points to identify patterns
- **Normalization**: Standardizing data formats for analysis
- **Feature Extraction**: Identifying relevant characteristics
- **Model Training**: Preparing data for machine learning models

Different processing pipelines handle data differently based on privacy requirements and intended use.

### 4. Application Layer

The application layer puts processed data to use:

- **Predictive Models**: Forecasting user behavior and needs
- **Recommendation Engines**: Suggesting features and resources
- **Contextual Enhancement**: Improving AI with historical context
- **Resource Generation**: Creating content based on patterns

## Data Flows

### 1. Cross-Company Insights Flow

This flow generates insights across different companies while preserving privacy:

```
Raw Data → Privacy Filter → Anonymization → Aggregation → Pattern Recognition → Cross-Company Models
```

### 2. Company-Specific Context Flow

This flow enhances company-specific experiences:

```
Raw Data → Consent Verification → Pseudonymization → Company Context Building → Personalization Models
```

### 3. Resource Generation Flow

This flow creates useful resources based on anonymized patterns:

```
Raw Data → Privacy Filter → Anonymization → Pattern Detection → Template Building → Resource Generation
```

## User Experience

From the user perspective, the Comprehensive Logging System manifests in three key areas:

1. **Consent Management**: Users can configure exactly how their data is used through granular consent options.
2. **Privacy Dashboard**: Users can view what data has been collected and how it's being used.
3. **Enhanced Features**: Users benefit from improved recommendations, more contextual AI interactions, and automatically generated resources.

## Business Value

The Comprehensive Logging System delivers substantial business value:

1. **Data-Driven Improvement**: Continuous product enhancement based on actual usage patterns
2. **Model Training**: Better AI models through comprehensive training data
3. **Personalization**: More relevant, contextual experiences for users
4. **Compliance**: Built-in adherence to privacy regulations
5. **Resource Efficiency**: Automated generation of templates and resources

## Key Challenges and Solutions

### 1. Privacy vs. Utility

**Challenge**: Maintaining data utility while respecting privacy.  
**Solution**: Tiered anonymization approach with different data processing pipelines for different purposes.

### 2. Scale and Performance

**Challenge**: Processing large volumes of log data without performance impact.  
**Solution**: Asynchronous logging, efficient storage design, and strategic partitioning.

### 3. Complexity Management

**Challenge**: Managing the complexity of different data types and processing needs.  
**Solution**: Modular architecture with clear separation of concerns and consistent interfaces.

## Next Steps

For detailed technical specifications, see the [Technical Architecture](TECHNICAL_ARCHITECTURE.md) document. For implementation timeline, refer to the [Implementation Plan](IMPLEMENTATION_PLAN.md).

================
File: docs/comprehensive-logging-system/PERMISSION_ISSUES_FIX.md
================
# Logging System Permission Issues Fix

## Problem

Users were experiencing errors when saving ideas in the application:

```
GET https://aerakewgxmkexuyzsomh.supabase.co/rest/v1/app_settings?select=value&key=eq.logging_enabled 406 (Not Acceptable)
POST https://aerakewgxmkexuyzsomh.supabase.co/rest/v1/logging_sessions 403 (Forbidden)
```

These errors indicate permission issues with the Supabase database tables:

1. **406 Not Acceptable**: The application was unable to read from the `app_settings` table to check if logging is enabled.
2. **403 Forbidden**: The application was unable to write to the `logging_sessions` table to create a new logging session.

## Root Cause

The issue was caused by missing or incorrect Row Level Security (RLS) policies on the following tables:

1. `app_settings`: Missing policies for authenticated users to read settings
2. `logging_sessions`: Missing policies for authenticated users to insert new sessions
3. `system_logs`: Missing policies for authenticated users to insert log entries

## Solution

We implemented a two-part solution:

### 1. Database Permissions Fix

Created a script (`scripts/fix-app-settings-logging-permissions.js`) that:

- Enables Row Level Security (RLS) on the affected tables
- Creates appropriate policies for authenticated users to:
  - Read from `app_settings`
  - Insert into `logging_sessions`
  - Insert into `system_logs`
  - Read/update their own logs and sessions

### 2. Enhanced Logging Service

Created an enhanced version of the logging service (`src/lib/services/logging.service.enhanced.ts`) that:

- Gracefully handles 406 and 403 errors
- Falls back to local storage when database operations fail
- Maintains the same API as the original logging service
- Provides additional methods to check if local logging is being used and to access locally stored logs

## Implementation

To apply the fix:

1. Run the database permissions fix script:
   ```
   node scripts/fix-app-settings-logging-permissions.js
   ```

2. Apply the enhanced logging service:
   ```
   node scripts/apply-logging-fix.js
   ```

3. Update imports in components to use the enhanced logging service:
   ```typescript
   // Change this:
   import { loggingService } from '../lib/services/logging.service';
   
   // To this:
   import { loggingService } from '../lib/services/logging.index';
   ```

## Benefits

- **Graceful Error Handling**: The application continues to function even when database permissions are not properly set up
- **Local Fallback**: Logs are stored locally when database operations fail
- **Backward Compatibility**: The enhanced service maintains the same API as the original service
- **Improved User Experience**: Users no longer see error messages when saving ideas

## Technical Details

### Enhanced Logging Service Features

- **Permission Issue Detection**: Automatically detects 406 and 403 errors and switches to local logging
- **Local Storage**: Stores logs in memory and optionally in localStorage for persistence
- **Log Retention**: Applies retention policies to local logs to prevent memory leaks
- **API Compatibility**: Maintains the same API as the original logging service
- **Additional Methods**:
  - `isUsingLocalLogging()`: Checks if local logging is being used
  - `getLocalLogs()`: Gets locally stored logs

### Database Permissions

The fix creates the following policies:

#### app_settings Table

- **Read Policy**: Allows authenticated users to read app settings
- **Update Policy**: Allows authenticated users to update app settings
- **Insert Policy**: Allows authenticated users to insert app settings

#### logging_sessions Table

- **Insert Policy**: Allows authenticated users to insert logging sessions
- **Read Policy**: Allows authenticated users to read their own logging sessions
- **Update Policy**: Allows authenticated users to update their own logging sessions

#### system_logs Table

- **Insert Policy**: Allows authenticated users to insert system logs
- **Read Policy**: Allows authenticated users to read their own system logs

## Future Improvements

1. **Sync Local Logs**: Implement a mechanism to sync locally stored logs to the database when permissions are fixed
2. **Admin Dashboard**: Create an admin dashboard to view and manage logs
3. **Monitoring**: Add monitoring to detect and alert on permission issues
4. **Automatic Retry**: Implement automatic retry logic for failed database operations

================
File: docs/comprehensive-logging-system/README.md
================
# Comprehensive Logging System

This directory contains documentation for the Comprehensive Logging System, which is designed to track all user interactions, AI operations, and system events to provide data for model training.

## Key Features

- **Extensive Event Logging**: Track user actions, AI interactions, API requests/responses, and system events
- **Privacy Controls**: Provide user consent management, data anonymization, and privacy request handling
- **Model Training Pipeline**: Extract features from logs, train models, and collect feedback for continuous improvement
- **Data Classification**: Automatically classify and handle data based on sensitivity
- **Retention Policies**: Apply automated data retention policies based on data classification

## Documentation Index

1. [Overview](./OVERVIEW.md) - High-level overview of the logging system
2. [Requirements](./REQUIREMENTS.md) - Detailed requirements for the system
3. [Technical Architecture](./TECHNICAL_ARCHITECTURE.md) - Architecture diagram and component descriptions
4. [Implementation Plan](./IMPLEMENTATION_PLAN.md) - Step-by-step implementation guide
5. [User Guide](./USER_GUIDE.md) - How to use the logging system in your code
6. [Compliance](./COMPLIANCE.md) - Overview of privacy and regulatory compliance features
7. [Data Modeling](./DATA_MODELING.md) - Database schema and data flow

## Quick Start

### Setting Up The System

1. Run the database migration:
   ```bash
   ./scripts/run-comprehensive-logging-migration.js
   ```

2. Import the logging service in your component:
   ```typescript
   import { loggingService } from '../lib/services/logging.service';
   ```

3. Log events throughout your application:
   ```typescript
   // Log a user action
   loggingService.logUserAction(
     'button_click',
     'MainNavigation',
     { button_id: 'dashboard', page: 'home' }
   );

   // Log an AI interaction
   loggingService.logAIInteraction(
     'generate_content',
     {
       model: 'gpt-4',
       prompt: 'Generate ideas for new feature',
       response: 'Here are some ideas...',
       tokens: 150
     }
   );

   // Log an error
   loggingService.logError(
     new Error('Something went wrong'),
     'DataLoader'
   );
   ```

### Testing the System

Run the test script to verify your system is working correctly:

```bash
./scripts/run-logging-test.js
```

### Training Models with Log Data

Extract features and train models:

```typescript
import { modelTrainingService } from '../lib/services/model-training.service';

// Extract features from the last 30 days of user behavior logs
const featureIds = await modelTrainingService.extractFeatures(
  'user_behavior',
  {
    startDate: '2025-02-15T00:00:00Z',
    endDate: '2025-03-16T00:00:00Z'
  }
);

// Register a trained model
const modelId = await modelTrainingService.registerModel({
  model_name: 'user_behavior_predictor',
  model_type: 'classifier',
  model_version: '1.0.0',
  model_description: 'Predicts user behavior based on past actions',
  feature_sets: featureIds,
  is_active: true
});
```

## Privacy Features

The system provides several privacy features:

- User consent management for different data uses
- Data anonymization and pseudonymization capabilities
- Privacy request handling (export, deletion, etc.)
- Automatic data classification and retention policies

## Key Components

- `LoggingService`: Central service for logging all events
- `PrivacyService`: Handles consent, anonymization, and privacy requests
- `ModelTrainingService`: Extracts features from logs for model training

## Database Schema

The system creates several tables:

- `system_logs`: Stores all log events
- `logging_sessions`: Tracks user sessions
- `consent_settings`: Stores user consent preferences
- `privacy_requests`: Manages user privacy requests
- `classification_rules`: Defines data classification rules
- `retention_policies`: Defines data retention policies
- `extracted_features`: Stores features extracted from logs
- `model_registry`: Tracks trained models
- `model_feedback`: Stores feedback on model predictions

## Contributing

When extending the logging system:

1. Update the database schema as needed in `supabase/migrations`
2. Add new types to `src/lib/types/logging.types.ts`
3. Extend the services in `src/lib/services/`
4. Update the documentation in this directory

================
File: docs/comprehensive-logging-system/RECURSIVE_POLICY_FIX.md
================
# Fixing Infinite Recursion in Policies

## Problem

The application encountered an error while attempting to fetch logs for feature extraction:

```
Error fetching logs for feature extraction: {
  code: '42P17', 
  details: null, 
  hint: null, 
  message: 'infinite recursion detected in policy for relation "company_members"'
}
```

This occurs because of a circular dependency in Row Level Security (RLS) policies:

1. When querying `system_logs` with company context
2. The RLS policy on `system_logs` checks if the user is a member of the company
3. This check queries the `company_members` table
4. The RLS policy on `company_members` recursively checks company membership again
5. This results in infinite recursion

## Solution

The solution involves two key components:

### 1. Database Migration

We created a migration file (`supabase/migrations/20250320064200_fix_system_logs_recursion.sql`) that:

- Creates a secure function `fetch_system_logs_securely` that bypasses RLS policies using SECURITY DEFINER
- Updates the RLS policies on system_logs to use the non-recursive `check_company_membership` function
- Drops problematic policies that might cause recursion

### 2. Code Changes

We modified the model-training service to use the secure function instead of directly querying the system_logs table:

```typescript
// Before:
let query = supabase
  .from('system_logs')
  .select('*')
  // ... more query filters ...
const { data: logs, error } = await query;

// After:
const { data: logs, error } = await supabase.rpc(
  'fetch_system_logs_securely',
  { 
    p_user_id: userId || (await supabase.auth.getUser()).data.user?.id,
    p_event_types: eventTypes,
    p_limit: limit
  }
);
```

This approach bypasses the problematic RLS policies and prevents the infinite recursion error.

## Best Practices

When designing Row Level Security policies:

1. **Avoid Circular References**: Ensure that RLS policies don't create circular dependencies
2. **Use Helper Functions**: Create SECURITY DEFINER functions for complex permission checks
3. **Isolate Queries**: Use dedicated functions for operations that may involve multiple tables with RLS
4. **Test with Complexity**: Test RLS policies with complex scenarios involving related tables

## Applying the Fix

To apply this fix to the production database:

1. Run the migration script: `node scripts/run-fix-system-logs-recursion.js`
2. Deploy the updated code with the modified model-training service

The changes should resolve the infinite recursion error while maintaining proper security controls.

================
File: docs/comprehensive-logging-system/REQUIREMENTS.md
================
# Requirements: Comprehensive Logging System

## Overview

The Comprehensive Logging System aims to capture all relevant system interactions for model training while maintaining privacy compliance and maximizing data utility. This document outlines the functional and technical requirements for the system.

## Functional Requirements

### 1. Data Capture Requirements

#### 1.1 User-Specific Data
- System must capture user interactions, preferences, and behavior
- User identity must be handled according to privacy settings
- Personal data must be classified appropriately
- User consent must be respected for all data collection

#### 1.2 Company Data
- System must associate relevant data with company context
- Company relationships and hierarchies must be preserved
- Inter-company patterns must be identifiable while preserving privacy
- Company-specific settings must govern data collection policies

#### 1.3 Interaction Data
- UI interactions must be logged with appropriate context
- Service calls and responses must be captured
- AI interactions must record prompts, responses, and context
- System events and errors must be logged for diagnostics

#### 1.4 Abstraction Layers
- System must define appropriate abstraction levels for different data types
- Data must be categorized by sensitivity and utility
- Relationships between data points must be preserved when appropriate
- Context must be maintained across abstraction layers

### 2. Privacy & Consent Management

#### 2.1 Consent System
- Users must be able to set granular consent preferences
- Consent must be captured for different data usage purposes
- System must respect consent settings across all logging operations
- Consent changes must take effect immediately

#### 2.2 Data Classification
- Personal data must be classified according to sensitivity
- Retention policies must be tied to classification levels
- Anonymization strategies must be appropriate to classification
- Classification must be automated where possible

#### 2.3 Privacy Controls
- Users must have access to all their logged data
- System must provide export mechanisms for user data
- Deletion requests must be honored across the system
- Anonymization must be available as an alternative to deletion

### 3. Processing & Model Training

#### 3.1 Data Processing Pipeline
- Raw data must be transformed into appropriate training formats
- Identifiable information must be handled according to privacy settings
- System must support different levels of anonymization
- Processing must preserve essential patterns while removing identifiers

#### 3.2 Cross-Company Insights
- System must enable pattern recognition across companies
- Company identifiers must be abstracted for cross-company analysis
- Statistical validity must be maintained despite anonymization
- Insights must be generalizable across similar companies

#### 3.3 Company-Specific Context
- Company-specific models must leverage company context
- Personalization must respect privacy boundaries
- Context enhancement must respect data classification
- Company admins must have appropriate controls

### 4. Integration Requirements

#### 4.1 UI Integration
- Logging must be seamlessly integrated with existing UI components
- Performance impact of logging must be minimal
- Consent UI must be intuitive and accessible
- Privacy dashboard must provide clear information

#### 4.2 Service Integration
- Service layer must incorporate logging consistently
- Authentication and authorization must be integrated with logging
- Error handling must include appropriate logging
- Third-party services must be properly wrapped

#### 4.3 OpenAI Integration
- All OpenAI API calls must be logged with context
- Prompts and responses must be captured for improvement
- Model performance metrics must be tracked
- Feedback loops must be established for model improvement

## Technical Requirements

### 1. Database Requirements

#### 1.1 Schema Design
- Logging tables must be optimized for high write volume
- Indices must support common query patterns without slowing writes
- Schema must support flexible data structures via JSONB
- Partitioning strategy must accommodate high data volume

#### 1.2 Security & Access Control
- Row-Level Security must be implemented for all logging tables
- Access patterns must be defined for different user roles
- Sensitive data must be protected via appropriate measures
- Audit trails must be maintained for security-relevant operations

#### 1.3 Performance
- Logging operations must not significantly impact application performance
- Database must handle high-volume inserts efficiently
- Query performance must be optimized for analytics
- Archiving strategy must be implemented for older data

### 2. Application Architecture

#### 2.1 Logging Service
- Core logging service must be centralized
- Service must be resilient to failures
- Batching and queuing mechanisms must be available
- Service must handle concurrent operations efficiently

#### 2.2 Privacy Infrastructure
- Consent management must be integrated with all logging operations
- Classification engine must be accurate and performant
- Anonymization utilities must be available throughout the system
- Data lifecycle management must be automated

#### 2.3 Analytics Architecture
- Data processing pipeline must be scalable
- Model training infrastructure must handle large datasets
- Insight generation must be automated where possible
- Feedback loop must improve models over time

### 3. API Requirements

#### 3.1 Logging API
- Consistent API for logging events from all sources
- Proper error handling and fallbacks
- Support for batched operations
- Context preservation across operations

#### 3.2 Privacy API
- API for consent management and verification
- Endpoints for user data access and export
- Deletion and anonymization request handling
- Privacy preference management

#### 3.3 Analytics API
- Controlled access to insights and patterns
- Appropriate abstraction levels for different consumers
- Privacy-preserving query mechanisms
- Insight recommendation algorithms

## Non-Functional Requirements

### 1. Performance Requirements
- Logging operations must add <50ms overhead to any operation
- Database must support at least 1000 logging operations per second
- Analytics queries must complete within acceptable timeframes
- System must handle peak loads during high-traffic periods

### 2. Scalability Requirements
- Architecture must scale with user and company growth
- Storage strategy must accommodate years of data retention
- Processing pipeline must handle increasing data volume
- Model training must scale with dataset size

### 3. Reliability Requirements
- Logging must be resilient to temporary failures
- Data integrity must be maintained throughout the pipeline
- Backup and recovery procedures must be established
- Monitoring must detect issues proactively

### 4. Compliance Requirements
- GDPR compliance must be fully implemented
- CCPA requirements must be satisfied
- Consent tracking must meet legal standards
- Data export and deletion must fulfill legal obligations

### 5. Maintainability Requirements
- Code must be well-documented and maintainable
- Configuration must be flexible for policy changes
- Admin tools must be provided for system management
- Documentation must be comprehensive for developers

================
File: docs/comprehensive-logging-system/SECURITY.md
================
# Security: Comprehensive Logging System

## Overview

This document details the security considerations, mechanisms, and best practices implemented in the Comprehensive Logging System. It covers how sensitive data is protected, authentication mechanisms, compliance with regulations, and incident response procedures.

## Table of Contents

1. [Data Protection](#data-protection)
2. [Authentication and Authorization](#authentication-and-authorization)
3. [API Security](#api-security)
4. [Compliance](#compliance)
5. [Audit Trail](#audit-trail)
6. [Incident Response](#incident-response)
7. [Security Testing](#security-testing)

## Data Protection

### Data Classification

The logging system automatically classifies all data according to sensitivity levels:

| Classification | Description | Examples | Handling |
|----------------|-------------|----------|----------|
| **Non-personal** | Data that doesn't contain any identifying or sensitive information | Aggregated metrics, feature usage counts | Standard logging, no special handling required |
| **Pseudonymized** | Data where direct identifiers have been replaced with aliases | Hashed user IDs, session identifiers | Requires mapping table security, limited retention |
| **Personal** | Data that can identify an individual | Email addresses, names, IP addresses | Subject to strict access controls, encrypted at rest |
| **Sensitive** | Data with special protection requirements | Health information, financial data | Maximum protection, minimal logging, strict retention limits |

### Encryption

The logging system implements multiple layers of encryption:

#### 1. Transport Layer Encryption

All data transmitted between components is protected using:

- TLS 1.3 for all HTTP communications
- Certificate pinning for API communications
- Perfect Forward Secrecy (PFS) to protect past communications

#### 2. Data at Rest Encryption

Database encryption is implemented with:

- Supabase's built-in encryption for all stored log data
- AES-256 encryption for sensitive fields
- Encryption keys managed through a secure key management service

#### 3. Field-Level Encryption

Particularly sensitive fields receive additional protection:

```typescript
// Example of field-level encryption
import { encrypt, decrypt } from '../utils/encryption';

class SecureLogging {
  private encryptionKey: string;
  
  constructor(encryptionKey: string) {
    this.encryptionKey = encryptionKey;
  }
  
  async logWithEncryptedFields(event: LogEvent): Promise<string> {
    // Check for sensitive fields that need encryption
    if (event.data.paymentInfo) {
      event.data.paymentInfo = encrypt(
        event.data.paymentInfo,
        this.encryptionKey
      );
      event.metadata = event.metadata || {};
      event.metadata.has_encrypted_payment = true;
    }
    
    // Log the event with encrypted fields
    return loggingService.logEvent(event);
  }
  
  async retrieveAndDecrypt(logId: string): Promise<LogEvent> {
    // Retrieve the log
    const log = await fetchLog(logId);
    
    // Decrypt any encrypted fields
    if (log.metadata?.has_encrypted_payment) {
      log.data.paymentInfo = decrypt(
        log.data.paymentInfo,
        this.encryptionKey
      );
    }
    
    return log;
  }
}
```

### Data Minimization

The system follows the principle of data minimization:

1. **Collection Limitation**: Only collect what's necessary for the specific purpose
2. **Automatic Filtering**: Sensitive fields are automatically filtered out
3. **Data Truncation**: Large text fields are truncated to prevent over-collection

```typescript
// Example of data minimization logic
function minimizeUserData(userData: UserProfile): SafeUserData {
  return {
    // Keep only what's needed
    user_id: userData.id,
    role: userData.role,
    account_type: userData.accountType,
    // Truncate free text fields
    bio_length: userData.bio ? userData.bio.length : 0,
    // Boolean flags instead of values
    has_profile_image: !!userData.profileImage,
    has_email: !!userData.email
    // Omit sensitive fields entirely (email, name, address, etc.)
  };
}
```

### Data Retention

The system implements a tiered retention policy based on data classification:

| Retention Policy | Duration | Description |
|------------------|----------|-------------|
| Transient | 24 hours | For temporary debugging data that should be quickly removed |
| Short-term | 30 days | For operational data with limited retention needs |
| Medium-term | 180 days | For standard logging data needed for quarterly analysis |
| Long-term | 2 years | For critical business data required for long-term analysis |

Retention is enforced through automated deletion:

```typescript
// Automated retention policy enforcement
export async function enforceRetentionPolicies(): Promise<void> {
  const retentionPolicies = {
    transient: 1, // 1 day
    short_term: 30, // 30 days
    medium_term: 180, // 180 days
    long_term: 730 // 2 years
  };
  
  for (const [policy, days] of Object.entries(retentionPolicies)) {
    const cutoffDate = new Date();
    cutoffDate.setDate(cutoffDate.getDate() - days);
    
    // Delete logs older than the cutoff date for this policy
    const result = await supabase
      .from('system_logs')
      .delete()
      .eq('retention_policy', policy)
      .lt('created_at', cutoffDate.toISOString());
      
    if (result.error) {
      console.error(`Failed to enforce ${policy} retention policy:`, result.error);
    } else {
      console.log(`Enforced ${policy} retention policy, removed ${result.count} logs`);
    }
  }
}
```

## Authentication and Authorization

### Authentication Mechanisms

The logging system leverages Supabase's authentication with additional controls:

1. **Service-to-Service Authentication**:
   - Mutual TLS (mTLS) for service-to-service communication
   - API key authentication with regular rotation
   - JWT tokens with short expiration windows

2. **User Authentication**:
   - Integration with the main application authentication system
   - Role-based access for viewing logs
   - Multi-factor authentication required for administrative access

### Authorization Model

Access to log data follows the principle of least privilege:

| Role | Capabilities |
|------|--------------|
| User | Can only view their own logs through privacy dashboard |
| Team Member | Can view anonymized logs for their team |
| Administrator | Can view logs for compliance and troubleshooting |
| Privacy Officer | Can access all logs for compliance purposes |
| System | Automated processes with specific, limited access |

Authorization is verified for every operation:

```typescript
// Example authorization check
async function canAccessLogs(
  userId: string,
  targetLogs: LogAccessRequest
): Promise<boolean> {
  // Get user's role
  const { data: userRole } = await supabase
    .from('user_roles')
    .select('role')
    .eq('user_id', userId)
    .single();
    
  // Apply role-based restrictions
  switch (userRole.role) {
    case 'user':
      // Users can only access their own logs
      return targetLogs.user_id === userId;
      
    case 'team_member':
      // Team members can access their team's logs
      const { data: userTeams } = await supabase
        .from('team_members')
        .select('team_id')
        .eq('user_id', userId);
        
      const teamIds = userTeams.map(t => t.team_id);
      return targetLogs.team_id && teamIds.includes(targetLogs.team_id);
      
    case 'administrator':
      // Admins have broader access but still have restrictions
      return !targetLogs.includes_sensitive;
      
    case 'privacy_officer':
      // Privacy officers have complete access
      return true;
      
    default:
      return false;
  }
}
```

## API Security

### Rate Limiting

To prevent abuse and protect system stability:

1. **Tiered Rate Limiting**:
   - Standard limits for normal users
   - Higher limits for authenticated services
   - Dynamic limits based on system load

2. **Implementation**:
   ```typescript
   // Rate limiting middleware
   const rateLimiter = rateLimit({
     windowMs: 15 * 60 * 1000, // 15 minutes
     max: (req) => {
       // Determine limit based on authentication
       if (req.headers['x-api-key']) return 1000; // Service
       if (req.user) return 100; // Authenticated user
       return 20; // Unauthenticated
     },
     standardHeaders: true,
     legacyHeaders: false,
     keyGenerator: (req) => {
       // Use API key or IP as key
       return req.headers['x-api-key'] || req.ip;
     }
   });
   ```

### Input Validation

All inputs to the logging system are strictly validated:

1. **Schema Validation**:
   - JSON Schema validation for all log events
   - Type checking and sanitization
   - Reject oversized payloads

2. **Implementation**:
   ```typescript
   // Validate log event against schema
   function validateLogEvent(event: LogEvent): boolean {
     // Check required fields
     if (!event.event_type || !event.event_source || !event.action) {
       return false;
     }
     
     // Validate types
     if (typeof event.event_type !== 'string' || 
         typeof event.event_source !== 'string' ||
         typeof event.action !== 'string') {
       return false;
     }
     
     // Validate data is not too large
     const dataSize = JSON.stringify(event.data).length;
     if (dataSize > MAX_DATA_SIZE) {
       return false;
     }
     
     // Check for dangerous content
     if (containsDangerousContent(event)) {
       return false;
     }
     
     return true;
   }
   ```

## Compliance

### Regulatory Compliance

The logging system is designed to support compliance with:

1. **GDPR (General Data Protection Regulation)**:
   - Data subject access rights
   - Right to be forgotten
   - Data minimization and purpose limitation
   - Consent management

2. **CCPA/CPRA (California Privacy Rights Act)**:
   - Consumer rights to access and delete data
   - Opt-out mechanisms
   - Service provider requirements

3. **HIPAA (Health Insurance Portability and Accountability Act)**:
   - Protected health information safeguards
   - Audit controls and integrity
   - Technical safeguards

### Privacy Impact Assessment

A privacy impact assessment is conducted for all logging activities:

1. **Data Flow Mapping**: Tracking where log data originates, how it's processed, and where it's stored
2. **Risk Assessment**: Identifying privacy risks in the logging system
3. **Mitigation Measures**: Implementing controls to address identified risks
4. **Regular Reviews**: Periodic reassessment of privacy impacts

### Data Processing Agreements

When integrated with third-party services, the system ensures:

1. **Vendor Assessment**: Security and privacy practices of all third parties
2. **Processing Limitations**: Clear limitations on how third parties can process log data
3. **Subprocessor Controls**: Requirements for sub-processor management
4. **Deletion Requirements**: Clear procedures for data deletion

## Audit Trail

### System Audit Logging

The logging system maintains its own audit logs for security events:

1. **Admin Actions**:
   - Log access attempts (successful and failed)
   - Configuration changes
   - User permission changes

2. **System Events**:
   - Service starts and stops
   - Backup and restore operations
   - Retention policy enforcement

```typescript
// Example of audit logging
async function auditLogAccess(
  userId: string,
  accessType: string,
  logId: string | null,
  success: boolean,
  reason?: string
): Promise<void> {
  await supabase
    .from('system_audit_logs')
    .insert({
      user_id: userId,
      action: 'log_access',
      access_type: accessType,
      log_id: logId,
      success,
      reason,
      timestamp: new Date().toISOString(),
      ip_address: getCurrentIpAddress(),
      user_agent: getCurrentUserAgent()
    });
}
```

### Non-repudiation

To ensure the integrity of the audit trail:

1. **Digital Signatures**:
   - Critical logs are digitally signed
   - Signatures are stored with log entries

2. **Blockchain Integration** (optional):
   - Cryptographic proof of log existence
   - Immutable record of security-critical events

```typescript
// Example of digital signing
async function signLogEvent(event: LogEvent): Promise<string> {
  // Create a canonical representation of the log
  const canonicalLog = JSON.stringify(event, Object.keys(event).sort());
  
  // Sign the log
  const signature = await cryptoService.sign(
    canonicalLog,
    process.env.LOG_SIGNING_KEY
  );
  
  // Return the signature
  return signature;
}
```

## Incident Response

### Security Incident Detection

The system includes mechanisms to detect potential security incidents:

1. **Anomaly Detection**:
   - Unusual access patterns
   - Spikes in error rates
   - Unexpected data access

2. **Alerting**:
   - Immediate alerts for critical security events
   - Escalation paths for different severity levels
   - Aggregation to prevent alert fatigue

```typescript
// Example anomaly detection
async function detectAnomalies(): Promise<void> {
  // Calculate baseline metrics
  const { data: baseline } = await supabase.rpc('get_access_baselines');
  
  // Check current metrics
  const { data: current } = await supabase.rpc('get_current_access_metrics');
  
  // Compare and detect anomalies
  for (const metric of current) {
    const baselineMetric = baseline.find(b => b.metric === metric.metric);
    
    if (!baselineMetric) continue;
    
    // Calculate deviation
    const deviation = Math.abs(
      (metric.value - baselineMetric.value) / baselineMetric.value
    );
    
    // Alert if deviation exceeds threshold
    if (deviation > ANOMALY_THRESHOLD) {
      await sendSecurityAlert({
        type: 'anomaly_detected',
        metric: metric.metric,
        value: metric.value,
        baseline: baselineMetric.value,
        deviation: deviation,
        timestamp: new Date().toISOString()
      });
    }
  }
}
```

### Incident Response Procedures

When security incidents are detected:

1. **Containment**:
   - Immediate access revocation
   - System isolation if necessary
   - Temporary enhanced logging

2. **Investigation**:
   - Forensic analysis of logs
   - Impact assessment
   - Root cause determination

3. **Recovery**:
   - Data restoration if needed
   - System hardening
   - Vulnerability remediation

4. **Post-incident Review**:
   - Process improvement
   - Documentation updates
   - Training enhancements

## Security Testing

### Vulnerability Scanning

Regular security testing includes:

1. **Automated Scanning**:
   - SAST (Static Application Security Testing)
   - DAST (Dynamic Application Security Testing)
   - Dependency vulnerability scanning

2. **Manual Testing**:
   - Code reviews focused on security
   - Penetration testing
   - Data privacy reviews

### Example Security Testing Strategy

| Test Type | Frequency | Tools | Responsibility |
|-----------|-----------|-------|----------------|
| Dependency Scanning | Weekly | NPM Audit, Snyk | CI/CD Pipeline |
| Static Code Analysis | On every PR | ESLint Security, SonarQube | CI/CD Pipeline |
| Dynamic Testing | Monthly | OWASP ZAP, Burp Suite | Security Team |
| Penetration Testing | Quarterly | Manual Testing | External Security Firm |
| Log Analysis Review | Monthly | Custom Scripts | Data Privacy Team |

## Security Best Practices

### Developer Guidelines

Developers working with the logging system should follow these guidelines:

1. **Never Log Sensitive Data**:
   - No passwords, tokens, or credentials
   - No full personal identifying information
   - No sensitive business information

2. **Use Structured Logging**:
   - Always use the provided logging abstractions
   - Never create custom logging without review
   - Follow the established classification system

3. **Validate All Inputs**:
   - Validate log data before sending
   - Sanitize user-provided information
   - Enforce size limits on all fields

4. **Error Handling**:
   - Never expose internal errors to users
   - Log failures in the logging system itself
   - Implement graceful degradation

5. **Regular Security Training**:
   - Stay current on data privacy requirements
   - Understand classification guidelines
   - Know how to respond to security incidents

By following these security practices, the Comprehensive Logging System maintains the confidentiality, integrity, and availability of log data while supporting compliance requirements and enabling valuable data analytics.

================
File: docs/comprehensive-logging-system/TECHNICAL_ARCHITECTURE.md
================
# Technical Architecture: Comprehensive Logging System

## Overview

The Comprehensive Logging System is designed to capture system interactions while maintaining privacy compliance and enabling model training. This document outlines the technical architecture, design decisions, and implementation details.

## System Components

### 1. Database Schema

The system introduces new tables for logging and privacy management:

```
┌─────────────────────┐      ┌──────────────────────┐      ┌──────────────────────┐
│  system_logs        │      │  user_consent        │      │  log_classification  │
│  (main log table)   │      │  (consent settings)  │      │  (classification rules)│
└─────────┬───────────┘      └──────────┬───────────┘      └──────────────────────┘
          │                             │                              
          │                             │                              
┌─────────┴───────────┐      ┌──────────┴───────────┐      ┌──────────────────────┐
│  log_partitions     │      │  privacy_requests    │      │  data_retention      │
│  (partitioning)     │      │  (export/deletion)   │      │  (retention policies)│
└─────────────────────┘      └──────────────────────┘      └──────────────────────┘
```

#### Database Table Definitions

```sql
-- Main logging table
CREATE TABLE system_logs (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  user_id UUID REFERENCES auth.users(id),
  persona_id UUID REFERENCES user_personas(id),
  company_id UUID,
  event_type TEXT NOT NULL,
  event_source TEXT NOT NULL,
  component TEXT,
  action TEXT NOT NULL,
  data JSONB NOT NULL,
  metadata JSONB DEFAULT '{}'::JSONB,
  data_classification TEXT NOT NULL 
    CHECK (data_classification IN ('non_personal', 'pseudonymized', 'personal', 'sensitive')),
  retention_policy TEXT NOT NULL
    CHECK (retention_policy IN ('transient', 'short_term', 'medium_term', 'long_term')),
  session_id TEXT,
  client_info JSONB,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- User consent settings
CREATE TABLE user_consent (
  user_id UUID PRIMARY KEY REFERENCES auth.users(id),
  essential BOOLEAN NOT NULL DEFAULT true, -- Cannot be disabled
  analytics BOOLEAN NOT NULL DEFAULT false,
  product_improvement BOOLEAN NOT NULL DEFAULT false,
  ai_training BOOLEAN NOT NULL DEFAULT false,
  cross_company_insights BOOLEAN NOT NULL DEFAULT false,
  personalization BOOLEAN NOT NULL DEFAULT false,
  last_updated TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  verified BOOLEAN NOT NULL DEFAULT false
);

-- Privacy requests (for GDPR/CCPA compliance)
CREATE TABLE privacy_requests (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  user_id UUID NOT NULL REFERENCES auth.users(id),
  request_type TEXT NOT NULL CHECK (request_type IN ('export', 'deletion', 'correction', 'restriction')),
  status TEXT NOT NULL CHECK (status IN ('pending', 'processing', 'completed', 'rejected')),
  request_details JSONB,
  submitted_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  completed_at TIMESTAMP WITH TIME ZONE,
  notes TEXT
);

-- Data classification rules
CREATE TABLE log_classification (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  data_type TEXT NOT NULL,
  pattern TEXT NOT NULL, -- Regex or description
  classification TEXT NOT NULL 
    CHECK (classification IN ('non_personal', 'pseudonymized', 'personal', 'sensitive')),
  retention_policy TEXT NOT NULL
    CHECK (retention_policy IN ('transient', 'short_term', 'medium_term', 'long_term')),
  description TEXT,
  is_active BOOLEAN NOT NULL DEFAULT true
);

-- Retention policy definitions
CREATE TABLE data_retention (
  policy TEXT PRIMARY KEY,
  retention_days INTEGER NOT NULL,
  description TEXT NOT NULL,
  anonymization_action TEXT CHECK (anonymization_action IN ('delete', 'pseudonymize', 'anonymize')),
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  last_updated TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);
```

**Key Design Decisions:**

1. **Flexible Schema**: Using JSONB columns for data and metadata to accommodate different log types.
2. **Classification Integration**: Direct classification and retention policy fields in the log table.
3. **Partitioning Ready**: Structure supports time-based partitioning for performance.
4. **Consent Granularity**: Detailed consent options to comply with privacy regulations.

### 2. Services Layer

The services layer organization:

```
┌───────────────────────┐
│ LoggingService        │
├───────────────────────┤
│ - logEvent()          │
│ - logUIAction()       │
│ - logAIInteraction()  │
│ - batchLog()          │
└─────────┬─────────────┘
          │
          │
┌─────────┼─────────────┐         ┌───────────────────────┐
│ PrivacyService        │◄────────┤AnalyticsService       │
├───────────────────────┤         ├───────────────────────┤
│ - checkConsent()      │         │ - processLogData()    │
│ - classifyData()      │         │ - trainModels()       │
│ - anonymizeData()     │         │ - generateInsights()  │
└───────────────────────┘         └───────────────────────┘
```

#### Core Services Implementation

```typescript
// src/lib/services/logging.service.ts
import { supabase } from '../supabase';
import { v4 as uuidv4 } from 'uuid';
import { privacyService } from './privacy.service';

export interface LogEvent {
  user_id?: string;
  persona_id?: string;
  company_id?: string;
  event_type: string;
  event_source: string;
  component?: string;
  action: string;
  data: any;
  metadata?: any;
  session_id?: string;
}

class LoggingService {
  private sessionId: string;
  private clientInfo: any;
  
  constructor() {
    this.sessionId = uuidv4();
    this.clientInfo = this.captureClientInfo();
  }
  
  private captureClientInfo() {
    return {
      userAgent: navigator.userAgent,
      language: navigator.language,
      viewport: {
        width: window.innerWidth,
        height: window.innerHeight
      },
      timestamp: new Date().toISOString()
    };
  }
  
  async logEvent(event: LogEvent): Promise<void> {
    try {
      // Check consent before logging
      const canLog = await privacyService.checkConsent(
        event.user_id, 
        this.getConsentTypeForEvent(event)
      );
      
      if (!canLog) return;
      
      // Classify data
      const { classification, retentionPolicy } = 
        await privacyService.classifyData(event.data);
      
      // Insert into logs
      const { error } = await supabase
        .from('system_logs')
        .insert({
          ...event,
          session_id: event.session_id || this.sessionId,
          client_info: this.clientInfo,
          data_classification: classification,
          retention_policy: retentionPolicy
        });
        
      if (error) console.error('Error logging event:', error);
    } catch (err) {
      console.error('Failed to log event:', err);
      // Even when logging fails, we don't want to break the application
    }
  }
  
  private getConsentTypeForEvent(event: LogEvent): string {
    // Map event types to consent types
    switch (event.event_type) {
      case 'user_action': return 'analytics';
      case 'ai_interaction': return 'ai_training';
      case 'cross_company': return 'cross_company_insights';
      case 'personalization': return 'personalization';
      case 'system': return 'essential';
      default: return 'product_improvement';
    }
  }
  
  // Specialized logging methods
  async logUserAction(action: string, data: any, metadata?: any): Promise<void> {
    const user = await supabase.auth.getUser();
    if (!user.data.user) return;
    
    return this.logEvent({
      user_id: user.data.user.id,
      event_type: 'user_action',
      event_source: 'ui',
      action,
      data,
      metadata
    });
  }
  
  async logAIInteraction(action: string, data: any, metadata?: any): Promise<void> {
    const user = await supabase.auth.getUser();
    
    return this.logEvent({
      user_id: user.data?.user?.id,
      event_type: 'ai_interaction',
      event_source: 'ai_service',
      action,
      data,
      metadata
    });
  }
  
  async batchLog(events: LogEvent[]): Promise<void> {
    // Process events in batches for efficiency
    for (const event of events) {
      await this.logEvent(event);
    }
  }
}

export const loggingService = new LoggingService();
```

```typescript
// src/lib/services/privacy.service.ts
import { supabase } from '../supabase';

class PrivacyService {
  async checkConsent(userId: string | undefined, consentType: string): Promise<boolean> {
    if (!userId) return consentType === 'essential'; // Only essential logging for non-logged-in users
    
    // Essential consent is always true
    if (consentType === 'essential') return true;
    
    try {
      const { data, error } = await supabase
        .from('user_consent')
        .select(consentType)
        .eq('user_id', userId)
        .single();
        
      if (error || !data) return false;
      
      return !!data[consentType];
    } catch (err) {
      console.error('Error checking consent:', err);
      return false; // Default to no consent on error
    }
  }
  
  async classifyData(data: any): Promise<{ classification: string; retentionPolicy: string }> {
    // Default classification
    let classification = 'non_personal';
    let retentionPolicy = 'medium_term';
    
    // Check for personal data patterns
    if (this.containsPersonalData(data)) {
      classification = 'personal';
      retentionPolicy = 'short_term';
    }
    
    // Check for sensitive data patterns
    if (this.containsSensitiveData(data)) {
      classification = 'sensitive';
      retentionPolicy = 'transient';
    }
    
    return { classification, retentionPolicy };
  }
  
  private containsPersonalData(data: any): boolean {
    // Check for common personal data patterns
    // This would be a more sophisticated implementation in production
    const dataString = JSON.stringify(data).toLowerCase();
    
    // Simple check for common personal identifiers
    return /email|name|phone|address|location|ip|user_id/.test(dataString);
  }
  
  private containsSensitiveData(data: any): boolean {
    // Check for sensitive data patterns
    // This would be a more sophisticated implementation in production
    const dataString = JSON.stringify(data).toLowerCase();
    
    // Simple check for common sensitive identifiers
    return /password|ssn|credit|token|health|religion|politics|ethnic/.test(dataString);
  }
  
  async anonymizeData(data: any, level: 'pseudonymize' | 'anonymize' = 'anonymize'): Promise<any> {
    if (level === 'pseudonymize') {
      // Replace identifiers with consistent pseudonyms
      return this.pseudonymizeData(data);
    } else {
      // Remove or generalize all identifiers
      return this.fullAnonymization(data);
    }
  }
  
  private pseudonymizeData(data: any): any {
    // Implementation would use consistent hashing for identifiers
    // This is a simplified version
    // ...
    return data; // Placeholder
  }
  
  private fullAnonymization(data: any): any {
    // Implementation would remove all identifiers
    // This is a simplified version
    // ...
    return data; // Placeholder
  }
  
  async handlePrivacyRequest(userId: string, requestType: 'export' | 'deletion' | 'correction' | 'restriction'): Promise<string> {
    // Create a privacy request
    const { data, error } = await supabase
      .from('privacy_requests')
      .insert({
        user_id: userId,
        request_type: requestType,
        status: 'pending'
      })
      .select()
      .single();
      
    if (error) {
      throw new Error(`Failed to create privacy request: ${error.message}`);
    }
    
    // Start processing the request asynchronously
    this.processPrivacyRequest(data.id);
    
    return data.id;
  }
  
  private async processPrivacyRequest(requestId: string): Promise<void> {
    // This would be implemented as a background job
    // ...
  }
}

export const privacyService = new PrivacyService();
```

```typescript
// src/lib/services/analytics.service.ts
import { supabase } from '../supabase';

class AnalyticsService {
  async processLogData(startDate?: Date, endDate?: Date): Promise<any> {
    // Process log data for analytics
    // This would extract patterns and prepare for model training
    // ...
  }
  
  async trainModels(): Promise<void> {
    // Train machine learning models on processed data
    // ...
  }
  
  async generateInsights(): Promise<any> {
    // Generate insights from processed data
    // ...
  }
  
  async getCompanyContext(companyId: string): Promise<any> {
    // Build context for a specific company
    // ...
  }
  
  async getCrossCompanyPatterns(industry?: string): Promise<any> {
    // Find patterns across companies (anonymized)
    // ...
  }
}

export const analyticsService = new AnalyticsService();
```

### 3. Data Models

Core TypeScript interfaces that define the system:

```typescript
// src/lib/types/logging.types.ts

// Main log event interface
export interface LogEvent {
  id: string;
  user_id?: string;
  persona_id?: string;
  company_id?: string;
  event_type: string;
  event_source: string;
  component?: string;
  action: string;
  data: any; // JSONB in database
  metadata?: any; // JSONB in database
  data_classification: 'non_personal' | 'pseudonymized' | 'personal' | 'sensitive';
  retention_policy: 'transient' | 'short_term' | 'medium_term' | 'long_term';
  session_id?: string;
  client_info?: any;
  created_at: string;
}

// Consent settings
export interface ConsentSettings {
  user_id: string;
  essential: boolean; // Cannot be disabled
  analytics: boolean;
  product_improvement: boolean;
  ai_training: boolean; 
  cross_company_insights: boolean;
  personalization: boolean;
  last_updated: string;
  verified: boolean;
}

// Privacy request
export interface PrivacyRequest {
  id: string;
  user_id: string;
  request_type: 'export' | 'deletion' | 'correction' | 'restriction';
  status: 'pending' | 'processing' | 'completed' | 'rejected';
  request_details?: any;
  submitted_at: string;
  completed_at?: string;
  notes?: string;
}

// Data classification rule
export interface ClassificationRule {
  id: string;
  data_type: string;
  pattern: string; // Regex or description
  classification: 'non_personal' | 'pseudonymized' | 'personal' | 'sensitive';
  retention_policy: 'transient' | 'short_term' | 'medium_term' | 'long_term';
  description?: string;
  is_active: boolean;
}

// Processed analytics data
export interface AnalyticsData {
  time_period: string;
  metrics: Record<string, number>;
  patterns: any[];
  insights: string[];
  generated_at: string;
}

// Company context
export interface CompanyContext {
  company_id: string;
  industry: string;
  company_size: string;
  usage_patterns: any[];
  common_workflows: any[];
  feature_preferences: any[];
  last_updated: string;
}
```

### 4. Component Architecture

UI components follow a hierarchical organization:

```
┌─────────────────────────────┐
│ LoggingProvider (Context)   │
├─────────────────────────────┤
│                             │
└─────────────────────────────┘

┌─────────────────────────────┐
│ ConsentManager              │
├─────────────────────────────┤
│ ┌───────────────────────┐   │
│ │ ConsentSettings       │   │
│ └───────────────────────┘   │
│ ┌───────────────────────┐   │
│ │ ConsentExplanation    │   │
│ └───────────────────────┘   │
└─────────────────────────────┘

┌─────────────────────────────┐
│ PrivacyDashboard            │
├─────────────────────────────┤
│ ┌───────────────────────┐   │
│ │ DataSummary           │   │
│ └───────────────────────┘   │
│ ┌───────────────────────┐   │
│ │ ExportTools           │   │
│ └───────────────────────┘   │
│ ┌───────────────────────┐   │
│ │ DeletionRequest       │   │
│ └───────────────────────┘   │
└─────────────────────────────┘
```

#### Core Component Implementation

```typescript
// src/lib/contexts/LoggingContext.tsx
import React, { createContext, useContext, ReactNode } from 'react';
import { loggingService } from '../services/logging.service';

interface LoggingContextProps {
  logUIEvent: (component: string, action: string, data: any, metadata?: any) => Promise<void>;
  logFormSubmission: (form: string, data: any, metadata?: any) => Promise<void>;
  logNavigation: (from: string, to: string, metadata?: any) => Promise<void>;
}

const LoggingContext = createContext<LoggingContextProps | undefined>(undefined);

export const LoggingProvider: React.FC<{children: ReactNode}> = ({ children }) => {
  const logUIEvent = async (component: string, action: string, data: any, metadata?: any) => {
    return loggingService.logEvent({
      event_type: 'ui_interaction',
      event_source: 'ui',
      component,
      action,
      data,
      metadata
    });
  };
  
  const logFormSubmission = async (form: string, data: any, metadata?: any) => {
    return loggingService.logEvent({
      event_type: 'form_submission',
      event_source: 'ui',
      component: form,
      action: 'submit',
      data,
      metadata
    });
  };
  
  const logNavigation = async (from: string, to: string, metadata?: any) => {
    return loggingService.logEvent({
      event_type: 'navigation',
      event_source: 'router',
      action: 'navigate',
      data: { from, to },
      metadata
    });
  };
  
  return (
    <LoggingContext.Provider value={{
      logUIEvent,
      logFormSubmission,
      logNavigation
    }}>
      {children}
    </LoggingContext.Provider>
  );
};

export const useLogging = () => {
  const context = useContext(LoggingContext);
  if (context === undefined) {
    throw new Error('useLogging must be used within a LoggingProvider');
  }
  return context;
};
```

```typescript
// src/components/privacy/ConsentManager.tsx
import React, { useState, useEffect } from 'react';
import { supabase } from '../../lib/supabase';
import { ConsentSettings } from '../../lib/types/logging.types';

const ConsentManager: React.FC = () => {
  const [consentSettings, setConsentSettings] = useState<Partial<ConsentSettings>>({
    essential: true, // Cannot be disabled
    analytics: false,
    product_improvement: false,
    ai_training: false,
    cross_company_insights: false,
    personalization: false,
  });
  
  const [loading, setLoading] = useState(true);
  const [userId, setUserId] = useState<string | null>(null);
  
  useEffect(() => {
    const fetchConsent = async () => {
      const { data: { user } } = await supabase.auth.getUser();
      if (!user) {
        setLoading(false);
        return;
      }
      
      setUserId(user.id);
      
      const { data, error } = await supabase
        .from('user_consent')
        .select('*')
        .eq('user_id', user.id)
        .single();
        
      if (error) {
        // If not found, we'll create with defaults
        if (error.code === 'PGRST116') {
          await supabase.from('user_consent').insert({
            user_id: user.id,
            essential: true,
            verified: true
          });
        } else {
          console.error('Error fetching consent settings:', error);
        }
      } else if (data) {
        setConsentSettings(data);
      }
      
      setLoading(false);
    };
    
    fetchConsent();
  }, []);
  
  const updateConsent = async (key: string, value: boolean) => {
    if (!userId) return;
    
    // Can't disable essential
    if (key === 'essential' && !value) return;
    
    setConsentSettings(prev => ({
      ...prev,
      [key]: value
    }));
    
    const { error } = await supabase
      .from('user_consent')
      .update({
        [key]: value,
        last_updated: new Date().toISOString()
      })
      .eq('user_id', userId);
      
    if (error) {
      console.error('Error updating consent setting:', error);
      // Revert on error
      setConsentSettings(prev => ({
        ...prev,
        [key]: !value
      }));
    }
  };
  
  if (loading) return <div>Loading consent settings...</div>;
  
  return (
    <div className="consent-manager">
      <h2>Data & Privacy Settings</h2>
      <p>Control how your data is used throughout the application.</p>
      
      <div className="consent-options">
        {/* Essential - can't disable */}
        <div className="consent-option">
          <label>
            <input 
              type="checkbox" 
              checked={consentSettings.essential} 
              disabled={true}
            />
            Essential
          </label>
          <p className="description">
            Required for the application to function properly. Includes authentication, 
            error logging, and security-related data.
          </p>
        </div>
        
        {/* Analytics */}
        <div className="consent-option">
          <label>
            <input 
              type="checkbox" 
              checked={consentSettings.analytics} 
              onChange={e => updateConsent('analytics', e.target.checked)}
            />
            Analytics
          </label>
          <p className="description">
            Helps us understand how the application is used, which features are popular, 
            and basic usage metrics.
          </p>
        </div>
        
        {/* Product Improvement */}
        <div className="consent-option">
          <label>
            <input 
              type="checkbox" 
              checked={consentSettings.product_improvement} 
              onChange={e => updateConsent('product_improvement', e.target.checked)}
            />
            Product Improvement
          </label>
          <p className="description">
            Allows us to analyze patterns and improve features based on how they're being used.
          </p>
        </div>
        
        {/* AI Training */}
        <div className="consent-option">
          <label>
            <input 
              type="checkbox" 
              checked={consentSettings.ai_training} 
              onChange={e => updateConsent('ai_training', e.target.checked)}
            />
            AI Training
          </label>
          <p className="description">
            Permits the use of your anonymized data to train AI models that power features 
            like recommendations and assistants.
          </p>
        </div>
        
        {/* Cross-Company Insights */}
        <div className="consent-option">
          <label>
            <input 
              type="checkbox" 
              checked={consentSettings.cross_company_insights} 
              onChange={e => updateConsent('cross_company_insights', e.target.checked)}
            />
            Cross-Company Insights
          </label>
          <p className="description">
            Enables the generation of anonymized, aggregated insights across similar 
            companies to provide benchmarking and best practices.
          </p>
        </div>
        
        {/* Personalization */}
        <div className="consent-option">
          <label>
            <input 
              type="checkbox" 
              checked={consentSettings.personalization} 
              onChange={e => updateConsent('personalization', e.target.checked)}
            />
            Personalization
          </label>
          <p className="description">
            Uses your behavior and preferences to customize your experience and provide 
            more relevant content and features.
          </p>
        </div>
      </div>
      
      <div className="consent-info">
        <h3>Your Rights</h3>
        <p>
          Under GDPR and CCPA, you have the right to access, correct, download, or
          request deletion of your personal data. Visit the Privacy Dashboard to
          exercise these rights.
        </p>
        <button className="secondary">
          Privacy Dashboard
        </button>
      </div>
    </div>
  );
};

export default ConsentManager;
```

### 5. OpenAI Integration

Wrapping the OpenAI client for comprehensive logging:

```typescript
// Enhanced openai-client.ts
import OpenAI from 'openai';
import { loggingService } from './services/logging.service';

const openai = new OpenAI({
  apiKey: import.meta.env.VITE_OPENAI_API_KEY,
  dangerouslyAllowBrowser: true
});

// Create a wrapped version that logs all calls
const loggedOpenAI = {
  chat: {
    completions: {
      create: async (params: any) => {
        // Log the request
        await loggingService.logAIInteraction('openai_request', {
          model: params.model,
          messages: params.messages,
          temperature: params.temperature,
          max_tokens: params.max_tokens
        });
        
        // Make the actual request
        try {
          const response = await openai.chat.completions.create(params);
          
          // Log the response
          await loggingService.logAIInteraction('openai_response', {
            model: params.model,
            completion: response.choices[0]?.message?.content,
            usage: response.usage
          });
          
          return response;
        } catch (error) {
          // Log the error
          await loggingService.logAIInteraction('openai_error', {
            model: params.model,
            error: error
          });
          throw error;
        }
      }
    }
  },
  // Wrap other OpenAI API methods similarly
};

export default loggedOpenAI;
```

## Database Migrations

The implementation requires a new migration file:

```sql
-- Migration: Comprehensive Logging System
-- Created: 2025-03-16
-- Description: Implements logging system with privacy controls

-- Enable extensions if not already enabled
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";

-- Step 1: Create the main logging tables

-- Main logging table
CREATE TABLE system_logs (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  user_id UUID REFERENCES auth.users(id),
  persona_id UUID REFERENCES user_personas(id),
  company_id UUID,
  event_type TEXT NOT NULL,
  event_source TEXT NOT NULL,
  component TEXT,
  action TEXT NOT NULL,
  data JSONB NOT NULL,
  metadata JSONB DEFAULT '{}'::JSONB,
  data_classification TEXT NOT NULL 
    CHECK (data_classification IN ('non_personal', 'pseudonymized', 'personal', 'sensitive')),
  retention_policy TEXT NOT NULL
    CHECK (retention_policy IN ('transient', 'short_term', 'medium_term', 'long_term')),
  session_id TEXT,
  client_info JSONB,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- User consent settings
CREATE TABLE user_consent (
  user_id UUID PRIMARY KEY REFERENCES auth.users(id),
  essential BOOLEAN NOT NULL DEFAULT true, -- Cannot be disabled
  analytics BOOLEAN NOT NULL DEFAULT false,
  product_improvement BOOLEAN NOT NULL DEFAULT false,
  ai_training BOOLEAN NOT NULL DEFAULT false,
  cross_company_insights BOOLEAN NOT NULL DEFAULT false,
  personalization BOOLEAN NOT NULL DEFAULT false,
  last_updated TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  verified BOOLEAN NOT NULL DEFAULT false
);

-- Privacy requests (for GDPR/CCPA compliance)
CREATE TABLE privacy_requests (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  user_id UUID NOT NULL REFERENCES auth.users(id),
  request_type TEXT NOT NULL CHECK (request_type IN ('export', 'deletion', 'correction', 'restriction')),
  status TEXT NOT NULL CHECK (status IN ('pending', 'processing', 'completed', 'rejected')),
  request_details JSONB,
  submitted_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  completed_at TIMESTAMP WITH TIME ZONE,
  notes TEXT
);

-- Data classification rules
CREATE TABLE log_classification (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  data_type TEXT NOT NULL,
  pattern TEXT NOT NULL, -- Regex or description
  classification TEXT NOT NULL 
    CHECK (classification IN ('non_personal', 'pseudonymized', 'personal', 'sensitive')),
  retention_policy TEXT NOT NULL
    CHECK (retention_policy IN ('transient', 'short_term', 'medium_term', 'long_term')),
  description TEXT,
  is_active BOOLEAN NOT NULL DEFAULT true
);

-- Retention policy definitions
CREATE TABLE data_retention (
  policy TEXT PRIMARY KEY,
  retention_days INTEGER NOT NULL,
  description TEXT NOT NULL,
  anonymization_action TEXT CHECK (anonymization_action IN ('delete', 'pseudonymize', 'anonymize')),
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  last_updated TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Step 2

================
File: docs/comprehensive-logging-system/USER_GUIDE.md
================
# User Guide: Comprehensive Logging System

## Introduction

This guide explains how you can manage your data and privacy settings within the application. Our Comprehensive Logging System is designed to gather information that helps improve your experience while respecting your privacy preferences.

## Privacy Controls

### Accessing Your Privacy Settings

1. Log in to your account
2. Click on your profile icon in the top-right corner
3. Select "Privacy & Data Settings" from the dropdown menu
4. You will be taken to the Privacy Dashboard

### Understanding Consent Options

The Privacy Dashboard allows you to control how your data is used. You can enable or disable the following consent options:

- **Essential** (cannot be disabled): Required for the application to function properly. Includes authentication, error logging, and security-related data.
- **Analytics**: Helps us understand how the application is used, which features are popular, and basic usage metrics.
- **Product Improvement**: Allows us to analyze patterns and improve features based on how they're being used.
- **AI Training**: Permits the use of your anonymized data to train AI models that power features like recommendations and assistants.
- **Cross-Company Insights**: Enables the generation of anonymized, aggregated insights across similar companies to provide benchmarking and best practices.
- **Personalization**: Uses your behavior and preferences to customize your experience and provide more relevant content and features.

### Managing Your Consent

To change your consent settings:

1. Navigate to the Privacy Dashboard
2. Review each consent category and its description
3. Toggle the switches to enable or disable specific consent categories
4. Changes take effect immediately

## Data Access & Control

### Viewing Your Data

You can view a summary of the data collected about you:

1. Go to the Privacy Dashboard
2. Select "My Data" tab
3. You'll see a breakdown of data categories and volumes
4. Use the filters to explore specific date ranges or data types

### Exporting Your Data

To export a copy of your data:

1. Go to the Privacy Dashboard
2. Select "Export Data" tab
3. Choose the data categories you wish to export
4. Select your preferred format (JSON, CSV, PDF)
5. Click "Generate Export"
6. You'll receive an email when your export is ready for download

### Requesting Data Deletion

To request deletion of your data:

1. Go to the Privacy Dashboard
2. Select "Data Deletion" tab
3. Choose the scope of deletion:
   - Specific data categories
   - Date range
   - All data
4. Provide a reason for deletion (optional)
5. Click "Request Deletion"
6. You'll receive confirmation when the deletion is complete

Note: Essential data required for account functionality cannot be deleted while your account is active.

## Privacy Impact on Features

Certain features of the application rely on collected data to function optimally. Disabling specific consent categories may impact these features:

### Analytics & Product Improvement

Disabling these options will not affect your immediate experience but may reduce future improvements based on usage patterns.

### AI Training

Disabling this option may reduce the effectiveness of:
- Personalized recommendations
- Smart assistants
- Content relevance

### Cross-Company Insights

Disabling this option will prevent you from receiving:
- Industry benchmarks
- Comparative performance metrics
- Best practice recommendations

### Personalization

Disabling this option will result in:
- Generic rather than personalized experiences
- Standard default settings
- Non-customized content and features

## FAQs

### What data is considered "Essential"?

Essential data includes:
- Account information (email, name)
- Authentication data
- Security-related information
- Basic application functionality data

### How is my data anonymized?

We use several techniques for anonymization:
- Removing direct identifiers
- Aggregating information
- Applying differential privacy techniques
- Generalizing specific data points

### Can I temporarily disable data collection?

You can use the "Pause All Non-Essential Collection" option in the Privacy Dashboard. This will temporarily disable all non-essential data collection until you choose to resume it.

### How long is my data retained?

Different types of data have different retention periods:
- Essential account data: As long as your account is active
- Personal identifiable information: According to your retention settings (default: 90 days)
- Anonymized data: Up to 5 years for long-term analysis

### Who has access to my data?

- You always have access to your own data
- System administrators have limited access for support purposes
- Automated systems process data according to your consent settings
- No third parties receive your data without explicit consent

## Privacy Policy

For a comprehensive explanation of our data practices, please review our [Privacy Policy](link-to-privacy-policy).

## Getting Help

If you have questions about your privacy settings or data:

1. Check the FAQs in the Privacy Dashboard
2. Contact our Data Protection team via the "Privacy Support" form
3. Email privacy@example.com with specific concerns

================
File: docs/enhanced-profile-system/IMPLEMENTATION_PLAN.md
================
# Implementation Plan: Enhanced Profile System

This document outlines the detailed implementation plan for the Enhanced Profile System, including specific tasks, dependencies, and timelines.

## Phase 1: Database Foundation (Week 1-2)

### Week 1: Schema Creation & Migration

#### Tasks

1. **Create Migration Script**
   - Develop SQL migration that adds new tables and relationships
   - Implement triggers for completion calculation
   - Create compatibility views
   - Set up Row-Level Security policies
   - ✅ *Complete* (see `supabase/migrations/20250316145000_enhanced_profile_completion.sql`)

2. **Update Type Definitions**
   - Create TypeScript interfaces for new data structures
   - Update existing interface definitions for compatibility
   - Ensure proper typing throughout the application

3. **Testing Database Changes**
   - Create test script to verify migration
   - Validate triggers and functions
   - Test compatibility views with existing data

### Week 2: Core Services Implementation

#### Tasks

1. **Profile Service Updates**
   - Implement enhanced profile retrieval
   - Create methods for section management
   - Build update mechanisms for profile data

2. **Completion Calculation Service**
   - Implement client-side completion calculation
   - Create section status tracking
   - Build profile completion visualization components

3. **Integration Test Harness**
   - Create testing utilities for new services
   - Implement mocks for development
   - Verify compatibility with existing services

## Phase 2: Onboarding Flow (Week 3-5)

### Week 3: Base Onboarding Components

#### Tasks

1. **Role Selection Step**
   - Implement role selection UI
   - Create primary/secondary role designation
   - Build role persistence mechanism

2. **Onboarding Flow Controller**
   - Create flow management logic
   - Implement step sequencing based on roles
   - Build progress tracking mechanism

3. **Onboarding Progress Component**
   - Develop visual progress indicator
   - Implement step navigation
   - Create completion status indicators

### Week 4: Role-Specific Onboarding

#### Tasks

1. **Founder-Specific Steps**
   - Create company stage selection
   - Implement idea stage questionnaire
   - Build existing company information collection

2. **Service Provider Steps**
   - Implement service category selection
   - Create service details collection
   - Build expertise and rate information inputs

3. **Company Member Steps**
   - Create invite code entry and validation
   - Implement company role specification
   - Build department and responsibility inputs

### Week 5: Onboarding Completion & Recommendations

#### Tasks

1. **Feature Recommendation Engine**
   - Create recommendation algorithms
   - Implement role-based recommendations
   - Build personalization based on responses

2. **Onboarding Completion View**
   - Create summary of collected information
   - Implement feature highlights
   - Build call-to-action for profile completion

3. **Analytics & Tracking**
   - Implement onboarding funnel analytics
   - Create completion rate tracking
   - Build dropout analysis mechanisms

## Phase 3: Profile Builder (Week 6-9)

### Week 6: Core Profile Builder Framework

#### Tasks

1. **Profile Navigation Component**
   - Create section navigation UI
   - Implement completion indicators
   - Build collapsible section groups

2. **Section Editor Components**
   - Create reusable form components
   - Implement inline validation
   - Build auto-save functionality

3. **Profile Data Management**
   - Create profile context provider
   - Implement optimistic updates
   - Build change tracking mechanism

### Week 7: Role-Specific Profile Sections

#### Tasks

1. **Universal Sections**
   - Create personal information editor
   - Implement bio and overview sections
   - Build skills and expertise editor

2. **Founder-Specific Sections**
   - Create founder experience section
   - Implement company information editor
   - Build founder achievements section

3. **Service Provider Sections**
   - Create service listing editor
   - Implement work samples gallery
   - Build rates and availability editor

### Week 8: Profile Completion & Notifications

#### Tasks

1. **Completion Tracking UI**
   - Create profile completion dashboard
   - Implement section completion indicators
   - Build completion recommendation engine

2. **Notification Components**
   - Create notification center
   - Implement notification cards
   - Build toast notification system

3. **Notification Service**
   - Implement notification generation
   - Create notification persistence
   - Build notification delivery mechanism

### Week 9: Preview & Publishing

#### Tasks

1. **Profile Preview Mode**
   - Create profile preview components
   - Implement role-specific views
   - Build visibility controls

2. **Publishing Controls**
   - Create profile visibility settings
   - Implement section-level privacy controls
   - Build public/private toggle mechanisms

3. **Profile Sharing Features**
   - Implement shareable profile links
   - Create profile export functionality
   - Build integration with messaging systems

## Phase 4: Integration & Testing (Week 10-11)

### Week 10: Key System Integration

#### Tasks

1. **Authentication Flow Integration**
   - Update auth flow to include onboarding
   - Implement conditional redirect logic
   - Build session management updates

2. **Navigation Integration**
   - Update layout components with profile info
   - Implement profile completion indicators
   - Build notification badging

3. **Existing Services Integration**
   - Update idea playground to use new profile
   - Make standup bot compatible with profile changes
   - Update task generation to use profile data

### Week 11: Testing & Quality Assurance

#### Tasks

1. **Comprehensive Testing**
   - Create end-to-end test suite
   - Implement integration tests for all paths
   - Build automated validation of profile data

2. **Performance Optimization**
   - Profile and optimize database queries
   - Implement lazy loading for profile sections
   - Create caching strategy for profile data

3. **Cross-Browser Testing**
   - Test in all major browsers
   - Verify mobile responsiveness
   - Ensure accessibility compliance

## Phase 5: Launch & Monitoring (Week 12)

### Week 12: Launch Preparation & Deployment

#### Tasks

1. **Documentation & Training**
   - Create user documentation
   - Implement in-app guidance
   - Prepare support team training

2. **Migration Strategy**
   - Create plan for existing user migration
   - Implement data backfill for new fields
   - Build rollback capability

3. **Launch & Monitoring**
   - Deploy to production
   - Implement monitoring dashboards
   - Create alert systems for issues

## Dependencies & Risks

### Critical Dependencies

1. **Database Schema Compatibility**
   - Existing services must continue functioning with new schema
   - Compatibility views must perform well under load
   - Triggers should not impact overall database performance

2. **User Experience Continuity**
   - Transition between old and new systems must be seamless
   - Users should not lose data during migration
   - Feature discoverability must be maintained

3. **Technical Integration Points**
   - Auth flow modifications require careful testing
   - Layout component updates affect entire application
   - Service compatibility requires thorough validation

### Risk Mitigation

1. **Progressive Rollout Strategy**
   - Implement feature flags for gradual enablement
   - Perform A/B testing with small user groups
   - Monitor key metrics during rollout

2. **Fallback Mechanisms**
   - Create dual-path rendering for critical components
   - Implement emergency rollback procedures
   - Maintain backward compatibility for core user flows

3. **Performance Safeguards**
   - Set up performance monitoring for new components
   - Create load testing simulations
   - Establish performance budgets for key interactions

## Success Metrics

### Key Performance Indicators

1. **Onboarding Completion Rate**
   - Target: >90% of new users complete onboarding
   - Measure time spent in onboarding
   - Track step-by-step conversion

2. **Profile Completion Metrics**
   - Target: >75% average profile completion
   - Measure section-by-section completion rates
   - Track time to complete profile

3. **System Performance**
   - Target: <200ms for profile data retrieval
   - Measure impact on existing services
   - Track database performance under load

### User Satisfaction Metrics

1. **User Feedback**
   - Collect explicit feedback on new profile system
   - Measure satisfaction with onboarding flow
   - Track feature utilization post-onboarding

2. **Engagement Metrics**
   - Measure return visits to profile
   - Track profile update frequency
   - Monitor sharing and visibility changes

================
File: docs/enhanced-profile-system/OVERVIEW.md
================
# Enhanced Profile System Overview

## Introduction

The Enhanced Profile System is a comprehensive upgrade to the platform's user profile management, onboarding experience, and role-based personalization. This system aims to create a more tailored user experience while maintaining compatibility with existing services such as Idea Playground, Standup Bot, and Task Generation.

## Key Features

### 1. Two-Phase Role-Based Onboarding
- **Initial Onboarding Phase:**
  - Quick role selection (Founder, Company Member, Service Provider)
  - Basic customization based on primary role
  - Key information capture (company stage for founders, service categories for providers)
  - Integration with idea playground, company formation, or service provider tooling
- **Detailed Profile Phase:**
  - Extended profile setup after initial onboarding
  - Multiple role support with primary role designation
  - Role-specific questions and comprehensive information collection
  - Tailored feature recommendations based on completed profile

### 2. Comprehensive Profile Management
- Section-based profile organization for better information structure
- Role-specific profile sections that appear based on selected roles
- Universal sections for information relevant to all users
- Auto-saving and progressive disclosure of complex fields

### 3. Profile Completion Tracking
- Overall profile completion percentage with visual indicators
- Section-by-section completion tracking
- Differentiation between required and optional fields
- Milestone notifications at key completion thresholds

### 4. Advanced Notification System
- Profile completion notifications and reminders
- Centralized notification center with categorization
- Toast notifications for immediate feedback
- Intelligent suggestions for profile improvements

### 5. Service Provider Features
- Detailed service listings with categories and expertise levels
- Rate and availability information
- Portfolio/work samples support
- Professional credentials and work history

### 6. Company Membership System
- Invite code validation for company members
- Company role and responsibility documentation
- Department and team association
- Company-specific skills highlighting

### 7. Founder-Specific Features
- Company stage tracking (idea stage, solid idea, existing company)
- Founder experience and previous venture documentation
- Goals and target market information
- Achievement and milestone tracking

## Technical Architecture

The Enhanced Profile System is built with a focus on backward compatibility and progressive enhancement:

1. **Database Layer**
   - Extended profile schema that preserves existing fields
   - New tables for specialized information (work experience, education, services)
   - Section-based organization for modular completion tracking
   - Database views for compatibility with existing services

2. **Services Layer**
   - Enhanced profile service with compatibility support
   - Completion calculation service for tracking progress
   - Notification service for milestone and improvement suggestions
   - Integration with existing authentication and company services

3. **UI Components**
   - Modular, role-based component architecture
   - Progressive disclosure of complex UI elements
   - Mobile-responsive design for all profile interactions
   - Accessibility-compliant implementations

## Implementation Status

The Enhanced Profile System is being implemented in phases:

- **Phase 1: ✅ Database Foundation**
  - SQL migration script created
  - Database schema design completed
  - Backward compatibility verified
  
- **Phase 2: ✅ Initial Onboarding Flow** (Completed)
  - InitialOnboardingWizard implementation
  - Role selection components
  - Company stage selection for founders
  - Service category selection for service providers
  - Company joining flow for company members
  - Integration with existing site features

- **Phase 3: 🔄 Detailed Profile Builder** (In Progress)
  - Section-based editors
  - Completion tracking UI
  - Role-specific profile sections
  - Notification components
  
- **Phase 3: 📅 Profile Builder** (Upcoming)
  - Section-based editors
  - Completion tracking UI
  - Notification components
  - Preview and publishing tools
  
- **Phase 4: 📅 Integration & Testing** (Planned)
  - Compatibility testing with existing services
  - Performance optimization
  - User acceptance testing
  - Bug fixes and refinements

## Getting Started

### For Developers

1. **Run the Database Migration**
   ```bash
   node scripts/run-enhanced-profile-migration.js
   ```

2. **Testing Onboarding**
   ```bash
   node scripts/test-initial-onboarding.js
   ```
   This script resets the onboarding state for the mock user, allowing you to test the full onboarding flow.

2. **Explore the Documentation**
   - `docs/enhanced-profile-system/REQUIREMENTS.md` - Detailed requirements
   - `docs/enhanced-profile-system/TECHNICAL_ARCHITECTURE.md` - Technical design
   - `docs/enhanced-profile-system/USER_STORIES.md` - User stories and acceptance criteria
   - `docs/enhanced-profile-system/IMPLEMENTATION_PLAN.md` - Phase-by-phase implementation plan

3. **Understanding the Code**
   - The SQL migration in `supabase/migrations/20250316145000_enhanced_profile_completion.sql`
   - Type definitions for the new profile system (upcoming)
   - Component architecture and service implementations (upcoming)

### For Product Stakeholders

The Enhanced Profile System addresses several key business objectives:

1. **Improved User Onboarding**
   - Reduces time-to-value for new users
   - Increases completion rates through personalization
   - Enables better feature discovery based on user needs

2. **Enhanced User Profiling**
   - Creates richer user profiles for better matching and recommendations
   - Supports multiple roles to reflect user reality
   - Improves information organization and accessibility

3. **Platform Growth Support**
   - Scalable architecture for future user types and roles
   - Extensible sections for additional profile information
   - Analytics foundation for understanding user needs

## Compatibility Considerations

The Enhanced Profile System is designed to maintain compatibility with:

- **Idea Playground:** All profile references and company associations preserved
- **Standup Bot:** Continued access to profile and company information
- **Task Generation:** Uninterrupted access to user context and preferences

### Temporary Feature Limitations

- **Persona Selection:** The persona selection functionality has been temporarily disabled during the onboarding overhaul
- **Multi-Persona Support:** While still available in the database schema, the UI components for multi-persona management have been hidden until the core onboarding flow is stabilized

This is achieved through:

1. Non-destructive schema changes
2. Compatibility database views
3. Service layer adapters
4. Feature flag-based rollout

## Next Steps

1. Complete the remaining implementation phases
2. Conduct comprehensive testing with existing services
3. Develop user migration strategy for existing profiles
4. Create user documentation and support resources
5. Plan phased rollout with monitoring

================
File: docs/enhanced-profile-system/README.md
================
# Enhanced Profile System

The Enhanced Profile System is a comprehensive upgrade to the platform's user profile management, onboarding experience, and role-based personalization.

## Documentation Index

- [Overview](./OVERVIEW.md) - High-level project overview
- [User Stories](./USER_STORIES.md) - User stories and acceptance criteria
- [Requirements](./REQUIREMENTS.md) - Detailed functional and technical requirements
- [Technical Architecture](./TECHNICAL_ARCHITECTURE.md) - Technical design and architecture
- [Implementation Plan](./IMPLEMENTATION_PLAN.md) - Phase-by-phase implementation plan

## Key Files

- **Database Migration**: `supabase/migrations/20250316145000_enhanced_profile_completion.sql`
- **Migration Script**: `scripts/run-enhanced-profile-migration.js`
- **Test Data Script**: `scripts/init-profile-sections.js`

## Quick Start

1. **Run the Database Migration**
   ```bash
   node scripts/run-enhanced-profile-migration.js
   ```

2. **Initialize Test Data**
   ```bash
   node scripts/init-profile-sections.js <user-id>
   ```

3. **Start Development Server**
   ```bash
   npm run dev
   ```

## Compatibility

The Enhanced Profile System is designed to maintain compatibility with existing services:

- **Idea Playground**: All profile references and company associations preserved
- **Standup Bot**: Continued access to profile and company information
- **Task Generation**: Uninterrupted access to user context and preferences

## Key Features

- **Role-Based Onboarding**: Personalized flow based on user roles
- **Comprehensive Profile Management**: Section-based profile organization
- **Profile Completion Tracking**: Visual indicators and completion percentage
- **Notification System**: Milestone notifications and improvement suggestions
- **Multiple Role Support**: Switch between different professional contexts

## Implementation Phases

1. **Database Foundation**: ✅ Complete
2. **Onboarding Flow**: 🔄 In Progress
3. **Profile Builder**: 📅 Upcoming
4. **Integration & Testing**: 📅 Planned

## Architecture

The system utilizes a layered architecture:

- **Database Layer**: Extended profile schema with backward compatibility
- **Services Layer**: Enhanced services with compatibility support
- **UI Components**: Role-based, modular components

## Contributing

1. Reference the `REQUIREMENTS.md` and `USER_STORIES.md` for feature specifications
2. Follow the architectural patterns defined in `TECHNICAL_ARCHITECTURE.md`
3. Implement components according to the phased approach in `IMPLEMENTATION_PLAN.md`
4. Ensure backward compatibility with existing services

## Security Considerations

- All new tables include Row-Level Security policies
- Authentication is required for accessing profile data
- Authorization checks prevent unauthorized profile access
- Input validation is enforced for all profile updates

================
File: docs/enhanced-profile-system/REQUIREMENTS.md
================
# Requirements: Enhanced Profile System

## Overview

The Enhanced Profile System aims to create a flexible, comprehensive user profile experience with role-based customization and completion tracking. This document outlines the functional and technical requirements for the system.

## Functional Requirements

### 1. Onboarding Flow

#### 1.1 Role Selection
- System must allow users to select from predefined roles (Founder, Company Member, Service Provider)
- Users must be able to select multiple roles if applicable
- System must identify and store primary role designation
- Role selection must influence subsequent onboarding steps

#### 1.2 Role-Specific Questions
- System must present role-specific questions for each selected role
- For Founders: Collect company stage information (idea stage, solid idea, existing company)
- For Service Providers: Collect service categories and expertise information
- For Company Members: Present invite code entry mechanism

#### 1.3 Progress Tracking
- System must display visual progress indicators during onboarding
- Users must be able to return to previous steps to modify answers
- System must save progress between sessions
- Completion status must be clearly indicated

#### 1.4 Feature Recommendations
- System must generate personalized feature recommendations based on role selection
- Recommendations must include direct access links to relevant features
- Users must be able to explore additional features beyond recommendations
- System must provide context for why each feature is recommended

### 2. Profile Management

#### 2.1 Profile Creation & Editing
- System must provide a comprehensive profile editor with appropriate sections
- Basic profile information must be accessible to all users
- Role-specific information must be grouped logically
- System must auto-save changes to prevent data loss

#### 2.2 Profile Completion Tracking
- System must calculate and display overall profile completion percentage
- Completion tracking must be section-based with individual percentages
- System must distinguish between required and optional fields
- Visual indicators must highlight incomplete sections

#### 2.3 Profile Sections
- System must support universal sections (personal info, bio, skills)
- System must provide role-specific sections based on user roles
- Sections must be organized in a logical sequence
- Users must be able to collapse/expand sections as needed

#### 2.4 Multiple Role Management
- System must allow users to maintain multiple roles simultaneously
- Users must be able to designate a primary role
- System must enable role switching with appropriate UI changes
- Role-specific information must be preserved when switching roles

### 3. Notification System

#### 3.1 Completion Notifications
- System must generate milestone notifications at key completion percentages
- Notifications must include actionable links to relevant sections
- System must prioritize notifications based on importance
- Users must be able to dismiss notifications as needed

#### 3.2 Notification Center
- System must provide a centralized notification center
- Notifications must be categorized by type
- Unread status must be visually indicated
- Users must be able to mark notifications as read

#### 3.3 Profile Improvement Suggestions
- System must generate intelligent suggestions for profile improvements
- Suggestions must be contextual to user's roles and current profile state
- System must provide clear guidance on how to implement suggestions
- Suggestion frequency must not overwhelm users

### 4. Role-Specific Features

#### 4.1 Founder Features
- System must collect and display company stage information
- Founders must be able to document previous ventures and experience
- System must store founder's goals and target markets
- Founder profiles must emphasize relevant entrepreneurial information

#### 4.2 Service Provider Features
- System must support detailed service listings with categories
- Service providers must be able to specify rates and availability
- System must support portfolio/work samples (future enhancement)
- Expertise levels must be clearly indicated

#### 4.3 Company Member Features
- System must validate company invitation codes
- Company members must be associated with their company
- System must collect role information within the company
- Company-specific skills and responsibilities must be captured

### 5. Integration Requirements

#### 5.1 Authentication Integration
- Onboarding flow must integrate with existing authentication system
- New users must be directed to onboarding after signup
- Returning users must continue from their last incomplete step
- Authentication state must be maintained throughout the flow

#### 5.2 Navigation Integration
- Profile completion status must be visible in global navigation
- Notification indicators must appear in appropriate navigation elements
- Role-specific navigation items must appear based on selected roles
- Quick access to profile editing must be available from navigation

#### 5.3 Existing Service Compatibility
- Profile changes must not break existing services (Idea Playground, Standup Bot, Task Generation)
- Compatibility views must provide necessary data to legacy components
- Performance impact on existing services must be minimal
- Data integrity must be maintained across systems

## Technical Requirements

### 1. Database Requirements

#### 1.1 Schema Design
- Database schema must extend existing profile table without breaking functionality
- New tables must use proper relationships and constraints
- Database indices must be optimized for common queries
- Migration must be reversible in case of issues

#### 1.2 Data Migration
- Existing user profile data must be preserved during migration
- System must handle incomplete or inconsistent data gracefully
- Migration must be transactional to prevent partial updates
- Validation must verify data integrity after migration

#### 1.3 Performance
- Database queries must complete within 200ms for profile retrieval
- System must efficiently cache frequently accessed profile data
- Completion calculation must be optimized to minimize database load
- Connection pooling must be properly configured for scalability

### 2. Frontend Requirements

#### 2.1 Component Architecture
- UI components must follow a modular, reusable design
- Components must support multiple device sizes and orientations
- Form components must include proper validation and error handling
- Visual design must be consistent with existing application

#### 2.2 State Management
- Profile data must be centrally managed for consistency
- State updates must be optimistic with proper error handling
- Caching strategy must minimize unnecessary network requests
- Context providers must efficiently manage shared state

#### 2.3 User Experience
- UI must follow accessibility guidelines (WCAG 2.1 AA compliance)
- Forms must maintain state between sessions
- Transitions between steps must be smooth and performant
- Loading states must be clearly indicated

### 3. API Requirements

#### 3.1 Service Layer
- API endpoints must follow RESTful principles
- Authentication and authorization must be enforced on all endpoints
- Response formats must be consistent and well-documented
- Error handling must provide clear, actionable messages

#### 3.2 Security
- All API requests must validate input data
- Authorization checks must prevent unauthorized profile access
- Sensitive profile data must be appropriately protected
- Rate limiting must be implemented for public endpoints

#### 3.3 Performance
- API response times must be under 300ms for 95% of requests
- Requests must be optimized to minimize database queries
- Caching must be implemented for appropriate endpoints
- Batch operations must be supported for bulk updates

### 4. Integration Requirements

#### 4.1 Service Compatibility
- API must maintain backward compatibility for existing services
- Compatibility layer must translate between old and new data formats
- Performance impact on existing services must be minimal
- Error handling must gracefully degrade in case of issues

#### 4.2 Analytics Integration
- System must track key metrics for onboarding funnel
- Profile completion analytics must be captured for monitoring
- User engagement with notifications must be measured
- A/B testing infrastructure must be supported

## Non-Functional Requirements

### 1. Performance Requirements
- Pages must load within 2 seconds on standard connections
- UI interactions must feel responsive (< 100ms response)
- Database queries must be optimized for common operations
- System must handle at least 100 concurrent users

### 2. Security Requirements
- All user data must be protected following security best practices
- Row-Level Security must be implemented for all tables
- Input validation must be performed on all user-provided data
- Authentication must be required for accessing private profile data

### 3. Scalability Requirements
- Architecture must support growing user base
- Database design must scale efficiently with additional profiles
- Component design must accommodate additional role types
- Service layer must support increased request volume

### 4. Reliability Requirements
- System must have 99.9% uptime during business hours
- Data backups must be performed daily
- Error handling must provide graceful degradation
- Monitoring must detect and alert on system issues

### 5. Compatibility Requirements
- System must function on all major browsers (Chrome, Safari, Firefox, Edge)
- Mobile responsiveness must support iOS and Android devices
- System must maintain compatibility with existing services
- Accessibility must meet WCAG 2.1 AA standards

================
File: docs/enhanced-profile-system/TECHNICAL_ARCHITECTURE.md
================
# Technical Architecture: Enhanced Profile System

## Overview

The Enhanced Profile System is designed to provide a flexible, comprehensive user profile system with role-based customization and completion tracking, while maintaining compatibility with existing services. This document outlines the technical architecture, design decisions, and implementation considerations.

## System Components

### 1. Database Schema

The system builds upon the existing profile structure while adding new tables for enhanced functionality:

```
┌─────────────────┐       ┌────────────────────┐       ┌──────────────────────┐
│   profiles      │       │  profile_sections  │       │  professional_services│
│ (existing table)│       │  (new table)       │       │  (new table)          │
└────────┬────────┘       └─────────┬──────────┘       └───────────┬───────────┘
         │                          │                               │
         │                          │                               │
         │                          │                               │
┌────────┴────────┐       ┌─────────┴──────────┐       ┌───────────┴───────────┐
│ user_education  │       │profile_notifications│       │  user_work_experience │
│ (new table)     │       │(new table)          │       │  (new table)          │
└─────────────────┘       └────────────────────┘        └─────────────────────┘
```

**Key Database Design Decisions:**
1. **Enhancing vs. Replacing:** We add to the existing `profiles` table rather than replacing it to ensure compatibility.
2. **Sectional Approach:** Using `profile_sections` for modular, role-specific information.
3. **Specialized Tables:** Dedicated tables for complex data like work experience and education.
4. **Compatibility Views:** Using database views to provide compatibility with existing code.

### 2. Services Layer

The services layer mediates between the database and UI components:

```
┌───────────────────────┐
│ ProfileService        │
├───────────────────────┤
│ - getProfile()        │
│ - updateProfile()     │
│ - getProfileSections()│
│ - updateSection()     │
└─────────┬─────────────┘
          │
          │
┌─────────┼─────────────┐         ┌───────────────────────┐
│ CompletionService     │◄────────┤NotificationService    │
├───────────────────────┤         ├───────────────────────┤
│ - calculateCompletion()│        │ - createNotification()│
│ - getSectionStatus()   │        │ - getNotifications()  │
└───────────────────────┘         └───────────────────────┘
```

**Key Service Design Decisions:**
1. **Service Separation:** Distinct services for different concerns (profile data, completion calculation, notifications).
2. **Compatibility Layer:** Services that support both old and new profile formats.
3. **Event-Driven Updates:** Completion calculations trigger notification creation when thresholds are reached.

### 3. Data Models

Core TypeScript interfaces that define the system:

```typescript
// Core profile information
interface Profile {
  id: string;
  full_name: string;
  email: string;
  avatar_url?: string;
  bio?: string;
  company_id?: string; // Maintained for compatibility
  company_role?: string; // Maintained for compatibility
  primary_role?: string; // New field
  additional_roles?: string[]; // New field
  completion_percentage: number; // New field
  is_public: boolean;
  // ... other existing fields
}

// Profile section for modular completion tracking
interface ProfileSection {
  id: string;
  user_id: string;
  section_key: string;
  title: string;
  description?: string;
  completion_percentage: number;
  is_required: boolean;
  display_order: number;
  required_fields: {field: string}[];
  optional_fields: {field: string}[];
  is_role_specific: boolean;
  applicable_roles?: string[];
  last_updated: string;
}

// Work experience entry
interface WorkExperience {
  id: string;
  user_id: string;
  company_name: string;
  title: string;
  start_date?: Date;
  end_date?: Date;
  is_current: boolean;
  description?: string;
  location?: string;
  skills?: string[];
}

// Service offering
interface ProfessionalService {
  id: string;
  user_id: string;
  category: string;
  title: string;
  description?: string;
  expertise_level: 'beginner' | 'intermediate' | 'advanced' | 'expert';
  rate_type: 'hourly' | 'project' | 'retainer';
  rate_range?: {min?: number; max?: number; currency?: string};
  availability: 'part_time' | 'full_time' | 'contract';
  is_active: boolean;
}

// Profile notification
interface ProfileNotification {
  id: string;
  user_id: string;
  type: 'milestone' | 'section' | 'inactivity' | 'quality' | 'recommendation';
  priority: 'critical' | 'important' | 'standard' | 'informational';
  title: string;
  description?: string;
  action_url?: string;
  action_label?: string;
  icon?: string;
  is_read: boolean;
  dismissible: boolean;
  created_at: string;
  expires_at?: string;
}
```

### 4. Component Architecture

UI components follow a hierarchical organization:

```
┌─────────────────────────────┐
│ OnboardingWizard            │
├─────────────────────────────┤
│ ┌───────────────────────┐   │
│ │ RoleSelectionStep     │   │
│ └───────────────────────┘   │
│ ┌───────────────────────┐   │
│ │ CompanyStageStep      │   │
│ └───────────────────────┘   │
│ ┌───────────────────────┐   │
│ │ InviteCodeStep        │   │
│ └───────────────────────┘   │
│ ┌───────────────────────┐   │
│ │ FeatureRecommendations│   │
│ └───────────────────────┘   │
└─────────────────────────────┘

┌─────────────────────────────┐
│ ProfileBuilder              │
├─────────────────────────────┤
│ ┌───────────────────────┐   │
│ │ ProfileNavigation     │   │
│ └───────────────────────┘   │
│ ┌───────────────────────┐   │
│ │ SectionEditor         │   │
│ └───────────────────────┘   │
│ ┌───────────────────────┐   │
│ │ CompletionTracker     │   │
│ └───────────────────────┘   │
│ ┌───────────────────────┐   │
│ │ NotificationCenter    │   │
│ └───────────────────────┘   │
└─────────────────────────────┘
```

**Key Component Design Decisions:**
1. **Modular Design:** Independent, reusable components that can be composed.
2. **Progressive Disclosure:** Components that reveal complexity progressively.
3. **Conditional Rendering:** Components that adapt based on user roles and context.
4. **State Management:** Centralized state for profile data with context providers.

## Integration with Existing Systems

### 1. Backward Compatibility Approach

The migration preserves compatibility with existing systems through several strategies:

1. **Non-destructive Schema Changes:** Only adding to existing tables, never removing columns.
2. **Database Views:** Creating views that combine old and new data structures.
3. **Service Layer Adapters:** Services that can work with both data models.
4. **Feature Flagging:** Gradual rollout with the ability to revert to old systems.

### 2. Integration Touchpoints

Key points where the new system interacts with existing components:

- **Authentication Flow:** Redirect to new onboarding after signup
- **Layout Components:** Enhanced profile UI in navigation
- **Idea Playground:** Respecting existing profile references and company_id 
- **Standup Bot:** Maintaining compatibility with profile queries
- **Task Generation:** Ensuring services can still access profile information

## Implementation Phases

The implementation will follow a phased approach:

### Phase 1: Database Foundation (1-2 weeks)
- Create new database tables and relationships
- Add compatibility views
- Implement basic profile service

### Phase 2: Onboarding Flow (2-3 weeks)
- Implement role selection components
- Create role-specific question flows
- Build feature recommendation system

### Phase 3: Profile Builder (3-4 weeks)
- Implement profile section components
- Build completion tracking system
- Create notification components

### Phase 4: Integration & Testing (2 weeks)
- Connect with existing services
- Perform comprehensive compatibility testing
- Address any integration issues

## Technical Considerations

### 1. Performance Optimizations
- Use of database indices for frequently queried fields
- Efficient completion calculation with appropriate caching
- Lazy loading of profile sections in the UI

### 2. Security Measures
- Row-Level Security policies for all new tables
- Careful permission checks in service layer
- Input validation for all profile updates

### 3. Scalability Considerations
- Database design supports growing number of profiles and sections
- Service architecture allows for future extensions
- Component design accommodates additional role types

## Future Enhancements

Potential future enhancements to the system:

1. **Advanced Role Management:** More sophisticated role permissions and visibility.
2. **Machine Learning Integration:** Smarter profile completion recommendations.
3. **Social Features:** Connections, endorsements, and network visualization.
4. **Integration APIs:** External system integration with the profile system.

================
File: docs/enhanced-profile-system/USER_STORIES.md
================
# User Stories for Enhanced Profile System

## Core User Stories

### Onboarding Experience

1. **Initial Role Selection**
   ```
   As a new user
   I want to select my primary role(s) during onboarding
   So that I can receive a personalized experience relevant to my needs
   ```
   - **Acceptance Criteria:**
     - User can select from roles: Founder, Company Member, or Service Provider
     - Multiple roles can be selected if applicable
     - Primary role is clearly designated
     - Role selection influences subsequent onboarding steps

2. **Role-Specific Onboarding Questions**
   ```
   As a user with a selected role
   I want to answer questions specific to my role
   So that I can provide the most relevant information about myself
   ```
   - **Acceptance Criteria:**
     - Founder: Asked about company stage (idea stage, solid idea, existing company)
     - Service Provider: Asked about services offered, expertise level
     - Company Member: Presented with invite code entry field
     - Questions are contextual and logically sequenced

3. **Onboarding Progress Indication**
   ```
   As a user going through onboarding
   I want to see my progress through the onboarding process
   So that I know how much remains to be completed
   ```
   - **Acceptance Criteria:**
     - Clear progress indicator shows current position in flow
     - Step labels indicate completed steps and upcoming steps
     - Visual cues reinforce progress (colors, animations)
     - Option to go back to previous steps

4. **Feature Recommendations**
   ```
   As a new user completing onboarding
   I want to receive personalized feature recommendations
   So that I can quickly discover the most relevant tools for my needs
   ```
   - **Acceptance Criteria:**
     - Features highlighted based on selected role(s) and onboarding answers
     - Clear descriptions of why each feature is recommended
     - Direct links to access recommended features
     - Option to explore additional features

### Profile Management

5. **Comprehensive Profile Creation**
   ```
   As a user
   I want to create a comprehensive profile with relevant professional information
   So that I can effectively represent myself on the platform
   ```
   - **Acceptance Criteria:**
     - Basic profile fields for all users (name, photo, bio, etc.)
     - Role-specific sections automatically shown/hidden based on roles
     - Information is organized in logical, manageable sections
     - Preview of how profile appears to others

6. **Profile Completion Tracking**
   ```
   As a user
   I want to see my profile completion status
   So that I know what information I still need to provide
   ```
   - **Acceptance Criteria:**
     - Overall completion percentage prominently displayed
     - Section-by-section completion breakdown
     - Clear indication of required vs. optional fields
     - Visual indicators for incomplete sections

7. **Profile Completion Notifications**
   ```
   As a user with an incomplete profile
   I want to receive helpful notifications about completing my profile
   So that I'm encouraged to provide important information
   ```
   - **Acceptance Criteria:**
     - Milestone notifications for completion thresholds (25%, 50%, 75%, 100%)
     - Non-intrusive reminders for important missing information
     - Clear messaging about benefits of completing profile
     - Easy navigation to sections that need completion

8. **Multiple Role Management**
   ```
   As a user with multiple roles
   I want to manage how I present myself in different contexts
   So that I can tailor my profile to different audiences
   ```
   - **Acceptance Criteria:**
     - Ability to select which role to present as primary
     - Role-specific information is grouped logically
     - Clear method to add/remove roles
     - Option to customize visibility of information by role

### Service Provider Specific

9. **Service Listing Creation**
   ```
   As a service provider
   I want to create detailed listings of my professional services
   So that potential clients can understand what I offer
   ```
   - **Acceptance Criteria:**
     - Fields for service title, description, and category
     - Options for pricing model (hourly, project, retainer)
     - Ability to set expertise level
     - Multiple services can be added to a single profile

10. **Professional Credentials**
    ```
    As a service provider
    I want to showcase my professional credentials and work history
    So that I can establish credibility with potential clients
    ```
    - **Acceptance Criteria:**
      - Fields for education, certifications, and work experience
      - Option to highlight relevant projects/case studies
      - Ability to add skill proficiency levels
      - Section for client testimonials (future feature)

### Company Member Specific

11. **Company Invitation**
    ```
    As a company member
    I want to join my company using an invitation code
    So that I can be properly associated with my organization
    ```
    - **Acceptance Criteria:**
      - Clear invitation code entry field
      - Validation of code with helpful error messages
      - Confirmation of company details before joining
      - Automatic association with company after successful verification

12. **Company Role Information**
    ```
    As a company member
    I want to specify my role within the company
    So that others understand my position and responsibilities
    ```
    - **Acceptance Criteria:**
      - Fields for job title, department, and responsibilities
      - Company information automatically linked to profile
      - Option to add skills specific to company role
      - Visibility controls for company information

### Founder Specific

13. **Company Stage Information**
    ```
    As a founder
    I want to indicate my company's current stage
    So that I receive relevant resources and connections
    ```
    - **Acceptance Criteria:**
      - Clear options for company stage (idea, formed, operational)
      - Tailored follow-up questions based on selected stage
      - Appropriate feature recommendations for each stage
      - Option to update stage as company progresses

14. **Founder Experience Profile**
    ```
    As a founder
    I want to showcase my founder experience and previous ventures
    So that I can establish credibility with potential partners
    ```
    - **Acceptance Criteria:**
      - Fields for previous startup experience
      - Option to add funding history
      - Section for achievements and exits
      - Links to previous companies

================
File: docs/examples/AdvancedVisualizationExample.tsx
================
import React, { useState, useEffect } from 'react';
import { useParams, useNavigate } from 'react-router-dom';
import { useCompany } from '@/lib/hooks/useCompany';
import { useAuth } from '@/lib/hooks/useAuth';
import {
  InteractiveJourneyMap,
  MilestoneCelebrationAnimation
} from '@/components/visualization';
import { journeyStepsService } from '@/lib/services/journeySteps.service';
⋮----
const fetchData = async () =>
⋮----
const handleStepClick = (stepId: string) =>
const handleStepCompletion = async (stepId: string) =>
⋮----
// Show celebration animation
⋮----
const handleCloseCelebration = () =>

================
File: docs/examples/FeedbackSystemIntegration.tsx
================
import React, { useState, useEffect } from 'react';
import { useParams } from 'react-router-dom';
import { useCompany } from '@/lib/hooks/useCompany';
import { useAuth } from '@/lib/hooks/useAuth';
import { JourneyStepDetails } from '@/components/company/journey/JourneyStepDetails';
import { StepAssistant } from '@/components/company/journey/StepAssistant';
import { StepRecommendations } from '@/components/company/journey/StepRecommendations';
import { InlineRatingComponent, StepImprovementSuggestionForm } from '@/components/feedback';
import { FeedbackService } from '@/lib/services/feedback.service';
import { journeyStepsService } from '@/lib/services/journeySteps.service';
import { motion, AnimatePresence } from 'framer-motion';
interface StepData {
  id: string;
  name: string;
  description: string;
  phase_id: string;
  phase_name: string;
  difficulty_level: number;
  estimated_time_min: number;
  estimated_time_max: number;
  status?: string;
}
⋮----
const fetchStepData = async () =>
⋮----
const fetchFeedbackData = async () =>
⋮----
const handleRatingSubmit = async (rating: number, comment: string) =>
⋮----
{/* Improvement suggestion form */}

================
File: docs/examples/JourneyHooksExample.tsx
================
import React, { useState } from 'react';
import {
  useCompanyJourney,
  useStepProgress,
  useJourneySteps,
  useJourneyTools
} from '../../src/lib/hooks';
⋮----
const handleStatusChange = async (stepId: string, status: 'not_started' | 'in_progress' | 'completed' | 'skipped') =>
⋮----
const handleToolSelect = async (toolId: string) =>
const handleToolRating = async (toolId: string) =>
⋮----
// Success notification could be added here
⋮----
// Error handling could be added here
⋮----
onChange=
⋮----

================
File: docs/examples/StepAssistantIntegration.tsx
================
import React, { useEffect, useState } from 'react';
import { useParams } from 'react-router-dom';
import { useCompany } from '@/lib/hooks/useCompany';
import { JourneyStepDetails } from '@/components/company/journey/JourneyStepDetails';
import { StepAssistant } from '@/components/company/journey/StepAssistant';
import { StepRecommendations } from '@/components/company/journey/StepRecommendations';
import { journeyStepsService } from '@/lib/services/journeySteps.service';
interface StepData {
  id: string;
  name: string;
  description: string;
  phase_id: string;
  phase_name: string;
  difficulty_level: number;
  estimated_time_min: number;
  estimated_time_max: number;
  status?: string;
}
⋮----
const fetchStepData = async () =>
⋮----
onStatusChange=

================
File: docs/huggingface-spaces-integration/API_KEY_SETUP_GUIDE.md
================
# Hugging Face API Key Setup Guide

This guide will help you properly set up a working Hugging Face API key for use in the application.

## Overview

The application can use either OpenAI or Hugging Face as the AI provider. Using Hugging Face requires:
1. A valid Hugging Face account
2. A correctly configured API key with the right permissions
3. Access to the models you want to use

## Creating a New API Key

Your current API key (`hf_HoW01cEmvCFamyyHQCiPwWTgiNqVhajikp`) is failing authentication. Let's create a new one:

1. Go to [Hugging Face Settings](https://huggingface.co/settings/tokens)
2. Sign in to your Hugging Face account if you're not already logged in
3. Under "User Access Tokens", click "New token"
4. Configure the token with these settings:
   - **Name**: `Wheel99 Application` (or any name you prefer)
   - **Role**: Select "Write" role (this includes read permissions plus ability to create model repos)
   - **Expiration**: Choose a suitable expiration period (e.g., 30 days)
5. Click "Generate a token"
6. **Important**: Copy the token immediately - you won't be able to see it again!

## Configuring the Application

1. Go to "Settings" in the application
2. Select the "Integrations" tab
3. In the Hugging Face Settings section:
   - Enter your new API key
   - Check the "Enable Hugging Face" box if you want to use it as your primary AI provider
   - Configure model IDs for each tier (or leave the default suggestions)
   - Click "Test Connection" to verify your key works
   - Save the settings

## Model Configuration

For optimal results, configure these models (or alternatives if you prefer):

- **Base Model**: `mistralai/Mistral-7B-Instruct-v0.2` (default, general usage)
- **Company Model**: `meta-llama/Llama-2-13b-chat-hf` (business context)
- **Abstraction Model**: `google/flan-t5-xxl` (concept abstraction)

## Troubleshooting

If you encounter API key issues:

1. **Verify Token Format**: All Hugging Face tokens start with "hf_"
2. **Check Key Permissions**: Make sure you selected at least "Read" role when creating the token
3. **Verify Token Status**: Go to Hugging Face Settings to confirm the token is active
4. **Model Access**: Some models like Llama-2 require special access - make sure your account has permission
5. **Generate New Token**: If problems persist, revoke the old token and generate a new one

## Advanced Testing

The application includes diagnostic tools to help troubleshoot Hugging Face integration:

```bash
# Test API key directly with Hugging Face API
node scripts/key-validator.js YOUR_API_KEY

# Test specific models
node scripts/test-huggingface-api-key-direct.js YOUR_API_KEY mistralai/Mistral-7B-Instruct-v0.2
```

## Important Notes

- Hugging Face has rate limits for free accounts that may affect performance
- Some models require specific format for prompts - check the model card for details
- Model download time can be significant for first-time use

## Support

If you continue to have issues with Hugging Face integration:
1. Verify your account is in good standing at [Hugging Face](https://huggingface.co/)
2. Check the Hugging Face status page for service disruptions
3. Consider using a different model that you know your account has access to

================
File: docs/huggingface-spaces-integration/IMPLEMENTATION_GUIDE.md
================
# Implementation Guide: Hugging Face Spaces Triple LLM

This document provides step-by-step instructions for implementing the Hugging Face Spaces Triple LLM system in our application.

## Prerequisites

- Hugging Face account with API access
- Access to our application's codebase
- Basic understanding of TypeScript and React
- Familiarity with our existing LLM implementation

## Step 1: Create Required Types

Create a new file: `src/types/huggingface.types.ts`

```typescript
export interface HuggingFaceSpaceResponse {
  generated_text: string;
  model_version?: string;
  generation_time?: number;
  error?: string;
}

export interface HuggingFaceModelInfo {
  modelId: string;
  version: string;
  description: string;
  metrics?: Record<string, any>;
}
```

## Step 2: Implement Hugging Face Client

Create a new file: `src/lib/huggingface-client.ts`

```typescript
import { HuggingFaceSpaceResponse } from '../types/huggingface.types';

// Initialize the HuggingFace client with API key from environment variables
const huggingface = {
  apiKey: import.meta.env.VITE_HUGGINGFACE_API_KEY,
  organization: import.meta.env.VITE_HUGGINGFACE_ORG || 'your-company-org',
  
  // Space names
  spaces: {
    base: import.meta.env.VITE_HF_BASE_SPACE || 'company-base-llm',
    company: import.meta.env.VITE_HF_COMPANY_SPACE || 'company-specific-llm',
    abstraction: import.meta.env.VITE_HF_ABSTRACTION_SPACE || 'company-abstraction-llm'
  },
  
  // Generate text using a Space endpoint
  async generate(spaceName: string, prompt: string, params = {}): Promise<HuggingFaceSpaceResponse> {
    try {
      const orgPrefix = this.organization ? `${this.organization}/` : '';
      const url = `https://huggingface.co/spaces/${orgPrefix}${spaceName}/api/generate`;
      
      const response = await fetch(url, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${this.apiKey}`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({ prompt, params })
      });
      
      if (!response.ok) {
        throw new Error(`Space API error: ${response.status}`);
      }
      
      return await response.json();
    } catch (error) {
      console.error('HuggingFace client error:', error);
      return {
        generated_text: '',
        error: error instanceof Error ? error.message : String(error)
      };
    }
  }
};

export default huggingface;
```

## Step 3: Create Hugging Face LLM Service Implementation

Create a new file: `src/lib/services/huggingface-llm.service.ts`

```typescript
import huggingface from '../huggingface-client';
import { supabase } from '../supabase';
import { loggingService } from './logging.service';
import { QueryContext, GeneralLLMService } from './general-llm.service';
import { useAuthStore } from '../store';

export class HuggingFaceGeneralLLMService implements GeneralLLMService {
  async query(input: string, context: QueryContext): Promise<any> {
    const startTime = Date.now();
    
    // Log the AI interaction start
    const interactionId = await loggingService.logAIInteraction(
      'query_start',
      {
        model: "huggingface-spaces",
        input_text: input,
        context_type: context.context || 'general',
        user_id: context.userId,
        company_id: context.companyId,
        features: {
          useCompanyModel: context.useCompanyModel || false,
          useAbstraction: context.useAbstraction || false,
          useExistingModels: context.useExistingModels || false
        }
      }
    );
    
    try {
      // Get the feature flags for Hugging Face
      const { featureFlags } = useAuthStore.getState();
      
      // Determine which Space to use based on context and feature flags
      let spaceName = huggingface.spaces.base;
      
      if (context.useCompanyModel && featureFlags.useHFCompanyModel?.enabled) {
        spaceName = huggingface.spaces.company;
      } else if (context.useAbstraction && featureFlags.useHFAbstractionModel?.enabled) {
        spaceName = huggingface.spaces.abstraction;
      } else if (!featureFlags.useHFBaseModel?.enabled) {
        // Fallback to OpenAI if no Hugging Face models are enabled
        throw new Error('No Hugging Face models are enabled');
      }
      
      // Prepare parameters for the Space
      const params = {
        max_length: 1024,
        temperature: context.temperature || 0.7
      };
      
      // Add context-specific parameters
      if (context.useCompanyModel && context.companyId) {
        Object.assign(params, {
          context: JSON.stringify({
            company_id: context.companyId
          })
        });
      }
      
      // Add conversation history if available
      if (context.conversationHistory?.length > 0) {
        const formattedHistory = context.conversationHistory
          .map(msg => `${msg.role}: ${msg.content}`)
          .join('\n');
          
        Object.assign(params, {
          conversation_history: formattedHistory
        });
      }
      
      // Call the Hugging Face Space
      const response = await huggingface.generate(spaceName, input, params);
      
      const duration = Date.now() - startTime;
      
      // Log successful interaction
      await loggingService.logAIInteraction(
        'query_complete',
        {
          interaction_id: interactionId,
          model: "huggingface-spaces",
          space_name: spaceName,
          input_text: input,
          output_text: response.generated_text,
          duration_ms: duration,
          status: 'success'
        }
      );
      
      // Return in the format expected by the system
      return {
        content: response.generated_text,
        role: 'assistant'
      };
    } catch (error) {
      console.error('Error in Hugging Face LLM query:', error);
      
      // Log error
      await loggingService.logAIInteraction(
        'query_error',
        {
          interaction_id: interactionId,
          model: "huggingface-spaces",
          input_text: input,
          error_message: error instanceof Error ? error.message : String(error),
          duration_ms: Date.now() - startTime,
          status: 'error'
        }
      );
      
      throw error;
    }
  }
}

export const huggingFaceLLMService = new HuggingFaceGeneralLLMService();
```

## Step 4: Update Feature Flags

Update the default feature flags in `src/lib/store.ts`:

```typescript
const defaultFeatureFlags: FeatureFlags = {
  // Existing flags...
  
  // LLM Provider flags
  useOpenAI: { enabled: true, visible: true },
  useHuggingFaceSpaces: { enabled: false, visible: true },
  
  // Hugging Face specific flags
  useHFBaseModel: { enabled: true, visible: true },
  useHFCompanyModel: { enabled: false, visible: true },
  useHFAbstractionModel: { enabled: false, visible: true }
};
```

## Step 5: Update LLM Service Factory

Modify `src/lib/services/general-llm.service.ts` to include the Hugging Face service:

```typescript
// Import the Hugging Face service
import { huggingFaceLLMService } from './huggingface-llm.service';

// Update the getLLMService function
const getLLMService = (): GeneralLLMService => {
  const { featureFlags } = useAuthStore.getState();
  
  console.log('Feature flags state:', {
    useRealAI: featureFlags.useRealAI?.enabled,
    useMockAI: featureFlags.useMockAI?.enabled,
    useHuggingFaceSpaces: featureFlags.useHuggingFaceSpaces?.enabled,
    useMultiTieredAI: featureFlags.useMultiTieredAI?.enabled
  });
  
  // Check if Hugging Face should be used - highest priority
  if (featureFlags.useHuggingFaceSpaces?.enabled) {
    console.log('Using Hugging Face Spaces LLM Service');
    try {
      return huggingFaceLLMService;
    } catch (error) {
      console.error('Error initializing Hugging Face service:', error);
      // Continue to fallback options
    }
  }
  
  // Existing OpenAI logic
  if (featureFlags.useRealAI?.enabled) {
    // Your existing code for real OpenAI vs. mock
    // ...
  }
  
  // Mock service is the ultimate fallback
  console.log('Using Mock General LLM Service');
  return mockGeneralLLMService;
};
```

## Step 6: Add Feature Flag Group to FeatureFlagsSettings Component

Update the `featureGroups` array in `src/components/admin/FeatureFlagsSettings.tsx`:

```typescript
const featureGroups: FeatureGroup[] = [
  // Existing groups...
  
  {
    name: 'LLM Providers',
    description: 'Control which LLM providers are used',
    features: [
      { 
        key: 'useOpenAI', 
        name: 'Use OpenAI', 
        description: 'Use OpenAI for LLM capabilities' 
      },
      { 
        key: 'useHuggingFaceSpaces', 
        name: 'Use Hugging Face Spaces', 
        description: 'Use Hugging Face Spaces for LLM capabilities' 
      },
      { 
        key: 'useHFBaseModel', 
        name: 'Use HF Base Model', 
        description: 'Use base model in triple LLM implementation' 
      },
      { 
        key: 'useHFCompanyModel', 
        name: 'Use HF Company Model', 
        description: 'Use company-specific model in triple LLM implementation' 
      },
      { 
        key: 'useHFAbstractionModel', 
        name: 'Use HF Abstraction Model', 
        description: 'Use abstraction model in triple LLM implementation' 
      }
    ]
  }
];
```

## Step 7: Update Environment Variables

Add the following environment variables to your `.env` file:

```
# Hugging Face Configuration
VITE_HUGGINGFACE_API_KEY=your-huggingface-key
VITE_HUGGINGFACE_ORG=your-company-name

# Space names (optional - defaults are defined in the client)
VITE_HF_BASE_SPACE=company-base-llm
VITE_HF_COMPANY_SPACE=company-specific-llm
VITE_HF_ABSTRACTION_SPACE=company-abstraction-llm
```

## Step 8: Add Database Schema Changes

Create a migration file for the necessary database schema changes:

```sql
-- Add Hugging Face specific columns to model_registry
ALTER TABLE model_registry ADD COLUMN IF NOT EXISTS hf_space_url TEXT;
ALTER TABLE model_registry ADD COLUMN IF NOT EXISTS hf_model_id TEXT;
ALTER TABLE model_registry ADD COLUMN IF NOT EXISTS hf_space_id TEXT;

-- Add a new table for Space configurations
CREATE TABLE IF NOT EXISTS hf_spaces_config (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  space_name TEXT NOT NULL,
  space_type TEXT NOT NULL, -- 'base', 'company', 'abstraction'
  api_endpoint TEXT NOT NULL,
  is_active BOOLEAN DEFAULT TRUE,
  model_version TEXT,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT now(),
  updated_at TIMESTAMP WITH TIME ZONE DEFAULT now()
);

-- Table for tracking Space deployments
CREATE TABLE IF NOT EXISTS hf_deployments (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  space_id UUID REFERENCES hf_spaces_config(id),
  model_id UUID REFERENCES model_registry(id),
  deployment_status TEXT NOT NULL,
  deployment_logs TEXT,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT now(),
  updated_at TIMESTAMP WITH TIME ZONE DEFAULT now()
);
```

## Step 9: Set Up Model Training Service Interface

Extend the model training service to support Hugging Face training:

```typescript
// Add these methods to the ModelTrainingService class in src/lib/services/model-training.service.ts

/**
 * Push a trained model to Hugging Face Model Hub
 * @param modelId The ID of the model in the local registry
 * @param options Options for pushing to Hugging Face
 * @returns The Hugging Face Model ID
 */
async pushToHuggingFace(
  modelId: string,
  options: {
    hfRepoName?: string;
    description?: string;
    tags?: string[];
  } = {}
): Promise<string | null> {
  try {
    // Get the model from the registry
    const model = await this.getModel(modelId);
    if (!model) {
      throw new Error(`Model not found: ${modelId}`);
    }
    
    // Use the model's name if no repo name is provided
    const repoName = options.hfRepoName || `${model.model_name.toLowerCase().replace(/\s+/g, '-')}`;
    
    // Log the push attempt
    loggingService.logEvent({
      event_type: 'model_training',
      event_source: 'huggingface',
      action: 'push_model_attempt',
      data: {
        model_id: modelId,
        model_name: model.model_name,
        hf_repo_name: repoName
      }
    });
    
    // In a real implementation, you would use the Hugging Face JS SDK or API
    // to push the model files to the Model Hub
    
    // Update the model registry with the Hugging Face information
    const { error } = await supabase
      .from('model_registry')
      .update({
        hf_model_id: repoName
      })
      .eq('id', modelId);
    
    if (error) {
      throw error;
    }
    
    // Log successful push
    loggingService.logEvent({
      event_type: 'model_training',
      event_source: 'huggingface',
      action: 'push_model_success',
      data: {
        model_id: modelId,
        model_name: model.model_name,
        hf_repo_name: repoName
      }
    });
    
    return repoName;
  } catch (error) {
    console.error('Error pushing model to Hugging Face:', error);
    loggingService.logError(
      error instanceof Error ? error : new Error(String(error)),
      'ModelTrainingService',
      {
        method: 'pushToHuggingFace',
        modelId
      }
    );
    return null;
  }
}

/**
 * Deploy a model to a Hugging Face Space
 * @param modelId The ID of the model in the local registry
 * @param spaceId The ID of the Hugging Face Space
 * @returns Whether the deployment was successful
 */
async deployToSpace(
  modelId: string,
  spaceId: string
): Promise<boolean> {
  try {
    // Get the model and space information
    const model = await this.getModel(modelId);
    if (!model) {
      throw new Error(`Model not found: ${modelId}`);
    }
    
    const { data: spaceConfig, error: spaceError } = await supabase
      .from('hf_spaces_config')
      .select('*')
      .eq('id', spaceId)
      .single();
    
    if (spaceError || !spaceConfig) {
      throw new Error(`Space not found: ${spaceId}`);
    }
    
    // Log the deployment attempt
    loggingService.logEvent({
      event_type: 'model_training',
      event_source: 'huggingface',
      action: 'deploy_model_attempt',
      data: {
        model_id: modelId,
        model_name: model.model_name,
        space_id: spaceId,
        space_name: spaceConfig.space_name
      }
    });
    
    // Create a deployment record
    const { data: deployment, error: deploymentError } = await supabase
      .from('hf_deployments')
      .insert({
        space_id: spaceId,
        model_id: modelId,
        deployment_status: 'in_progress'
      })
      .select()
      .single();
    
    if (deploymentError || !deployment) {
      throw new Error('Failed to create deployment record');
    }
    
    // In a real implementation, you would use the Hugging Face API
    // to update the Space with the new model
    
    // Update the deployment record
    const { error: updateError } = await supabase
      .from('hf_deployments')
      .update({
        deployment_status: 'completed',
        updated_at: new Date().toISOString()
      })
      .eq('id', deployment.id);
    
    if (updateError) {
      throw updateError;
    }
    
    // Log successful deployment
    loggingService.logEvent({
      event_type: 'model_training',
      event_source: 'huggingface',
      action: 'deploy_model_success',
      data: {
        model_id: modelId,
        model_name: model.model_name,
        space_id: spaceId,
        space_name: spaceConfig.space_name,
        deployment_id: deployment.id
      }
    });
    
    return true;
  } catch (error) {
    console.error('Error deploying model to Space:', error);
    loggingService.logError(
      error instanceof Error ? error : new Error(String(error)),
      'ModelTrainingService',
      {
        method: 'deployToSpace',
        modelId,
        spaceId
      }
    );
    return false;
  }
}
```

## Testing the Implementation

1. Set up your Hugging Face Spaces following the [Space Setup Guide](./SPACE_SETUP_GUIDE.md)
2. Add the necessary environment variables
3. Implement the code changes described in this guide
4. Use the admin interface to enable Hugging Face feature flags
5. Test the implementation with simple queries first
6. Gradually test more complex scenarios and all three tiers
7. Monitor logs for any errors or performance issues

## Troubleshooting

- **Authentication Issues**: Ensure your Hugging Face API key is correctly set in the environment variables
- **Space Not Found**: Verify the Space names in your environment variables match your actual Spaces
- **Rate Limiting**: Check if you're hitting Hugging Face API rate limits
- **Response Parsing**: Ensure your Spaces return responses in the expected format
- **Feature Flags**: Confirm the correct feature flags are enabled in the admin interface

================
File: docs/huggingface-spaces-integration/README.md
================
# Hugging Face Spaces Triple LLM Implementation

## Overview

This document outlines our strategy for transitioning from OpenAI to a Hugging Face Spaces-based triple LLM implementation. This approach leverages Hugging Face's managed infrastructure while providing the flexibility to train and deploy custom models on company data and abstracted patterns.

## Why Hugging Face Spaces?

- **Cost Optimization**: Predictable pricing model with potentially lower costs than OpenAI
- **Data Sovereignty**: Full control over model training data and processes
- **Customization**: Models specifically trained for company needs and use cases
- **Flexibility**: Access to a wide range of open-source models and architectures
- **Scalability**: Hugging Face Spaces provides managed infrastructure without self-hosting

## Triple LLM Architecture

The implementation consists of three specialized LLM tiers:

1. **Base Model**: General-purpose LLM for non-company-specific queries
2. **Company Model**: LLM fine-tuned with company-specific data and context
3. **Abstraction Model**: LLM trained on abstracted patterns across similar businesses

This tiered approach allows us to balance general knowledge with company-specific expertise and higher-level business pattern recognition.

## Key Components

- **Feature Flag System**: Easy toggling between OpenAI and Hugging Face Spaces during development
- **Hugging Face Client**: Adapter for interacting with Hugging Face Spaces API
- **Tiered LLM Service**: Implementation of the GeneralLLMService interface for Hugging Face
- **Logging Integration**: Comprehensive logging of model usage and feedback
- **Admin Interface**: UI for toggling between LLM providers and model tiers

## Documentation Structure

This documentation is organized into several sections:

- [Technical Architecture](./TECHNICAL_ARCHITECTURE.md): Detailed system design and data flow
- [Implementation Guide](./IMPLEMENTATION_GUIDE.md): Step-by-step implementation instructions
- [Space Setup Guide](./SPACE_SETUP_GUIDE.md): Guide for setting up Hugging Face Spaces
- [Testing and Deployment](./TESTING_AND_DEPLOYMENT.md): Testing strategy and deployment plan

## Getting Started

1. Review the [Technical Architecture](./TECHNICAL_ARCHITECTURE.md) document
2. Follow the [Space Setup Guide](./SPACE_SETUP_GUIDE.md) to set up your Hugging Face Spaces
3. Implement the client-side code following the [Implementation Guide](./IMPLEMENTATION_GUIDE.md)
4. Test and deploy according to the [Testing and Deployment](./TESTING_AND_DEPLOYMENT.md) guide

## Environment Variables

The following environment variables are required:

```
# Hugging Face Configuration
VITE_HUGGINGFACE_API_KEY=your-huggingface-key
VITE_HUGGINGFACE_ORG=your-company-name

# Space names (optional - defaults are defined in the client)
VITE_HF_BASE_SPACE=company-base-llm
VITE_HF_COMPANY_SPACE=company-specific-llm
VITE_HF_ABSTRACTION_SPACE=company-abstraction-llm

================
File: docs/huggingface-spaces-integration/SETTINGS_FIX.md
================
# Hugging Face Settings Fix

## Issue
Users were unable to save Hugging Face settings in the UI. The issue was due to the Row Level Security (RLS) policies on the `app_settings` table in the database. The existing policies restricted access to admin users only, and there was no policy for INSERT operations.

## Solution
The fix involves creating a new migration file that:

1. Drops the existing restrictive RLS policies on the `app_settings` table
2. Creates new, more permissive policies that allow:
   - All authenticated users to view app settings
   - All authenticated users to insert app settings
   - All authenticated users to update app settings
3. Adds an `is_admin` column to the profiles table if it doesn't exist, as it was referenced in the previous policies

## Implementation
The fix consists of two files:

1. `supabase/migrations/20250319151900_fix_app_settings_policies.sql` - SQL migration file
2. `scripts/run-huggingface-settings-fix.js` - Helper script to run the migration

### How to apply the fix:

1. Run the migration to update the database policies:
   ```bash
   node scripts/run-huggingface-settings-fix.js
   ```

2. Restart the application server after running the migration.

### After applying the fix:

- Users should now be able to save Hugging Face settings in the Settings UI
- Settings will be properly saved to the `app_settings` table with the key 'huggingface'

## Technical Details

### Previous RLS Policies:
```sql
-- Previous policies only allowed admin users to view/update settings
CREATE POLICY "Admin users can view app settings"
  ON public.app_settings
  FOR SELECT
  USING (EXISTS (SELECT 1 FROM public.profiles WHERE id = auth.uid() AND is_admin = true));

CREATE POLICY "Admin users can update app settings"
  ON public.app_settings
  FOR UPDATE
  USING (EXISTS (SELECT 1 FROM public.profiles WHERE id = auth.uid() AND is_admin = true));
```

### New RLS Policies:
```sql
-- New policies allow any authenticated user to view/insert/update settings
CREATE POLICY "Users can view app settings"
  ON public.app_settings
  FOR SELECT
  USING (auth.uid() IS NOT NULL);

CREATE POLICY "Users can insert app settings"
  ON public.app_settings
  FOR INSERT
  WITH CHECK (auth.uid() IS NOT NULL);

CREATE POLICY "Users can update app settings"
  ON public.app_settings
  FOR UPDATE
  USING (auth.uid() IS NOT NULL);
```

## Related Files
- `src/components/admin/HuggingFaceSettings.tsx` - The Hugging Face settings UI component
- `src/lib/services/app-settings.service.ts` - Service for managing app settings
- `src/pages/SettingsPage.tsx` - Page that includes the Hugging Face settings component

================
File: docs/huggingface-spaces-integration/SETTINGS_TABLE_FIX.md
================
# Hugging Face Settings - Fixed as App-Wide Settings

## Issue

The Hugging Face settings were not saving correctly because of an inconsistency in how the app was handling settings storage. The core issue was:

1. The application has two settings tables:
   - `app_settings`: Global, app-wide settings (shared by all users)
   - `user_settings`: User-specific settings (separate for each user)

2. The `HuggingFaceSettings.tsx` component was trying to store settings directly in the correct table (`app_settings`):
   ```javascript
   const { error: updateError } = await supabase
     .from('app_settings')
     .upsert({
       key: 'huggingface',
       value: settings,
       updated_at: new Date().toISOString()
     }, {
       onConflict: 'key'
     });
   ```

3. However, there was no dedicated method in the `appSettingsService` to handle global settings; it only had methods for user-specific settings:
   ```javascript
   async updateUserSettings(userId: string, settings: Partial<AppSettings>): Promise<AppSettings | null> {
     // ...
   }
   ```

## Solution

Since Hugging Face settings should be app-wide (global) rather than user-specific, we needed to:

1. Add proper global settings methods to the `appSettingsService` 
2. Modify the component to use these new methods instead of direct database access

### Key Changes

1. Added global settings methods to the service:
   ```typescript
   // In app-settings.service.ts
   async getGlobalSettings(key: string): Promise<any> {
     try {
       const { data, error } = await supabase
         .from('app_settings')
         .select('value')
         .eq('key', key)
         .single();

       if (error) {
         console.error(`Error fetching global settings for key ${key}:`, error);
         return null;
       }
       
       return data.value;
     } catch (error) {
       console.error(`Error in getGlobalSettings for key ${key}:`, error);
       return null;
     }
   }
   
   async updateGlobalSettings(key: string, value: any): Promise<any> {
     try {
       const { data, error } = await supabase
         .from('app_settings')
         .upsert({
           key,
           value,
           updated_at: new Date().toISOString()
         }, {
           onConflict: 'key'
         })
         .select()
         .single();

       if (error) {
         console.error(`Error updating global settings for key ${key}:`, error);
         return null;
       }
       
       return data.value;
     } catch (error) {
       console.error(`Error in updateGlobalSettings for key ${key}:`, error);
       return null;
     }
   }
   ```

2. Modified the component to use these new global settings methods:
   ```typescript
   const loadSettings = async () => {
     try {
       const huggingfaceSettings = await appSettingsService.getGlobalSettings('huggingface');
       
       if (huggingfaceSettings) {
         setSettings(huggingfaceSettings);
       }
     } catch (error) {
       console.error('Error loading Hugging Face settings:', error);
     }
   };

   const handleSave = async () => {
     // validation...
     try {
       const updatedSettings = await appSettingsService.updateGlobalSettings('huggingface', settings);
       
       if (!updatedSettings) {
         throw new Error('Failed to update settings');
       }
       
       setSuccess('Hugging Face settings saved successfully!');
     } catch (error) {
       // error handling...
     }
   };
   ```

## Why This Works

1. **Appropriate Storage**: Now the settings are properly stored as app-wide settings in the `app_settings` table
2. **Consistent Interface**: We have a proper service interface for accessing global settings
3. **Code Organization**: Direct database access is encapsulated in the service layer
4. **Error Handling**: We have consistent error handling patterns throughout the application

## Testing

To verify the fix works:

1. Navigate to the Hugging Face settings page
2. Enter valid settings
3. Save the settings
4. Refresh the page - the settings should persist
5. Log in as a different user - they should see the same settings (unlike user-specific settings)

================
File: docs/huggingface-spaces-integration/SPACE_SETUP_GUIDE.md
================
# Hugging Face Spaces Setup Guide

This guide provides step-by-step instructions for setting up the three Hugging Face Spaces required for our triple LLM implementation.

## Prerequisites

- Hugging Face account with Pro subscription (for private Spaces)
- Python knowledge for Space implementation
- Access to our model training pipeline
- Hugging Face CLI installed locally (`pip install huggingface_hub`)

## Overview

We'll be creating three Spaces:

1. **Base Model Space**: General-purpose LLM endpoint for non-company-specific queries
2. **Company Model Space**: LLM fine-tuned with company-specific data
3. **Abstraction Model Space**: LLM trained on abstracted business patterns

Each Space will be configured with a Gradio interface that exposes an API endpoint for our application to call.

## Step 1: Base Model Space Setup

### 1.1 Create the Space

1. Log in to your Hugging Face account
2. Navigate to your profile and click on "New Space"
3. Fill in the following details:
   - Owner: Your organization
   - Space name: `company-base-llm` (or your preferred name)
   - License: Apache 2.0
   - SDK: Gradio
   - Space hardware: CPU (basic) or GPU (if needed for your model size)
   - Visibility: Private

### 1.2 Create the Space Files

Create a file named `app.py` with the following content:

```python
import gradio as gr
from transformers import AutoModelForCausalLM, AutoTokenizer

# Load models once at startup
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3-8B")
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3-8B", 
    device_map="auto",
    load_in_8bit=True
)

def generate_response(prompt, max_length=1024, temperature=0.7):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    # Generate response
    outputs = model.generate(
        inputs.input_ids,
        max_length=max_length,
        temperature=temperature,
        top_p=0.95,
        do_sample=True
    )
    
    # Decode and return response
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()

# API endpoint
def api_generate(prompt, params=None):
    if params is None:
        params = {}
    
    max_length = params.get("max_length", 1024)
    temperature = params.get("temperature", 0.7)
    
    response = generate_response(prompt, max_length, temperature)
    return {"generated_text": response}

# Create Gradio interface with both UI and API
with gr.Blocks() as demo:
    gr.Markdown("# Base LLM API")
    with gr.Row():
        with gr.Column():
            prompt = gr.Textbox(label="Prompt")
            max_length = gr.Slider(minimum=64, maximum=4096, value=1024, label="Max Length")
            temperature = gr.Slider(minimum=0.1, maximum=1.0, value=0.7, label="Temperature")
            submit_btn = gr.Button("Generate")
        with gr.Column():
            output = gr.Textbox(label="Response")
    
    submit_btn.click(
        generate_response,
        inputs=[prompt, max_length, temperature],
        outputs=output
    )

# Mount API endpoint
demo.queue()
demo.launch()
```

Create a `requirements.txt` file:

```
gradio>=3.50.2
torch>=2.0.0
transformers>=4.30.0
accelerate>=0.20.0
bitsandbytes>=0.39.0
```

### 1.3 Configure the Space

1. Create a file named `.gitattributes` with the following content:
   ```
   *.pt filter=lfs diff=lfs merge=lfs -text
   *.bin filter=lfs diff=lfs merge=lfs -text
   ```

2. Create a `README.md` file with a description of your Space:
   ```markdown
   # Base LLM API

   This Space provides an API endpoint for generating text using a base LLM.

   ## API Usage

   Send POST requests to `/api/generate` with the following JSON payload:

   ```json
   {
     "prompt": "Your prompt here",
     "params": {
       "max_length": 1024,
       "temperature": 0.7
     }
   }
   ```

   The response will be a JSON object with a `generated_text` field.
   ```

3. Set the following repository secrets in your Space settings:
   - `HF_TOKEN`: Your Hugging Face API token

## Step 2: Company Model Space Setup

### 2.1 Create the Space

Follow the same steps as for the Base Model Space, but name it `company-specific-llm`.

### 2.2 Create the Space Files

Create an `app.py` file with the following content:

```python
import gradio as gr
from transformers import AutoModelForCausalLM, AutoTokenizer
import json
import os

# Environment variable for versioning
MODEL_VERSION = os.environ.get("MODEL_VERSION", "v1")
MODEL_PATH = f"./models/company-specific-{MODEL_VERSION}"

# Load fine-tuned model and tokenizer
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    device_map="auto",
    load_in_8bit=True
)

def generate_response(prompt, max_length=1024, temperature=0.7, context=None):
    # Format prompt with company context if provided
    if context:
        try:
            context_data = json.loads(context)
            formatted_prompt = f"""Company Context:
            Company Name: {context_data.get('company_name', 'Unknown')}
            Industry: {context_data.get('industry', 'Unknown')}
            
            User Query: {prompt}
            """
        except:
            formatted_prompt = prompt
    else:
        formatted_prompt = prompt
    
    # Tokenize and generate
    inputs = tokenizer(formatted_prompt, return_tensors="pt").to(model.device)
    
    outputs = model.generate(
        inputs.input_ids,
        max_length=max_length,
        temperature=temperature,
        top_p=0.92,
        do_sample=True
    )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(formatted_prompt, "").strip()

# API endpoint
def api_generate(prompt, params=None):
    if params is None:
        params = {}
    
    max_length = params.get("max_length", 1024)
    temperature = params.get("temperature", 0.7)
    context = params.get("context", None)
    
    response = generate_response(prompt, max_length, temperature, context)
    return {"generated_text": response}

# Create Gradio interface with both UI and API
with gr.Blocks() as demo:
    gr.Markdown(f"# Company-Specific LLM API (Version: {MODEL_VERSION})")
    with gr.Row():
        with gr.Column():
            prompt = gr.Textbox(label="Prompt")
            context = gr.Textbox(label="Company Context (JSON)")
            max_length = gr.Slider(minimum=64, maximum=4096, value=1024, label="Max Length")
            temperature = gr.Slider(minimum=0.1, maximum=1.0, value=0.7, label="Temperature")
            submit_btn = gr.Button("Generate")
        with gr.Column():
            output = gr.Textbox(label="Response")
    
    submit_btn.click(
        generate_response,
        inputs=[prompt, max_length, temperature, context],
        outputs=output
    )

# Mount API endpoint
demo.queue()
demo.launch()
```

Create a similar `requirements.txt` file as for the Base Model Space.

### 2.3 Set Up the Model Directory

1. Create a `models` directory in your Space repository
2. Add a placeholder fine-tuned model that will be replaced by your actual fine-tuned model:
   ```
   models/
   └── company-specific-v1/
       ├── config.json
       ├── tokenizer.json
       ├── tokenizer_config.json
       └── pytorch_model.bin
   ```

## Step 3: Abstraction Model Space Setup

### 3.1 Create the Space

Follow the same steps as for the previous Spaces, but name it `company-abstraction-llm`.

### 3.2 Create the Space Files

Create an `app.py` file with the following content:

```python
import gradio as gr
from transformers import AutoModelForCausalLM, AutoTokenizer
import json
import os

# Environment variable for versioning
MODEL_VERSION = os.environ.get("MODEL_VERSION", "v1")
MODEL_PATH = f"./models/abstraction-{MODEL_VERSION}"

# Load fine-tuned model and tokenizer
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    device_map="auto",
    load_in_8bit=True
)

def generate_response(prompt, max_length=1024, temperature=0.7, abstraction_context=None):
    # Format prompt with abstraction context if provided
    if abstraction_context:
        try:
            context_data = json.loads(abstraction_context)
            formatted_prompt = f"""Abstraction Context:
            Industry: {context_data.get('industry', 'Unknown')}
            Business Pattern: {context_data.get('pattern', 'Unknown')}
            Pattern Strength: {context_data.get('strength', 'medium')}
            
            User Query: {prompt}
            """
        except:
            formatted_prompt = prompt
    else:
        formatted_prompt = prompt
    
    # Tokenize and generate
    inputs = tokenizer(formatted_prompt, return_tensors="pt").to(model.device)
    
    outputs = model.generate(
        inputs.input_ids,
        max_length=max_length,
        temperature=temperature,
        top_p=0.92,
        do_sample=True
    )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(formatted_prompt, "").strip()

# API endpoint
def api_generate(prompt, params=None):
    if params is None:
        params = {}
    
    max_length = params.get("max_length", 1024)
    temperature = params.get("temperature", 0.7)
    abstraction_context = params.get("abstraction_context", None)
    
    response = generate_response(prompt, max_length, temperature, abstraction_context)
    return {"generated_text": response}

# Create Gradio interface with both UI and API
with gr.Blocks() as demo:
    gr.Markdown(f"# Abstraction Model API (Version: {MODEL_VERSION})")
    with gr.Row():
        with gr.Column():
            prompt = gr.Textbox(label="Prompt")
            abstraction_context = gr.Textbox(label="Abstraction Context (JSON)")
            max_length = gr.Slider(minimum=64, maximum=4096, value=1024, label="Max Length")
            temperature = gr.Slider(minimum=0.1, maximum=1.0, value=0.7, label="Temperature")
            submit_btn = gr.Button("Generate")
        with gr.Column():
            output = gr.Textbox(label="Response")
    
    submit_btn.click(
        generate_response,
        inputs=[prompt, max_length, temperature, abstraction_context],
        outputs=output
    )

# Mount API endpoint
demo.queue()
demo.launch()
```

Set up the model directory in the same way as for the Company Model Space.

## Step 4: Model Deployment

### 4.1 Deploy Initial Models

For the initial setup, you can start with the base model (Llama-3-8B) for all three Spaces while you work on fine-tuning the specialized models. This ensures you have a working system from the beginning.

### 4.2 Fine-tuning Process

1. Use our model training pipeline to fine-tune the base model on:
   - Company-specific data for the Company Model
   - Abstracted business patterns for the Abstraction Model

2. Push the fine-tuned models to Hugging Face Model Hub:
   ```bash
   huggingface-cli login
   python -c "from huggingface_hub import push_to_hub; push_to_hub('path/to/your/model', 'your-org/model-name')"
   ```

3. Update the Space to use the fine-tuned model:
   - Edit the `app.py` file to point to your fine-tuned model
   - Or create a GitHub Actions workflow to automate this process

### 4.3 Automating Deployment

Create a GitHub Actions workflow to automate deployment. Create a file named `.github/workflows/deploy.yml` in your Space repository:

```yaml
name: Deploy model to Space

on:
  workflow_dispatch:
    inputs:
      model_id:
        description: 'Model ID to deploy'
        required: true
      model_version:
        description: 'Model version'
        required: true
        default: 'v1'

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install huggingface_hub
      
      - name: Download model
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          python -c "from huggingface_hub import snapshot_download; snapshot_download('${{ github.event.inputs.model_id }}', local_dir='models/${{ github.event.inputs.model_version }}')"
      
      - name: Update model version in app.py
        run: |
          echo "MODEL_VERSION=${{ github.event.inputs.model_version }}" > .env
      
      - name: Push changes
        uses: stefanzweifel/git-auto-commit-action@v4
        with:
          commit_message: Update model to ${{ github.event.inputs.model_id }} version ${{ github.event.inputs.model_version }}
```

## Step 5: Testing the Spaces

### 5.1 Manual Testing

1. Access each Space's UI in your browser
2. Test with different prompts and parameters
3. Verify the responses meet your expectations

### 5.2 API Testing

Test the API using curl or another API client:

```bash
curl -X POST \
  https://huggingface.co/spaces/your-org/company-base-llm/api/generate \
  -H 'Authorization: Bearer YOUR_HF_TOKEN' \
  -H 'Content-Type: application/json' \
  -d '{
    "prompt": "What are the key considerations for a startup in the tech industry?",
    "params": {
      "max_length": 1024,
      "temperature": 0.7
    }
  }'
```

### 5.3 Integration Testing

Update your application's environment variables to point to your Spaces and test the integration:

```
VITE_HF_BASE_SPACE=your-org/company-base-llm
VITE_HF_COMPANY_SPACE=your-org/company-specific-llm
VITE_HF_ABSTRACTION_SPACE=your-org/company-abstraction-llm
```

## Troubleshooting

### Common Issues

1. **Space startup failures**: Check the Space logs for errors
2. **Model loading issues**: Ensure your model files are correctly uploaded and accessible
3. **Out of memory errors**: Adjust your model's loading parameters (e.g., load_in_8bit=True)
4. **API authentication issues**: Verify your HF_TOKEN is correctly set and has the necessary permissions
5. **Slow response times**: Consider upgrading your Space's hardware or optimizing your model loading

### Debugging Tools

- Space logs: Available in the "Logs" tab of your Space
- Environment variables: Set `DEBUG=1` to enable additional logging
- Space metrics: Monitor CPU, GPU, and memory usage in the "Metrics" tab

## Next Steps

Once your Spaces are set up and working correctly, you can:

1. Set up monitoring alerts for Space outages or performance issues
2. Implement automated testing with GitHub Actions
3. Create a CI/CD pipeline for model deployment
4. Implement a more sophisticated model versioning system
5. Explore model distillation techniques to improve performance

================
File: docs/huggingface-spaces-integration/TECHNICAL_ARCHITECTURE.md
================
# Technical Architecture: Hugging Face Spaces Triple LLM

## High-Level Architecture

```mermaid
graph TD
    A[Frontend Client] --> B[LLM Service Facade]
    B --> C{Model Selector}
    C -->|Tier 1| D[HF Spaces Base Model]
    C -->|Tier 2| E[HF Spaces Company Model]
    C -->|Tier 3| F[HF Spaces Abstraction Model]
    D & E & F --> G[Response Handler]
    G --> B
    B --> A
    H[Logging & Feedback System] <--> B
    I[Training Pipeline] --> J[HF Model Hub]
    J --> K[Deploy to Spaces]
    K --> D & E & F
    L[Supabase Database] <--> H
    M[Company Data] --> I
    N[Abstracted Data] --> I
```

## Component Descriptions

### 1. LLM Service Facade

The LLM Service Facade serves as the primary entry point for all LLM-related operations. It coordinates requests between the client applications and the appropriate model tier based on the specific context and requirements of each request.

**Responsibilities:**
- Route requests to the appropriate model based on context
- Handle authentication and authorization
- Implement caching strategies
- Process and format responses
- Log requests and responses
- Implement fallback mechanisms for service disruptions

### 2. Model Selector

The Model Selector determines which tier of the LLM architecture should handle a specific request based on:

- Request context (company-specific, general knowledge, etc.)
- User permissions and settings
- Feature flags and system configuration
- Request priority and performance requirements

### 3. Hugging Face Spaces Endpoints

Three distinct Hugging Face Spaces will be created to handle the different tiers of the architecture:

1. **Base Model Space**: Serves as the foundation for general queries without specific company context
2. **Company Model Space**: Specialized for company-specific knowledge and queries
3. **Abstraction Model Space**: Handles pattern recognition and insights across similar businesses

Each Space exposes an API endpoint that can be called from our application.

### 4. Response Handler

Processes responses from the models, ensuring consistent formatting, error handling, and post-processing:

- Standardizes response formats
- Enriches responses with metadata
- Filters inappropriate content
- Handles errors and timeouts
- Implements retry logic when necessary

### 5. Training Pipeline

Responsible for preparing data, training models, and deploying them to Hugging Face Spaces:

- Data extraction and preprocessing
- Model selection and configuration
- Training and evaluation workflows
- Model versioning and deployment
- Performance monitoring and feedback loop

## Data Flow

### Request Flow

1. User interacts with the application, triggering an LLM request
2. Request is sent to the LLM Service Facade
3. Feature flags are checked to determine if Hugging Face Spaces should be used
4. If Hugging Face is enabled, the Model Selector determines which tier to use
5. Request is formatted and sent to the appropriate Hugging Face Space
6. Space processes the request and returns a response
7. Response is processed by the Response Handler
8. Processed response is returned to the user

### Feedback and Learning Flow

1. User interaction with LLM responses is logged
2. Feedback is stored in the database
3. Training pipeline extracts features from logs and feedback
4. New models are trained using company data and abstracted patterns
5. Trained models are pushed to Hugging Face Model Hub
6. Models are deployed to their respective Spaces
7. Future requests benefit from improved models

## Feature Flag System

The feature flag system allows for easy toggling between different LLM providers and model configurations:

```mermaid
graph TD
    A[Feature Flags] --> B{Provider Selection}
    B -->|OpenAI| C[OpenAI Client]
    B -->|Hugging Face| D[HF Client]
    D --> E{Model Tier Selection}
    E -->|Base| F[Base Model Space]
    E -->|Company| G[Company Model Space]
    E -->|Abstraction| H[Abstraction Model Space]
    C --> I[Existing OpenAI Implementation]
    F & G & H & I --> J[Response Processing]
```

## Database Schema Extensions

```sql
-- Add Hugging Face specific columns to model_registry
ALTER TABLE model_registry ADD COLUMN IF NOT EXISTS hf_space_url TEXT;
ALTER TABLE model_registry ADD COLUMN IF NOT EXISTS hf_model_id TEXT;
ALTER TABLE model_registry ADD COLUMN IF NOT EXISTS hf_space_id TEXT;

-- Add a new table for Space configurations
CREATE TABLE IF NOT EXISTS hf_spaces_config (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  space_name TEXT NOT NULL,
  space_type TEXT NOT NULL, -- 'base', 'company', 'abstraction'
  api_endpoint TEXT NOT NULL,
  is_active BOOLEAN DEFAULT TRUE,
  model_version TEXT,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT now(),
  updated_at TIMESTAMP WITH TIME ZONE DEFAULT now()
);

-- Table for tracking Space deployments
CREATE TABLE IF NOT EXISTS hf_deployments (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  space_id UUID REFERENCES hf_spaces_config(id),
  model_id UUID REFERENCES model_registry(id),
  deployment_status TEXT NOT NULL,
  deployment_logs TEXT,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT now(),
  updated_at TIMESTAMP WITH TIME ZONE DEFAULT now()
);
```

## Integration with Existing Systems

This implementation integrates with several existing systems:

1. **Feature Flags System**: Controls which LLM provider to use
2. **Logging Service**: Tracks LLM usage and performance metrics
3. **Model Training Service**: Facilitates training and deployment of new models
4. **Supabase Database**: Stores model metadata, configurations, and feedback

## Security Considerations

1. **API Key Management**: Hugging Face API keys must be securely stored in environment variables
2. **Request/Response Encryption**: All communication with Hugging Face Spaces is encrypted via HTTPS
3. **Access Control**: Spaces can be configured with private visibility to restrict access
4. **Data Privacy**: Ensure sensitive data is anonymized before being used for training
5. **Rate Limiting**: Implement rate limiting to prevent abuse and manage costs

================
File: docs/huggingface-spaces-integration/TROUBLESHOOTING_ENDPOINTS.md
================
# Hugging Face Spaces Integration Troubleshooting

## Common Issues with Hugging Face Spaces

When integrating with Hugging Face Spaces, several common issues can occur that prevent successful connection. This guide addresses the most frequent problems and their solutions.

## 1. 404 Error (Not Found) when connecting to Space API

This is the error you're currently facing with the message:
```
POST https://alie354-company-base-expert-model.hf.space/api/predict 404 (Not Found)
```

### Possible Causes and Solutions:

#### Wrong API Endpoint
Hugging Face Spaces can use different API endpoints depending on how the Space is set up:

| Framework | Common Endpoints |
|-----------|-----------------|
| Gradio (newer) | `/api/predict` |
| Gradio (older) | `/run/predict` |
| LLM-focused Spaces | `/api/generate` |
| FastAPI/Custom | `/predict`, `/run`, or custom path |

**Solution:** The fix script (`run-huggingface-fix.js`) automatically updates your settings to use `/api/predict`, which is the standard endpoint for most Gradio Spaces. If this doesn't work, you may need to check your Space's documentation or logs to find the correct endpoint.

#### Space is Paused or Not Running
Hugging Face automatically pauses inactive Spaces after a period of inactivity.

**Solution:** Visit your Space directly at `https://huggingface.co/spaces/yourusername/yourspace` and ensure it's running. If it shows "This Space is currently paused", click the "Restart Space" button and wait for it to start.

#### Space URL Format Issues
You may be using the wrong URL format. Hugging Face Spaces can be accessed using two URL formats:

1. Standard: `https://huggingface.co/spaces/username/space-name`
2. Direct: `https://username-space-name.hf.space`

**Solution:** Make sure your URL follows one of these formats. Our component automatically converts between these formats when needed.

## 2. 401 Error (Unauthorized) when connecting to Space API

This occurs when your Space is private and requires authentication.

### Solution:

1. Obtain a Hugging Face API token from [Hugging Face settings](https://huggingface.co/settings/tokens)
2. Add this token to the "Authentication Token" field in the Space configuration
3. Ensure the token has the necessary permissions to access the Space

## 3. Request Format Issues

Hugging Face Spaces may expect different input formats depending on the model and configuration:

1. Standard format:
```json
{
  "inputs": "Your query text",
  "parameters": {
    "max_new_tokens": 50,
    "temperature": 0.7
  }
}
```

2. Data array format:
```json
{
  "data": ["Your query text"]
}
```

**Solution:** Our fix script tries multiple input formats automatically.

## Testing Your Space Connection

You can test your Space connection directly using the following command:

```bash
curl -X POST \
  https://yourusername-yourspace.hf.space/api/predict \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -d '{"inputs": "Test query"}'
```

Replace `yourusername-yourspace` with your actual Space name and `YOUR_TOKEN` with your Hugging Face token if the Space is private.

## Running the Fix Script

To fix the API endpoint issue, you can run:

```bash
node scripts/run-huggingface-fix.js
```

This script will:
1. Update all your Space configurations to use the `/api/predict` endpoint
2. Display diagnostic information to help troubleshoot connection issues
3. Update the settings in your database

## Additional Resources

- [Hugging Face Spaces Documentation](https://huggingface.co/docs/hub/spaces)
- [Gradio API Reference](https://www.gradio.app/guides/sharing-your-app#api-page)
- [FastAPI in Spaces](https://huggingface.co/docs/hub/spaces-sdks-fastapi)

If you continue to have issues, check the Space's specific documentation or contact the Space owner for the correct API endpoint and input format.

================
File: docs/huggingface-spaces-integration/TROUBLESHOOTING.md
================
# Hugging Face Spaces Integration Troubleshooting Guide

## Common Issues

### 1. Connection Failing with 404 Not Found

If you're seeing a 404 Not Found error when trying to connect to your Hugging Face Space, it could be due to:

- Incorrect Space URL format
- Wrong API endpoint path
- Space is paused or offline

### 2. Authentication Issues (401 Unauthorized)

If you're seeing a 401 Unauthorized error, it means:

- Your Space is private and requires authentication
- The authentication token is invalid or missing
- You need to provide a valid token in the Settings UI

## Fix Script

We've created a fix script that updates the Hugging Face Spaces settings to use the correct URL format and API endpoint. This script:

1. Updates the Space URL to use the correct format (`https://huggingface.co/spaces/username/space-name`)
2. Changes the API endpoint to `/api/predict` which is the standard endpoint used by Gradio Spaces
3. Preserves any existing authentication tokens
4. Makes sure the feature flag is properly configured

To run the fix, execute:

```bash
node scripts/fix-huggingface-spaces-migration.js
```

Then open the Settings UI and:
1. Enter your authentication token for the Space
2. Enable the Hugging Face Spaces feature flag
3. Test the connection

## Technical Details

### Correct URL Format

Hugging Face Spaces URLs come in two formats:
- `https://huggingface.co/spaces/username/space-name` (standard format)
- `https://username-space-name.hf.space` (direct domain format)

Our client automatically converts the standard format to the direct domain format when making API calls.

### API Endpoints

Gradio Spaces typically expose their API at one of these endpoints:

- `/api/predict` - Standard Gradio API endpoint (newer versions)
- `/run/predict` - Alternative format for older Gradio
- `/api/generate` - Common for LLM Spaces
- `/run` - Direct endpoint (newer Gradio with FastAPI)
- `/predict` - Direct predict endpoint

The most common and reliable endpoint is `/api/predict` which we now use as the default.

### Request Format

The request should be formatted as:

```json
{
  "inputs": "Your prompt text",
  "parameters": {
    "max_new_tokens": 25,
    "temperature": 0.1
  }
}
```

Or for newer Gradio versions:

```json
{
  "data": ["Your prompt text"]
}
```

The diagnostic script tries both formats to determine which one works with your Space.

## Diagnostic Script

We've also created a diagnostic script that tests different configurations to find the one that works with your Space:

```bash
node scripts/test-huggingface-spaces-connection.js
```

This script:
1. Checks the current settings in the database
2. Tests the Space with the configured endpoint
3. Tries different endpoint formats if the configured one fails
4. Tests different request formats

Use the output from this script to troubleshoot your Space connection issues.

================
File: docs/huggingface-spaces-integration/USER_GUIDE.md
================
# Hugging Face Spaces Integration: User Guide

This guide explains how to set up and use Hugging Face Spaces with your application, allowing you to leverage custom deployed LLM models.

## Table of Contents

1. [Introduction](#introduction)
2. [Getting Started](#getting-started)
3. [Setting Up Your Hugging Face Space](#setting-up-your-hugging-face-space)
4. [Configuring the Application](#configuring-the-application)
5. [Multi-Tier Space Configuration](#multi-tier-space-configuration)
6. [Testing Your Configuration](#testing-your-configuration)
7. [Troubleshooting](#troubleshooting)
8. [Best Practices](#best-practices)

---

## Introduction

Hugging Face Spaces lets you deploy machine learning models in a serverless environment. This integration enables you to:

- Use your own custom-trained language models
- Maintain full control over your model and prompts
- Implement custom logic for specialized use cases
- Keep your data private within your own infrastructure
- Scale based on your needs with Hugging Face's platform

## Getting Started

### Prerequisites

- A Hugging Face account ([Sign up here](https://huggingface.co/join))
- At least one Hugging Face Space set up with an LLM
- Database migration for Hugging Face Spaces completed

### Running the Migration

Before you can use Hugging Face Spaces, you need to run the migration script that sets up the necessary database entries:

```bash
node scripts/run-huggingface-spaces-migration.js
```

This only needs to be done once. The script will create the required settings entries in your database.

---

## Setting Up Your Hugging Face Space

### Creating a Space

1. Log in to your [Hugging Face account](https://huggingface.co/login)
2. Go to your profile page
3. Click on "New" and select "Space"
4. Choose a suitable template (e.g., Gradio, Streamlit, or Docker)
5. Name your Space and set it to Public or Private as needed
6. Clone the repository and add your code

### Required API Format

Your Space needs to expose an API endpoint that follows this format:

#### Request

```json
{
  "inputs": "Your prompt text here",
  "parameters": {
    "max_new_tokens": 100,
    "temperature": 0.7
  }
}
```

#### Response

```json
{
  "generated_text": "The model's response text"
}
```

### Sample Space Implementation

Here's a minimal example of a Gradio Space that implements the required API:

```python
import gradio as gr
from transformers import pipeline

# Initialize the model
generator = pipeline('text-generation', model='gpt2')

def generate_text(inputs, max_new_tokens=100, temperature=0.7):
    generated = generator(inputs, max_new_tokens=max_new_tokens, temperature=temperature)
    return {"generated_text": generated[0]["generated_text"]}

# Create a Gradio API interface
gr.Interface.load("huggingface/gpt2",
                  inputs=["text", gr.Slider(0, 100), gr.Slider(0, 1)],
                  outputs="text").launch()

# Create the API endpoint
app = gr.mount_gradio_app(app, gr.routes.App(generate_text), path="/api/generate")
```

### Space URL Formats

Your Space will have a URL in one of these formats:
- `https://huggingface.co/spaces/username/space-name`
- `https://username-space-name.hf.space`

Both are compatible with this integration.

---

## Configuring the Application

### Enabling the Feature

1. Navigate to Settings > Features > AI Providers
2. Enable the "Hugging Face Spaces" toggle
3. Save changes

### Setting Up Space Configuration

1. Go to Settings > Integrations > Hugging Face Spaces Settings
2. Toggle "Enable Hugging Face Spaces" to turn on the integration
3. Choose a default tier (usually "Base Model")
4. Configure at least one Space:
   - Enter the Space URL (e.g., `https://username-space-name.hf.space`)
   - Set the API endpoint (default: `/api/generate`)
   - Optionally add an authentication token for private Spaces
   - Optionally specify a model version/revision for tracking purposes
5. Click "Test Connection" to verify your Space is working
6. Save settings

### Authentication for Private Spaces

If your Space is private, you'll need to provide an authentication token:

1. Go to your [Hugging Face settings page](https://huggingface.co/settings/tokens)
2. Create a new access token with at least "read" permissions
3. Copy the token and paste it in the "Authentication Token" field for your Space configuration

---

## Multi-Tier Space Configuration

The application supports four tiers of Spaces:

1. **Base Model**: General-purpose model used as the default
2. **Company Model**: Model fine-tuned for company-specific use cases
3. **Abstraction Model**: Specialized model for abstract reasoning tasks
4. **User Model**: Personalized model for individual users

You can configure any or all of these tiers with different Spaces, allowing for specialized models optimized for different tasks.

### When to Use Multiple Tiers

- **Base Model**: Use for general queries and fallback
- **Company Model**: Use for company-specific knowledge and terminology
- **Abstraction Model**: Use for complex reasoning, planning, and strategic thinking
- **User Model**: Use for personalized responses tailored to individual users

### Switching Between Tiers

The application will automatically use the default tier specified in settings. However, you can also switch tiers programmatically using the Hugging Face Spaces client:

```typescript
import huggingFaceSpacesClient from '../lib/huggingface-spaces-client';

// Use the base model (default)
const response = await huggingFaceSpacesClient.generate("Your prompt", "base");

// Use the company model for company-specific knowledge
const companyResponse = await huggingFaceSpacesClient.generate("Your prompt", "company");
```

---

## Testing Your Configuration

### Manual Testing

After setting up your Space configuration:

1. Go to Settings > Integrations > Hugging Face Spaces Settings
2. Click "Test Connection" for each configured Space
3. Check for a successful connection message

### Using the Test Script

We provide a diagnostic script that tests your entire Hugging Face Spaces configuration:

```bash
node scripts/test-huggingface-spaces-connection.js
```

This script:
- Verifies database settings
- Checks feature flag status
- Tests connection to each configured Space
- Provides detailed feedback

### Testing in the Application

To test if the integration is working in your application:

1. Enable the Hugging Face Spaces feature flag
2. Navigate to a feature that uses AI (like the Idea Playground)
3. Try generating content - it should now use your configured Space

---

## Troubleshooting

If you encounter issues with your Hugging Face Spaces integration, check the following:

1. Verify your Space is online (free Spaces pause after inactivity)
2. Ensure the Space URL is correct and the Space is publicly accessible
3. Check that your Space API follows the expected request/response format
4. Test the connection directly using the test script
5. Look for error messages in the browser console or server logs

For more detailed troubleshooting, see the [Troubleshooting Guide](./TROUBLESHOOTING.md).

---

## Best Practices

### Performance Optimization

1. Use the smallest model that meets your needs to reduce latency
2. Consider a paid Hugging Face plan for better performance and reliability
3. Implement client-side caching for frequently requested responses
4. Keep prompts concise and specific

### Space Implementation Tips

1. Use a model optimized for your specific use case
2. Set reasonable token limits to prevent excessive resource usage
3. Log requests for monitoring and debugging
4. Implement error handling in your Space
5. Consider implementing fallbacks for model failures

### Security Considerations

1. Use private Spaces for sensitive applications
2. Implement authentication for your Space API
3. Regularly rotate authentication tokens
4. Validate and sanitize inputs to prevent prompt injection
5. Consider CORS settings if your Space will be accessed from specific domains

### Model Deployment Strategies

1. Use staging Spaces for testing before production
2. Implement versioning in your Space URLs (e.g., `/v1/generate`)
3. Consider A/B testing between different models
4. Monitor usage and performance to determine when to scale

---

## Advanced Usage

### Structured Generation

For generating structured data like JSON:

```typescript
// Define the structure you want
const schema = {
  title: "string",
  description: "string",
  pros: "string[]",
  cons: "string[]"
};

// Generate structured output
const result = await huggingFaceSpacesClient.generateStructure(
  "Create a product idea for a smart garden device", 
  schema
);

// result will be a typed object matching the schema
console.log(result.title);
console.log(result.pros);
```

### Generating Multiple Variations

To generate multiple different responses to the same prompt:

```typescript
const variations = await huggingFaceSpacesClient.generateVariations(
  "Create a product slogan for an eco-friendly water bottle",
  3,  // Number of variations to generate
  "company"  // Use the company model
);

// variations will be an array of strings
variations.forEach((slogan, index) => {
  console.log(`Slogan ${index + 1}: ${slogan}`);
});
```

### Adding Context

To provide additional context for generation:

```typescript
const context = {
  industry: "healthcare",
  targetAudience: "medical professionals",
  key_features: ["portable", "sterilizable", "long battery life"]
};

const response = await huggingFaceSpacesClient.generate(
  "Create a product description", 
  "base",
  context
);
```

## Further Resources

- [Hugging Face Spaces Documentation](https://huggingface.co/docs/hub/spaces-overview)
- [API Endpoints Guide](https://huggingface.co/docs/api-inference/quicktour)
- [Transformers Documentation](https://huggingface.co/docs/transformers/index)
- [Gradio Documentation](https://www.gradio.app/docs)

================
File: docs/idea-playground/components/AI_SERVICE_LAYER.md
================
# AI Service Layer Architecture

The AI Service Layer is a critical component of the Idea Playground rebuild, providing robust integration with large language models for idea generation, variation creation, and idea refinement. This document details the architecture and implementation of this layer.

## Architecture Overview

The AI Service Layer follows a multi-tiered architecture designed for reliability, flexibility, and performance:

```mermaid
graph TD
    Client[Domain Services] --> AIService[AI Service]
    
    subgraph "AI Service Layer"
        AIService --> Orchestrator[AI Orchestrator]
        
        Orchestrator --> PromptManager[Prompt Manager]
        Orchestrator --> ResponseProcessor[Response Processor]
        Orchestrator --> ErrorHandler[Error Handler]
        Orchestrator --> ProviderManager[Provider Manager]
        
        PromptManager --> TemplateRepository[(Template Repository)]
        PromptManager --> ContextEnricher[Context Enricher]
        PromptManager --> TokenOptimizer[Token Optimizer]
        
        ResponseProcessor --> SchemaValidator[Schema Validator]
        ResponseProcessor --> ResponseParser[Response Parser]
        ResponseProcessor --> ResponseTransformer[Response Transformer]
        
        ErrorHandler --> RetryMechanism[Retry Mechanism]
        ErrorHandler --> FallbackSystem[Fallback System]
        ErrorHandler --> ErrorReporter[Error Reporter]
        
        ProviderManager --> OpenAIProvider[OpenAI Provider]
        ProviderManager -.-> AlternateProvider[Alternate Provider]
    end
    
    OpenAIProvider --> OpenAI[OpenAI API]
    AlternateProvider -.-> OtherLLM[Other LLM API]
```

## Key Components

### 1. AI Service Interface

The AI Service provides a clean, domain-focused interface for the rest of the application:

```typescript
interface AIService {
  // Core methods for idea operations
  generateIdea(params: IdeaGenerationParams): Promise<IdeaGenerationResult>;
  generateVariations(params: VariationParams): Promise<VariationResult>;
  mergeIdeas(params: MergeParams): Promise<MergeResult>;
  refineIdea(params: RefinementParams): Promise<RefinementResult>;
  
  // Streaming variants for real-time updates
  generateIdeaStream(params: IdeaGenerationParams): Observable<IdeaGenerationPartialResult>;
  generateVariationsStream(params: VariationParams): Observable<VariationPartialResult>;
  
  // Utility methods
  getCostEstimate(operation: AIOperation): CostEstimate;
  getModelCapabilities(): ModelCapabilities;
}
```

### 2. AI Orchestrator

The Orchestrator is the core component that coordinates all AI operations:

```typescript
class AIOrchestrator {
  constructor(
    private readonly promptManager: PromptManager,
    private readonly responseProcessor: ResponseProcessor,
    private readonly errorHandler: ErrorHandler,
    private readonly providerManager: ProviderManager,
    private readonly logger: Logger
  ) {}
  
  async execute<T>(request: AIRequest, validator: SchemaValidator<T>): Promise<T> {
    try {
      // 1. Prepare the prompt
      const preparedPrompt = this.promptManager.preparePrompt(request);
      
      // 2. Select appropriate provider
      const provider = this.providerManager.selectProvider(request.requirements);
      
      // 3. Execute request
      const rawResponse = await provider.execute(preparedPrompt);
      
      // 4. Process and validate response
      const processedResponse = await this.responseProcessor.process(
        rawResponse, 
        validator
      );
      
      return processedResponse;
    } catch (error) {
      // Handle errors with proper recovery
      return this.errorHandler.handleError(error, request);
    }
  }
  
  // Streaming variant
  executeStream<T>(request: AIRequest, validator: SchemaValidator<T>): Observable<Partial<T>> {
    // Similar implementation but with streaming
  }
}
```

### 3. Prompt Management

The Prompt Manager handles all aspects of preparing effective prompts:

#### Template Repository

```typescript
interface PromptTemplate {
  id: string;
  version: string;
  template: string;
  parameters: string[];
  description: string;
  expectedResponseSchema: Record<string, unknown>;
}

class TemplateRepository {
  private templates: Map<string, PromptTemplate> = new Map();
  
  getTemplate(id: string, version?: string): PromptTemplate {
    // Implementation to get the appropriate template
  }
  
  registerTemplate(template: PromptTemplate): void {
    // Implementation to register a new template
  }
}
```

#### Context Enricher

```typescript
interface ContextProvider {
  getContext(params: Record<string, unknown>): Promise<Record<string, unknown>>;
}

class ContextEnricher {
  constructor(private readonly providers: Map<string, ContextProvider>) {}
  
  async enrichContext(
    baseContext: Record<string, unknown>,
    params: Record<string, unknown>,
    providerIds: string[]
  ): Promise<Record<string, unknown>> {
    // Implementation to enrich context from multiple providers
  }
}
```

#### Prompt Manager

```typescript
class PromptManager {
  constructor(
    private readonly templateRepository: TemplateRepository,
    private readonly contextEnricher: ContextEnricher,
    private readonly tokenOptimizer: TokenOptimizer
  ) {}
  
  async preparePrompt(request: AIRequest): Promise<PreparedPrompt> {
    // 1. Get the appropriate template
    const template = this.templateRepository.getTemplate(
      request.templateId,
      request.templateVersion
    );
    
    // 2. Enrich context with providers
    const enrichedContext = await this.contextEnricher.enrichContext(
      request.baseContext || {},
      request.parameters || {},
      request.contextProviders || []
    );
    
    // 3. Fill template with context
    let filledTemplate = template.template;
    for (const key of template.parameters) {
      const value = enrichedContext[key];
      if (value) {
        filledTemplate = filledTemplate.replace(`{{${key}}}`, String(value));
      }
    }
    
    // 4. Optimize for token usage
    const optimizedPrompt = this.tokenOptimizer.optimize(
      filledTemplate,
      request.constraints
    );
    
    return {
      content: optimizedPrompt,
      expectedSchema: template.expectedResponseSchema,
      parameters: enrichedContext
    };
  }
}
```

### 4. Response Processing

The Response Processor handles validation and parsing of AI responses:

#### Schema Validator

```typescript
interface SchemaValidator<T> {
  validate(data: unknown): Result<T, ValidationError>;
}

class ZodSchemaValidator<T> implements SchemaValidator<T> {
  constructor(private readonly schema: z.ZodSchema<T>) {}
  
  validate(data: unknown): Result<T, ValidationError> {
    try {
      const validated = this.schema.parse(data);
      return Result.ok(validated);
    } catch (error) {
      return Result.fail(new ValidationError(error.message, error));
    }
  }
}
```

#### Response Parser

```typescript
class ResponseParser {
  parse<T>(response: string, validator: SchemaValidator<T>): Result<T, ParsingError> {
    try {
      // First attempt: Direct parsing
      const parsed = JSON.parse(response);
      return validator.validate(parsed);
    } catch (jsonError) {
      try {
        // Second attempt: Repair malformed JSON
        const repaired = this.repairJson(response);
        const parsed = JSON.parse(repaired);
        return validator.validate(parsed);
      } catch (repairError) {
        try {
          // Third attempt: Extract structured data
          const extracted = this.extractStructured(response);
          return validator.validate(extracted);
        } catch (extractError) {
          return Result.fail(
            new ParsingError('Failed to parse response', {
              jsonError,
              repairError,
              extractError,
              response
            })
          );
        }
      }
    }
  }
  
  private repairJson(malformedJson: string): string {
    // Implementation of JSON repair techniques
  }
  
  private extractStructured(text: string): unknown {
    // Implementation to extract structured data using regex and heuristics
  }
}
```

### 5. Error Handling

The Error Handler provides robust error recovery:

```typescript
class ErrorHandler {
  constructor(
    private readonly retryMechanism: RetryMechanism,
    private readonly fallbackSystem: FallbackSystem,
    private readonly errorReporter: ErrorReporter
  ) {}
  
  async handleError<T>(error: Error, request: AIRequest): Promise<T> {
    // Log the error
    this.errorReporter.reportError(error, request);
    
    // Determine if retryable
    if (this.retryMechanism.isRetryable(error)) {
      return await this.retryMechanism.retry<T>(
        () => this.executeRequest<T>(request),
        error
      );
    }
    
    // Try fallback
    return await this.fallbackSystem.executeFallback<T>(request, error);
  }
  
  private async executeRequest<T>(request: AIRequest): Promise<T> {
    // Implementation to re-execute a request
  }
}
```

#### Retry Mechanism

```typescript
class RetryMechanism {
  constructor(
    private readonly maxRetries: number = 3,
    private readonly initialDelay: number = 1000,
    private readonly backoffFactor: number = 2
  ) {}
  
  isRetryable(error: Error): boolean {
    // Determine if an error is retryable
  }
  
  async retry<T>(operation: () => Promise<T>, originalError: Error): Promise<T> {
    let lastError = originalError;
    let delay = this.initialDelay;
    
    for (let attempt = 1; attempt <= this.maxRetries; attempt++) {
      try {
        // Wait exponentially longer between each retry
        await new Promise(resolve => setTimeout(resolve, delay));
        delay *= this.backoffFactor;
        
        // Attempt the operation again
        return await operation();
      } catch (error) {
        lastError = error;
        
        // If error is not retryable, stop retrying
        if (!this.isRetryable(error)) {
          break;
        }
      }
    }
    
    throw lastError;
  }
}
```

### 6. Provider Management

The Provider Manager handles selection and configuration of AI providers:

```typescript
interface LLMProvider {
  execute(prompt: PreparedPrompt): Promise<string>;
  executeStream(prompt: PreparedPrompt): Observable<string>;
  getCapabilities(): ProviderCapabilities;
}

class ProviderManager {
  constructor(
    private readonly providers: Map<string, LLMProvider>,
    private readonly defaultProvider: string
  ) {}
  
  selectProvider(requirements?: ProviderRequirements): LLMProvider {
    if (!requirements) {
      return this.providers.get(this.defaultProvider)!;
    }
    
    // Find the most suitable provider based on requirements
    const candidates = Array.from(this.providers.values()).filter(
      provider => this.meetsRequirements(provider, requirements)
    );
    
    if (candidates.length === 0) {
      return this.providers.get(this.defaultProvider)!;
    }
    
    // Select the best candidate based on requirements
    return candidates[0];
  }
  
  private meetsRequirements(
    provider: LLMProvider,
    requirements: ProviderRequirements
  ): boolean {
    // Check if provider meets the specified requirements
  }
}
```

## Solving JSON Parsing Issues

JSON parsing issues have been a major challenge in the existing system. Here's how we solve them:

### 1. Multi-Stage Parsing

Our approach uses a multi-stage parsing strategy:

1. **Direct Parsing**: Try standard JSON.parse
2. **JSON Repair**: If direct parsing fails, apply repair techniques
3. **Structured Extraction**: If repair fails, use regex and heuristics

### 2. Common Repair Techniques

```typescript
function repairJson(malformedJson: string): string {
  let repaired = malformedJson;
  
  // Fix unescaped quotes in JSON strings
  repaired = repaired.replace(/([^\\])"([^"]*?)([^\\])"/g, '$1\\"$2$3\\"');
  
  // Fix trailing commas in arrays and objects
  repaired = repaired.replace(/,\s*([\]}])/g, '$1');
  
  // Fix missing commas between objects in arrays
  repaired = repaired.replace(/}\s*{/g, '},{');
  
  // Balance brackets and braces
  repaired = balanceBrackets(repaired);
  
  return repaired;
}
```

### 3. Schema Validation

Every response is validated against a predefined schema:

```typescript
// Example schema for idea generation
const ideaSchema = z.object({
  title: z.string(),
  description: z.string(),
  problemStatement: z.string(),
  targetAudience: z.string(),
  uniqueValue: z.string(),
  strengths: z.array(z.string()),
  weaknesses: z.array(z.string()),
  opportunities: z.array(z.string()),
  threats: z.array(z.string())
});

type Idea = z.infer<typeof ideaSchema>;
```

### 4. Function Calling

We use OpenAI's function calling to enforce structured output:

```typescript
async function generateIdeaWithFunctionCalling(prompt: string): Promise<Idea> {
  const response = await openai.chat.completions.create({
    model: "gpt-4",
    messages: [{ role: "user", content: prompt }],
    functions: [
      {
        name: "generateBusinessIdea",
        description: "Generate a business idea",
        parameters: {
          type: "object",
          properties: {
            title: {
              type: "string",
              description: "The title of the business idea"
            },
            description: {
              type: "string",
              description: "Detailed description of the business idea"
            },
            // ... other fields
          },
          required: ["title", "description", "problemStatement", "targetAudience", "uniqueValue"]
        }
      }
    ],
    function_call: { name: "generateBusinessIdea" }
  });
  
  const functionCall = response.choices[0].message.function_call;
  if (!functionCall) {
    throw new Error("No function call in response");
  }
  
  try {
    const parsedArgs = JSON.parse(functionCall.arguments);
    return ideaSchema.parse(parsedArgs);
  } catch (error) {
    // Fall back to our robust parsing
    return responseParser.parse(functionCall.arguments, new ZodSchemaValidator(ideaSchema)).unwrap();
  }
}
```

## Integration with Domain Services

The AI Service is integrated with domain services through a clean dependency injection pattern:

```typescript
class IdeaGenerationService {
  constructor(private readonly aiService: AIService) {}
  
  async generateIdea(params: IdeaGenerationServiceParams): Promise<Idea> {
    // Transform service params to AI service params
    const aiParams: IdeaGenerationParams = {
      industry: params.industry,
      constraints: params.constraints,
      templateId: 'idea-generation',
      contextProviders: ['user-profile', 'company-data']
    };
    
    // Use AI service
    const result = await this.aiService.generateIdea(aiParams);
    
    // Transform result if needed and return
    return result.idea;
  }
}
```

## Streaming Responses for Real-time Feedback

The AI Service supports streaming responses to provide real-time feedback to users:

```typescript
class IdeaPlaygroundComponent {
  constructor(private readonly ideaService: IdeaGenerationService) {}
  
  generateIdeaWithFeedback() {
    const params = this.getParams();
    
    // Start with empty state
    this.setState({ isGenerating: true, partialIdea: null });
    
    // Use streaming API
    const subscription = this.ideaService.generateIdeaStream(params).subscribe({
      next: (partialResult) => {
        // Update UI with partial results as they arrive
        this.setState({
          partialIdea: {
            ...this.state.partialIdea,
            ...partialResult
          }
        });
      },
      error: (error) => {
        this.setState({ isGenerating: false, error });
      },
      complete: () => {
        this.setState({ isGenerating: false, isComplete: true });
      }
    });
    
    // Store subscription for cleanup
    this.activeSubscription = subscription;
  }
  
  componentWillUnmount() {
    // Clean up subscription if component is unmounted
    if (this.activeSubscription) {
      this.activeSubscription.unsubscribe();
    }
  }
}
```

## Performance Considerations

The AI Service incorporates several performance optimizations:

1. **Prompt Caching**: Similar prompts are cached to reduce API calls
2. **Response Caching**: Responses are cached with appropriate invalidation
3. **Token Optimization**: Prompts are optimized to minimize token usage
4. **Batched Requests**: Multiple operations are batched when possible
5. **Concurrent Operations**: Independent operations are processed concurrently

## Monitoring and Observability

The AI Service includes comprehensive monitoring:

1. **Operation Metrics**: Track success rates, latency, and token usage
2. **Error Tracking**: Categorize and track error types and frequency
3. **Cost Monitoring**: Track API costs by operation type
4. **Quality Assessment**: Monitor response quality metrics

## Implementation Strategy

The AI Service will be implemented in phases:

1. **Phase 1**: Core functionality with basic error handling
2. **Phase 2**: Enhanced parsing and validation
3. **Phase 3**: Streaming responses and progress indicators
4. **Phase 4**: Advanced error recovery and fallbacks
5. **Phase 5**: Performance optimization and monitoring

## Conclusion

The AI Service Layer is designed to provide robust, reliable AI capabilities to the Idea Playground. By addressing the key challenges of the current implementation—particularly JSON parsing issues and error handling—it will significantly improve the user experience and system reliability.

================
File: docs/idea-playground/components/DOMAIN_SERVICES.md
================
# Domain Services Architecture

This document describes the domain services layer of the Idea Playground, which contains the core business logic and domain rules of the application.

## Domain-Driven Design Approach

The Idea Playground adopts a domain-driven design (DDD) approach to ensure that the codebase directly reflects the business domain. This architecture provides several benefits:

1. **Business Alignment**: The code structure mirrors the business concepts
2. **Maintainability**: Business rules are centralized and explicitly modeled
3. **Flexibility**: The domain layer is isolated from infrastructure concerns
4. **Testability**: Business logic can be tested independently

## Domain Model

```mermaid
classDiagram
    class Canvas {
        +id: UUID
        +name: string
        +description: string
        +ownerId: UUID
        +tags: string[]
        +createIdea(details: IdeaDetails): Idea
        +getIdeas(): Idea[]
        +addCollaborator(userId: UUID, permission: Permission): void
        +removeCollaborator(userId: UUID): void
    }
    
    class Idea {
        +id: UUID
        +canvasId: UUID
        +title: string
        +description: string
        +problemStatement: string
        +targetAudience: string
        +uniqueValue: string
        +components: Component[]
        +generateVariations(count: number): Variation[]
        +addComponent(component: Component): void
        +removeComponent(componentId: UUID): void
        +update(details: IdeaDetails): void
    }
    
    class Variation {
        +id: UUID
        +parentIdeaId: UUID
        +title: string
        +description: string
        +problemStatement: string
        +targetAudience: string
        +uniqueValue: string
        +swot: SWOTAnalysis
        +isSelected: boolean
        +select(): void
        +deselect(): void
        +update(details: VariationDetails): void
    }
    
    class SWOTAnalysis {
        +strengths: string[]
        +weaknesses: string[]
        +opportunities: string[]
        +threats: string[]
        +addStrength(strength: string): void
        +addWeakness(weakness: string): void
        +addOpportunity(opportunity: string): void
        +addThreat(threat: string): void
    }
    
    class MergedIdea {
        +id: UUID
        +canvasId: UUID
        +title: string
        +description: string
        +problemStatement: string
        +targetAudience: string
        +uniqueValue: string
        +swot: SWOTAnalysis
        +sourceVariations: UUID[]
        +isSelected: boolean
        +select(): void
        +deselect(): void
        +update(details: MergedIdeaDetails): void
        +convertToIdea(): Idea
    }
    
    class Component {
        +id: UUID
        +ideaId: UUID
        +type: ComponentType
        +name: string
        +content: any
        +update(content: any): void
    }
    
    class Collaborator {
        +canvasId: UUID
        +userId: UUID
        +permission: Permission
        +updatePermission(permission: Permission): void
    }
    
    class Comment {
        +id: UUID
        +ideaId: UUID
        +userId: UUID
        +content: string
        +createdAt: Date
        +update(content: string): void
    }
    
    Canvas "1" -- "0..*" Idea
    Canvas "1" -- "0..*" Collaborator
    Idea "1" -- "0..*" Variation
    Idea "1" -- "0..*" Component
    Idea "1" -- "0..*" Comment
    Variation "0..*" -- "0..*" MergedIdea
    Variation "1" -- "1" SWOTAnalysis
    MergedIdea "1" -- "1" SWOTAnalysis
```

## Aggregate Roots

The domain model has the following aggregate roots:

### 1. Canvas Aggregate

The Canvas is the primary container for organizing ideas. It manages:
- Canvas metadata (name, description, tags)
- Canvas ownership and collaboration
- Access control for canvas contents

```typescript
class Canvas extends AggregateRoot {
  private _id: UUID;
  private _name: string;
  private _description: string;
  private _ownerId: UUID;
  private _tags: string[];
  private _collaborators: Map<UUID, Permission>;
  
  // Constructor sets initial state
  constructor(params: CanvasCreationParams, ownerId: UUID) {
    super();
    this._id = params.id || UUID.generate();
    this._name = params.name;
    this._description = params.description || '';
    this._ownerId = ownerId;
    this._tags = params.tags || [];
    this._collaborators = new Map();
    
    // Invariant checks
    this.validateCanvasName(this._name);
    
    // Register creation event
    this.registerDomainEvent(new CanvasCreatedEvent(this));
  }
  
  // Getters for immutable access to state
  get id(): UUID { return this._id; }
  get name(): string { return this._name; }
  get description(): string { return this._description; }
  get ownerId(): UUID { return this._ownerId; }
  get tags(): string[] { return [...this._tags]; }
  
  // Commands to modify state
  public rename(name: string): void {
    this.validateCanvasName(name);
    this._name = name;
    this.registerDomainEvent(new CanvasRenamedEvent(this));
  }
  
  public updateDescription(description: string): void {
    this._description = description;
    this.registerDomainEvent(new CanvasUpdatedEvent(this));
  }
  
  public addTag(tag: string): void {
    if (!this._tags.includes(tag)) {
      this._tags.push(tag);
      this.registerDomainEvent(new CanvasTagsUpdatedEvent(this));
    }
  }
  
  public removeTag(tag: string): void {
    this._tags = this._tags.filter(t => t !== tag);
    this.registerDomainEvent(new CanvasTagsUpdatedEvent(this));
  }
  
  public addCollaborator(userId: UUID, permission: Permission): void {
    if (userId === this._ownerId) {
      throw new DomainError('Cannot add owner as collaborator');
    }
    
    this._collaborators.set(userId, permission);
    this.registerDomainEvent(new CollaboratorAddedEvent(this, userId, permission));
  }
  
  public removeCollaborator(userId: UUID): void {
    if (!this._collaborators.has(userId)) {
      throw new DomainError('Collaborator not found');
    }
    
    this._collaborators.delete(userId);
    this.registerDomainEvent(new CollaboratorRemovedEvent(this, userId));
  }
  
  public canUserAccess(userId: UUID): boolean {
    return userId === this._ownerId || this._collaborators.has(userId);
  }
  
  public canUserEdit(userId: UUID): boolean {
    return userId === this._ownerId || 
           (this._collaborators.has(userId) && 
            this._collaborators.get(userId) === Permission.Edit);
  }
  
  // Invariant validations
  private validateCanvasName(name: string): void {
    if (!name || name.trim().length === 0) {
      throw new DomainError('Canvas name cannot be empty');
    }
    
    if (name.length > 100) {
      throw new DomainError('Canvas name cannot exceed 100 characters');
    }
  }
}
```

### 2. Idea Aggregate

The Idea represents a business concept with all its details. It manages:
- Idea metadata (title, description, etc.)
- Problem statement and target audience
- Idea components and variations

```typescript
class Idea extends AggregateRoot {
  private _id: UUID;
  private _canvasId: UUID;
  private _title: string;
  private _description: string;
  private _problemStatement: string;
  private _targetAudience: string;
  private _uniqueValue: string;
  private _components: Component[];
  
  constructor(params: IdeaCreationParams, canvasId: UUID) {
    super();
    this._id = params.id || UUID.generate();
    this._canvasId = canvasId;
    this._title = params.title;
    this._description = params.description;
    this._problemStatement = params.problemStatement;
    this._targetAudience = params.targetAudience;
    this._uniqueValue = params.uniqueValue;
    this._components = [];
    
    // Validate invariants
    this.validateTitle(this._title);
    
    // Register domain event
    this.registerDomainEvent(new IdeaCreatedEvent(this));
  }
  
  // Getters
  get id(): UUID { return this._id; }
  get canvasId(): UUID { return this._canvasId; }
  get title(): string { return this._title; }
  get description(): string { return this._description; }
  get problemStatement(): string { return this._problemStatement; }
  get targetAudience(): string { return this._targetAudience; }
  get uniqueValue(): string { return this._uniqueValue; }
  get components(): Component[] { return [...this._components]; }
  
  // Commands
  public update(details: IdeaUpdateParams): void {
    if (details.title) {
      this.validateTitle(details.title);
      this._title = details.title;
    }
    
    if (details.description) {
      this._description = details.description;
    }
    
    if (details.problemStatement) {
      this._problemStatement = details.problemStatement;
    }
    
    if (details.targetAudience) {
      this._targetAudience = details.targetAudience;
    }
    
    if (details.uniqueValue) {
      this._uniqueValue = details.uniqueValue;
    }
    
    this.registerDomainEvent(new IdeaUpdatedEvent(this));
  }
  
  public addComponent(componentParams: ComponentParams): Component {
    const component = new Component(
      componentParams,
      this._id
    );
    
    this._components.push(component);
    this.registerDomainEvent(new ComponentAddedEvent(this, component));
    
    return component;
  }
  
  public removeComponent(componentId: UUID): void {
    const index = this._components.findIndex(c => c.id === componentId);
    
    if (index === -1) {
      throw new DomainError('Component not found');
    }
    
    const component = this._components[index];
    this._components.splice(index, 1);
    
    this.registerDomainEvent(new ComponentRemovedEvent(this, component));
  }
  
  // Invariant validation
  private validateTitle(title: string): void {
    if (!title || title.trim().length === 0) {
      throw new DomainError('Idea title cannot be empty');
    }
    
    if (title.length > 100) {
      throw new DomainError('Idea title cannot exceed 100 characters');
    }
  }
}
```

### 3. Variation Aggregate

The Variation represents a derivative of an original idea. It manages:
- Variation details (title, description, etc.)
- SWOT analysis
- Selection state for merging

```typescript
class Variation extends AggregateRoot {
  private _id: UUID;
  private _parentIdeaId: UUID;
  private _title: string;
  private _description: string;
  private _problemStatement: string;
  private _targetAudience: string;
  private _uniqueValue: string;
  private _swot: SWOTAnalysis;
  private _isSelected: boolean;
  
  constructor(params: VariationCreationParams, parentIdeaId: UUID) {
    super();
    this._id = params.id || UUID.generate();
    this._parentIdeaId = parentIdeaId;
    this._title = params.title;
    this._description = params.description;
    this._problemStatement = params.problemStatement;
    this._targetAudience = params.targetAudience;
    this._uniqueValue = params.uniqueValue;
    this._swot = new SWOTAnalysis(params.swot);
    this._isSelected = false;
    
    // Register domain event
    this.registerDomainEvent(new VariationCreatedEvent(this));
  }
  
  // Getters
  get id(): UUID { return this._id; }
  get parentIdeaId(): UUID { return this._parentIdeaId; }
  get title(): string { return this._title; }
  get description(): string { return this._description; }
  get problemStatement(): string { return this._problemStatement; }
  get targetAudience(): string { return this._targetAudience; }
  get uniqueValue(): string { return this._uniqueValue; }
  get swot(): SWOTAnalysis { return this._swot; }
  get isSelected(): boolean { return this._isSelected; }
  
  // Commands
  public update(details: VariationUpdateParams): void {
    if (details.title) {
      this._title = details.title;
    }
    
    if (details.description) {
      this._description = details.description;
    }
    
    if (details.problemStatement) {
      this._problemStatement = details.problemStatement;
    }
    
    if (details.targetAudience) {
      this._targetAudience = details.targetAudience;
    }
    
    if (details.uniqueValue) {
      this._uniqueValue = details.uniqueValue;
    }
    
    if (details.swot) {
      this._swot.update(details.swot);
    }
    
    this.registerDomainEvent(new VariationUpdatedEvent(this));
  }
  
  public select(): void {
    if (!this._isSelected) {
      this._isSelected = true;
      this.registerDomainEvent(new VariationSelectedEvent(this));
    }
  }
  
  public deselect(): void {
    if (this._isSelected) {
      this._isSelected = false;
      this.registerDomainEvent(new VariationDeselectedEvent(this));
    }
  }
}
```

### 4. Merged Idea Aggregate

The Merged Idea represents a combination of multiple variations. It manages:
- Merged idea details
- Source variation relationships
- Final selection state

```typescript
class MergedIdea extends AggregateRoot {
  private _id: UUID;
  private _canvasId: UUID;
  private _title: string;
  private _description: string;
  private _problemStatement: string;
  private _targetAudience: string;
  private _uniqueValue: string;
  private _swot: SWOTAnalysis;
  private _sourceVariations: UUID[];
  private _isSelected: boolean;
  
  constructor(params: MergedIdeaCreationParams, canvasId: UUID, sourceVariations: UUID[]) {
    super();
    this._id = params.id || UUID.generate();
    this._canvasId = canvasId;
    this._title = params.title;
    this._description = params.description;
    this._problemStatement = params.problemStatement;
    this._targetAudience = params.targetAudience;
    this._uniqueValue = params.uniqueValue;
    this._swot = new SWOTAnalysis(params.swot);
    this._sourceVariations = [...sourceVariations];
    this._isSelected = false;
    
    // Register domain event
    this.registerDomainEvent(new MergedIdeaCreatedEvent(this));
  }
  
  // Getters
  get id(): UUID { return this._id; }
  get canvasId(): UUID { return this._canvasId; }
  get title(): string { return this._title; }
  get description(): string { return this._description; }
  get problemStatement(): string { return this._problemStatement; }
  get targetAudience(): string { return this._targetAudience; }
  get uniqueValue(): string { return this._uniqueValue; }
  get swot(): SWOTAnalysis { return this._swot; }
  get sourceVariations(): UUID[] { return [...this._sourceVariations]; }
  get isSelected(): boolean { return this._isSelected; }
  
  // Commands
  public update(details: MergedIdeaUpdateParams): void {
    if (details.title) {
      this._title = details.title;
    }
    
    if (details.description) {
      this._description = details.description;
    }
    
    if (details.problemStatement) {
      this._problemStatement = details.problemStatement;
    }
    
    if (details.targetAudience) {
      this._targetAudience = details.targetAudience;
    }
    
    if (details.uniqueValue) {
      this._uniqueValue = details.uniqueValue;
    }
    
    if (details.swot) {
      this._swot.update(details.swot);
    }
    
    this.registerDomainEvent(new MergedIdeaUpdatedEvent(this));
  }
  
  public select(): void {
    if (!this._isSelected) {
      this._isSelected = true;
      this.registerDomainEvent(new MergedIdeaSelectedEvent(this));
    }
  }
  
  public deselect(): void {
    if (this._isSelected) {
      this._isSelected = false;
      this.registerDomainEvent(new MergedIdeaDeselectedEvent(this));
    }
  }
  
  public convertToIdea(): IdeaCreationParams {
    return {
      title: this._title,
      description: this._description,
      problemStatement: this._problemStatement,
      targetAudience: this._targetAudience,
      uniqueValue: this._uniqueValue
    };
  }
}
```

## Value Objects

Value objects are immutable objects that represent concepts in the domain that don't have an identity:

### 1. SWOT Analysis

```typescript
class SWOTAnalysis {
  private readonly _strengths: string[];
  private readonly _weaknesses: string[];
  private readonly _opportunities: string[];
  private readonly _threats: string[];
  
  constructor(params: SWOTParams) {
    this._strengths = params.strengths || [];
    this._weaknesses = params.weaknesses || [];
    this._opportunities = params.opportunities || [];
    this._threats = params.threats || [];
  }
  
  get strengths(): string[] { return [...this._strengths]; }
  get weaknesses(): string[] { return [...this._weaknesses]; }
  get opportunities(): string[] { return [...this._opportunities]; }
  get threats(): string[] { return [...this._threats]; }
  
  // Since value objects are immutable, we return a new instance on update
  public update(params: SWOTParams): SWOTAnalysis {
    return new SWOTAnalysis({
      strengths: params.strengths || this._strengths,
      weaknesses: params.weaknesses || this._weaknesses,
      opportunities: params.opportunities || this._opportunities,
      threats: params.threats || this._threats
    });
  }
}
```

### 2. Component

```typescript
class Component {
  private readonly _id: UUID;
  private readonly _ideaId: UUID;
  private readonly _type: ComponentType;
  private readonly _name: string;
  private readonly _content: any;
  
  constructor(params: ComponentParams, ideaId: UUID) {
    this._id = params.id || UUID.generate();
    this._ideaId = ideaId;
    this._type = params.type;
    this._name = params.name;
    this._content = params.content;
  }
  
  get id(): UUID { return this._id; }
  get ideaId(): UUID { return this._ideaId; }
  get type(): ComponentType { return this._type; }
  get name(): string { return this._name; }
  get content(): any { return this._content; }
  
  // Return a new instance on update
  public update(content: any): Component {
    return new Component({
      id: this._id,
      type: this._type,
      name: this._name,
      content
    }, this._ideaId);
  }
}
```

## Domain Services

Domain services implement business logic that doesn't naturally fit into entities:

### 1. Canvas Service

```typescript
interface CanvasService {
  createCanvas(params: CanvasCreationParams, userId: UUID): Promise<Canvas>;
  getCanvas(canvasId: UUID, userId: UUID): Promise<Canvas>;
  updateCanvas(canvasId: UUID, params: CanvasUpdateParams, userId: UUID): Promise<Canvas>;
  deleteCanvas(canvasId: UUID, userId: UUID): Promise<void>;
  listCanvases(userId: UUID): Promise<Canvas[]>;
  addCollaborator(canvasId: UUID, collaboratorId: UUID, permission: Permission, userId: UUID): Promise<void>;
  removeCollaborator(canvasId: UUID, collaboratorId: UUID, userId: UUID): Promise<void>;
}
```

### 2. Idea Service

```typescript
interface IdeaService {
  createIdea(params: IdeaCreationParams, canvasId: UUID, userId: UUID): Promise<Idea>;
  getIdea(ideaId: UUID, userId: UUID): Promise<Idea>;
  updateIdea(ideaId: UUID, params: IdeaUpdateParams, userId: UUID): Promise<Idea>;
  deleteIdea(ideaId: UUID, userId: UUID): Promise<void>;
  listIdeas(canvasId: UUID, userId: UUID): Promise<Idea[]>;
  
  generateIdea(params: IdeaGenerationParams, canvasId: UUID, userId: UUID): Promise<Idea>;
  refineIdea(ideaId: UUID, params: IdeaRefinementParams, userId: UUID): Promise<Idea>;
}
```

### 3. Variation Service

```typescript
interface VariationService {
  generateVariations(ideaId: UUID, count: number, userId: UUID): Promise<Variation[]>;
  getVariation(variationId: UUID, userId: UUID): Promise<Variation>;
  updateVariation(variationId: UUID, params: VariationUpdateParams, userId: UUID): Promise<Variation>;
  listVariations(ideaId: UUID, userId: UUID): Promise<Variation[]>;
  selectVariation(variationId: UUID, userId: UUID): Promise<void>;
  deselectVariation(variationId: UUID, userId: UUID): Promise<void>;
}
```

### 4. Merge Service

```typescript
interface MergeService {
  mergeVariations(variationIds: UUID[], userId: UUID): Promise<MergedIdea[]>;
  getMergedIdea(mergedIdeaId: UUID, userId: UUID): Promise<MergedIdea>;
  updateMergedIdea(mergedIdeaId: UUID, params: MergedIdeaUpdateParams, userId: UUID): Promise<MergedIdea>;
  listMergedIdeas(canvasId: UUID, userId: UUID): Promise<MergedIdea[]>;
  selectMergedIdea(mergedIdeaId: UUID, userId: UUID): Promise<void>;
  deselectMergedIdea(mergedIdeaId: UUID, userId: UUID): Promise<void>;
  convertToIdea(mergedIdeaId: UUID, userId: UUID): Promise<Idea>;
}
```

## Domain Events

Domain events represent important changes in the domain:

```typescript
// Base domain event class
abstract class DomainEvent {
  readonly type: string;
  readonly timestamp: Date;
  
  constructor(type: string) {
    this.type = type;
    this.timestamp = new Date();
  }
}

// Canvas events
class CanvasCreatedEvent extends DomainEvent {
  readonly canvas: Canvas;
  
  constructor(canvas: Canvas) {
    super('canvas.created');
    this.canvas = canvas;
  }
}

class CanvasUpdatedEvent extends DomainEvent {
  readonly canvas: Canvas;
  
  constructor(canvas: Canvas) {
    super('canvas.updated');
    this.canvas = canvas;
  }
}

// Idea events
class IdeaCreatedEvent extends DomainEvent {
  readonly idea: Idea;
  
  constructor(idea: Idea) {
    super('idea.created');
    this.idea = idea;
  }
}

class IdeaUpdatedEvent extends DomainEvent {
  readonly idea: Idea;
  
  constructor(idea: Idea) {
    super('idea.updated');
    this.idea = idea;
  }
}

// Variation events
class VariationCreatedEvent extends DomainEvent {
  readonly variation: Variation;
  
  constructor(variation: Variation) {
    super('variation.created');
    this.variation = variation;
  }
}

class VariationSelectedEvent extends DomainEvent {
  readonly variation: Variation;
  
  constructor(variation: Variation) {
    super('variation.selected');
    this.variation = variation;
  }
}
```

## Domain Event Handling

Domain events are processed by event handlers:

```typescript
interface DomainEventHandler<T extends DomainEvent> {
  handle(event: T): Promise<void>;
}

class CanvasCreatedEventHandler implements DomainEventHandler<CanvasCreatedEvent> {
  constructor(private readonly accessLogService: AccessLogService) {}
  
  async handle(event: CanvasCreatedEvent): Promise<void> {
    // Log the canvas creation
    await this.accessLogService.logCanvasCreation(
      event.canvas.id,
      event.canvas.ownerId,
      event.timestamp
    );
  }
}
```

## Domain Repositories

Repositories abstract data access for domain objects:

```typescript
interface CanvasRepository {
  save(canvas: Canvas): Promise<void>;
  findById(id: UUID): Promise<Canvas | null>;
  findByOwner(ownerId: UUID): Promise<Canvas[]>;
  findShared(userId: UUID): Promise<Canvas[]>;
  delete(id: UUID): Promise<void>;
}

interface IdeaRepository {
  save(idea: Idea): Promise<void>;
  findById(id: UUID): Promise<Idea | null>;
  findByCanvas(canvasId: UUID): Promise<Idea[]>;
  delete(id: UUID): Promise<void>;
}

interface VariationRepository {
  save(variation: Variation): Promise<void>;
  findById(id: UUID): Promise<Variation | null>;
  findByParentIdea(ideaId: UUID): Promise<Variation[]>;
  findSelected(ideaId: UUID): Promise<Variation[]>;
  delete(id: UUID): Promise<void>;
}

interface MergedIdeaRepository {
  save(mergedIdea: MergedIdea): Promise<void>;
  findById(id: UUID): Promise<MergedIdea | null>;
  findByCanvas(canvasId: UUID): Promise<MergedIdea[]>;
  findBySourceVariation(variationId: UUID): Promise<MergedIdea[]>;
  delete(id: UUID): Promise<void>;
}
```

## Service Implementation Example

Here's an example of a domain service implementation:

```typescript
class IdeaServiceImpl implements IdeaService {
  constructor(
    private readonly ideaRepository: IdeaRepository,
    private readonly canvasRepository: CanvasRepository,
    private readonly aiService: AIService,
    private readonly eventPublisher: DomainEventPublisher
  ) {}
  
  async createIdea(params: IdeaCreationParams, canvasId: UUID, userId: UUID): Promise<Idea> {
    // 1. Check if canvas exists and user has access
    const canvas = await this.canvasRepository.findById(canvasId);
    if (!canvas) {
      throw new DomainError('Canvas not found');
    }
    
    if (!canvas.canUserEdit(userId)) {
      throw new DomainError('User does not have edit permission');
    }
    
    // 2. Create the idea
    const idea = new Idea(params, canvasId);
    
    // 3. Save the idea
    await this.ideaRepository.save(idea);
    
    // 4. Publish domain events
    idea.domainEvents.forEach(event => {
      this.eventPublisher.publish(event);
    });
    
    return idea;
  }
  
  async generateIdea(params: IdeaGenerationParams, canvasId: UUID, userId: UUID): Promise<Idea> {
    // 1. Check if canvas exists and user has access
    const canvas = await this.canvasRepository.findById(canvasId);
    if (!canvas) {
      throw new DomainError('Canvas not found');
    }
    
    if (!canvas.canUserEdit(userId)) {
      throw new DomainError('User does not have edit permission');
    }
    
    // 2. Generate the idea using AI service
    const generatedIdeaParams = await this.aiService.generateIdea({
      industry: params.industry,
      constraints: params.constraints,
      targetAudience: params.targetAudience
    });
    
    // 3. Create and save the idea
    const idea = new Idea(generatedIdeaParams, canvasId);
    await this.ideaRepository.save(idea);
    
    // 4. Publish domain events
    idea.domainEvents.forEach(event => {
      this.eventPublisher.publish(event);
    });
    
    return idea;
  }
  
  // Other methods implementation...
}
```

## Transaction Management

To ensure consistency, operations that span multiple aggregates use transactions:

```typescript
class TransactionalVariationService implements VariationService {
  constructor(
    private readonly variationRepository: VariationRepository,
    private readonly ideaRepository: IdeaRepository,
    private readonly transactionManager: TransactionManager,
    private readonly aiService: AIService,
    private readonly eventPublisher: DomainEventPublisher
  ) {}
  
  async generateVariations(ideaId: UUID, count: number, userId: UUID): Promise<Variation[]> {
    // 1. Check if idea exists and user has access
    const idea = await this.ideaRepository.findById(ideaId);
    if (!idea) {
      throw new DomainError('Idea not found');
    }
    
    // 2. Use transaction to ensure all variations are saved or none
    return this.transactionManager.runInTransaction(async () => {
      // 3. Generate variations using AI service
      const variationParamsList = await this.aiService.generateVariations({
        ideaId,
        count,
        title: idea.title,
        description: idea.description,
        problemStatement: idea.problemStatement,
        targetAudience: idea.targetAudience,
        uniqueValue: idea.uniqueValue
      });
      
      // 4. Create variation entities
      const variations: Variation[] = [];
      
      for (

================
File: docs/idea-playground/components/PATHWAY1_INTEGRATION.md
================
# Pathway 1 AI Integration in Modular Architecture

## Overview

This document explains the integration of the Pathway 1 AI service into the modular architecture of the Idea Playground system. The implementation follows the modular design principles outlined in [MODULAR_ARCHITECTURE.md](../MODULAR_ARCHITECTURE.md).

## Structure

The Pathway 1 AI functionality has been refactored to use the following components:

1. **Original AI Service**: `src/lib/services/idea-pathway1-ai.service.ts`
   - Contains the core AI functionality including suggestion generation and merging
   - Exports a singleton instance `ideaPathway1AIService` for easy use across the application

2. **Modular AI Service**: `src/lib/services/idea-playground/llm/pathway1/ai.service.ts`
   - Wraps the original AI service to fit the modular architecture
   - Adds consistent error handling and logging
   - Provides a cleaner API for the adapter layer

3. **Pathway Adapter**: `src/lib/services/idea-playground/pathway1-adapter.ts`
   - Bridges the gap between the UI/application layer and the AI service
   - Handles persistence and data transformations
   - Uses the modular AI service for AI operations

## Benefits of This Approach

- **Backward Compatibility**: Existing code can still use the original AI service directly
- **Improved Modularity**: Follows the separation of concerns principle
- **Better Error Handling**: Consistent error handling and fallbacks
- **Enhanced Maintainability**: Easier to update or replace individual components
- **Consistent Logging**: Integrated with system-wide logging service

## Usage Examples

### Using the Modular Service through the Adapter (Recommended)

```typescript
import { pathway1Adapter } from '../lib/services/idea-playground';

// Generate suggestions
const suggestions = await pathway1Adapter.generateCompanySuggestions(idea, userId, 5);

// Merge suggestions
const merged = await pathway1Adapter.mergeSuggestions(selectedSuggestions, userId);
```

### Direct Access to the AI Service (Advanced Use Cases)

```typescript
import { pathway1AIService } from '../lib/services/idea-playground';

// Direct access to the AI service
const suggestions = await pathway1AIService.generateCompanySuggestions(idea, userId, 5);
```

## Implementation Details

### Error Handling

The implementation includes robust error handling with graceful fallbacks:

- If the AI service fails to generate suggestions, it falls back to mock suggestions after a 45-second timeout
- Both the UI layer and the service layer use synchronized 45-second timeouts to ensure consistency
- If the merging functionality fails, it provides a basic merging algorithm as fallback
- All errors are properly logged but don't crash the application with clear error messages to users

### Null Safety

The service handles edge cases like:

- Null or undefined ideas
- Missing properties in ideas or suggestions
- Empty suggestions arrays
- Invalid AI responses

## Future Improvements

1. Add unit tests for each component
2. Further refine the adapter to support more operations
3. Improve the validation of AI responses
4. Enhance the logging with more detailed context information

================
File: docs/idea-playground/components/UI_INTEGRATION.md
================
# UI Integration Architecture

This document details the UI component architecture and integration approach for the Idea Playground rebuild, focusing on creating a responsive, maintainable, and user-friendly interface.

## Architecture Overview

The UI architecture follows a component-based approach with clear separation of concerns:

```mermaid
graph TD
    subgraph "UI Layer"
        Components[UI Components]
        Pages[Pages]
        Routing[Routing]
    end
    
    subgraph "State Management"
        GlobalState[Global State]
        LocalState[Component State]
        QueryCache[API Query Cache]
    end
    
    subgraph "API Communication"
        APIHooks[API Hooks]
        QueryClient[Query Client]
        MutationClient[Mutation Client]
    end
    
    Components --> LocalState
    Pages --> Components
    Pages --> Routing
    Pages --> GlobalState
    Components --> APIHooks
    APIHooks --> QueryClient
    APIHooks --> MutationClient
    QueryClient --> QueryCache
    MutationClient --> GlobalState
```

## Design System

### 1. Component Library

The component library is structured as a hierarchy of increasingly specialized components:

- **Atomic Components**: Low-level UI primitives (buttons, inputs, etc.)
- **Molecular Components**: Combinations of atomic components (form fields, cards, etc.)
- **Organism Components**: Complex UI elements that form coherent sections of the interface
- **Template Components**: Page-level layouts and containers

### 2. Component Architecture

Each component follows a consistent architecture:

```typescript
// Component definition
export interface ButtonProps {
  variant: 'primary' | 'secondary' | 'danger';
  size: 'small' | 'medium' | 'large';
  label: string;
  onClick: () => void;
  disabled?: boolean;
  isLoading?: boolean;
}

export const Button: React.FC<ButtonProps> = ({
  variant,
  size,
  label,
  onClick,
  disabled = false,
  isLoading = false
}) => {
  // Event handlers
  const handleClick = () => {
    if (!disabled && !isLoading) {
      onClick();
    }
  };
  
  // CSS class composition
  const classes = cn(
    'button',
    `button--${variant}`,
    `button--${size}`,
    {
      'button--disabled': disabled,
      'button--loading': isLoading
    }
  );
  
  // Render
  return (
    <button className={classes} onClick={handleClick} disabled={disabled}>
      {isLoading ? <Spinner size="small" /> : label}
    </button>
  );
};
```

### 3. Responsive Design

The UI is built with mobile-first responsive design principles:

- Fluid layouts that adapt to different screen sizes
- Breakpoint-based styling for different device categories
- Touch-friendly interaction patterns

```typescript
// Responsive container example
export const ResponsiveContainer: React.FC<PropsWithChildren> = ({ children }) => {
  return (
    <div className="container">
      <div className="container__inner">{children}</div>
    </div>
  );
};

// CSS (using CSS modules or styled-components)
/*
.container {
  width: 100%;
  margin: 0 auto;
  padding: 0 1rem;
  
  @media (min-width: 640px) {
    max-width: 640px;
  }
  
  @media (min-width: 768px) {
    max-width: 768px;
  }
  
  @media (min-width: 1024px) {
    max-width: 1024px;
  }
}

.container__inner {
  width: 100%;
}
*/
```

## State Management

The state management architecture uses Zustand for global state with a slice pattern:

### 1. Store Structure

```typescript
// Store definition
interface AppState {
  // User slice
  user: User | null;
  isAuthenticated: boolean;
  setUser: (user: User | null) => void;
  
  // Canvas slice
  canvases: Canvas[];
  currentCanvas: Canvas | null;
  isLoadingCanvases: boolean;
  fetchCanvases: () => Promise<void>;
  setCurrentCanvas: (canvasId: string) => Promise<void>;
  
  // Idea slice
  ideas: Idea[];
  currentIdea: Idea | null;
  isLoadingIdeas: boolean;
  fetchIdeas: (canvasId: string) => Promise<void>;
  setCurrentIdea: (ideaId: string) => Promise<void>;
  
  // Variation slice
  variations: Variation[];
  selectedVariations: string[];
  isLoadingVariations: boolean;
  fetchVariations: (ideaId: string) => Promise<void>;
  toggleVariationSelection: (variationId: string) => void;
  
  // UI slice
  sidebarOpen: boolean;
  activeView: 'canvas' | 'idea' | 'variation' | 'merge';
  toggleSidebar: () => void;
  setActiveView: (view: 'canvas' | 'idea' | 'variation' | 'merge') => void;
}

// Store implementation
export const useStore = create<AppState>((set, get) => ({
  // User slice implementation
  user: null,
  isAuthenticated: false,
  setUser: (user) => set({ user, isAuthenticated: !!user }),
  
  // Canvas slice implementation
  canvases: [],
  currentCanvas: null,
  isLoadingCanvases: false,
  fetchCanvases: async () => {
    set({ isLoadingCanvases: true });
    try {
      const canvases = await canvasService.getCanvases();
      set({ canvases, isLoadingCanvases: false });
    } catch (error) {
      set({ isLoadingCanvases: false });
      throw error;
    }
  },
  setCurrentCanvas: async (canvasId) => {
    set({ isLoadingCanvases: true });
    try {
      const canvas = await canvasService.getCanvas(canvasId);
      set({ currentCanvas: canvas, isLoadingCanvases: false });
      await get().fetchIdeas(canvasId);
    } catch (error) {
      set({ isLoadingCanvases: false });
      throw error;
    }
  },
  
  // Additional slices implementation...
}));
```

### 2. Store Selectors

```typescript
// Component usage with selectors
const CanvasList: React.FC = () => {
  // Select only what the component needs
  const canvases = useStore((state) => state.canvases);
  const isLoading = useStore((state) => state.isLoadingCanvases);
  const fetchCanvases = useStore((state) => state.fetchCanvases);
  const setCurrentCanvas = useStore((state) => state.setCurrentCanvas);
  
  useEffect(() => {
    fetchCanvases();
  }, [fetchCanvases]);
  
  if (isLoading) {
    return <Loading />;
  }
  
  return (
    <div className="canvas-list">
      {canvases.map(canvas => (
        <CanvasCard
          key={canvas.id}
          canvas={canvas}
          onClick={() => setCurrentCanvas(canvas.id)}
        />
      ))}
    </div>
  );
};
```

### 3. Component-Level State

For component-specific state, React hooks are used:

```typescript
const IdeaForm: React.FC<IdeaFormProps> = ({ onSubmit, initialValues }) => {
  // Form state
  const [formState, setFormState] = useState<IdeaFormState>({
    title: initialValues?.title || '',
    description: initialValues?.description || '',
    problemStatement: initialValues?.problemStatement || '',
    targetAudience: initialValues?.targetAudience || '',
    uniqueValue: initialValues?.uniqueValue || ''
  });
  
  // Validation state
  const [errors, setErrors] = useState<IdeaFormErrors>({});
  
  // Change handler
  const handleChange = (e: React.ChangeEvent<HTMLInputElement | HTMLTextAreaElement>) => {
    const { name, value } = e.target;
    setFormState(prev => ({
      ...prev,
      [name]: value
    }));
    
    // Clear error when field is edited
    if (errors[name as keyof IdeaFormErrors]) {
      setErrors(prev => ({
        ...prev,
        [name]: undefined
      }));
    }
  };
  
  // Submit handler
  const handleSubmit = (e: React.FormEvent) => {
    e.preventDefault();
    
    // Validate
    const validationErrors = validateIdeaForm(formState);
    if (Object.keys(validationErrors).length > 0) {
      setErrors(validationErrors);
      return;
    }
    
    // Submit
    onSubmit(formState);
  };
  
  // ... render form
};
```

## API Communication

API communication is handled through React Query, providing automatic caching, refetching, and error handling:

### 1. Query Hooks

```typescript
// Query hook for fetching canvases
export const useCanvases = () => {
  return useQuery<Canvas[], Error>(
    ['canvases'],
    () => canvasService.getCanvases(),
    {
      staleTime: 5 * 60 * 1000, // 5 minutes
      cacheTime: 30 * 60 * 1000, // 30 minutes
      refetchOnWindowFocus: true,
      retry: 3
    }
  );
};

// Query hook for fetching a specific canvas
export const useCanvas = (id: string) => {
  return useQuery<Canvas, Error>(
    ['canvas', id],
    () => canvasService.getCanvas(id),
    {
      enabled: !!id,
      staleTime: 5 * 60 * 1000,
      cacheTime: 30 * 60 * 1000,
      refetchOnWindowFocus: true,
      retry: 3
    }
  );
};

// Query hook for fetching ideas for a canvas
export const useIdeas = (canvasId: string) => {
  return useQuery<Idea[], Error>(
    ['ideas', canvasId],
    () => ideaService.getIdeas(canvasId),
    {
      enabled: !!canvasId,
      staleTime: 5 * 60 * 1000,
      cacheTime: 30 * 60 * 1000,
      refetchOnWindowFocus: true,
      retry: 3
    }
  );
};
```

### 2. Mutation Hooks

```typescript
// Mutation hook for creating a canvas
export const useCreateCanvas = () => {
  const queryClient = useQueryClient();
  
  return useMutation<Canvas, Error, CanvasCreationParams>(
    (params) => canvasService.createCanvas(params),
    {
      onSuccess: (newCanvas) => {
        // Update canvases query
        queryClient.setQueryData<Canvas[]>(['canvases'], (old) => {
          return old ? [...old, newCanvas] : [newCanvas];
        });
      }
    }
  );
};

// Mutation hook for updating an idea
export const useUpdateIdea = () => {
  const queryClient = useQueryClient();
  
  return useMutation<Idea, Error, { id: string; params: IdeaUpdateParams }>(
    ({ id, params }) => ideaService.updateIdea(id, params),
    {
      onSuccess: (updatedIdea) => {
        // Update single idea query
        queryClient.setQueryData(['idea', updatedIdea.id], updatedIdea);
        
        // Update ideas list query
        queryClient.setQueryData<Idea[]>(
          ['ideas', updatedIdea.canvasId],
          (old) => {
            return old
              ? old.map(idea => idea.id === updatedIdea.id ? updatedIdea : idea)
              : [updatedIdea];
          }
        );
      }
    }
  );
};
```

### 3. Custom Hooks

Custom hooks combine queries, mutations, and state to provide feature-specific functionality:

```typescript
// Custom hook for idea generation workflow
export const useIdeaGeneration = (canvasId: string) => {
  const [generationParams, setGenerationParams] = useState<IdeaGenerationParams>({
    industry: '',
    constraints: []
  });
  
  const generateIdeaMutation = useMutation<Idea, Error, IdeaGenerationParams>(
    (params) => ideaService.generateIdea(params, canvasId),
    {
      onSuccess: (newIdea) => {
        queryClient.setQueryData<Idea[]>(['ideas', canvasId], (old) => {
          return old ? [...old, newIdea] : [newIdea];
        });
      }
    }
  );
  
  const regenerateIdeaMutation = useMutation<Idea, Error, string>(
    (ideaId) => ideaService.regenerateIdea(ideaId),
    {
      onSuccess: (updatedIdea) => {
        queryClient.setQueryData(['idea', updatedIdea.id], updatedIdea);
        queryClient.setQueryData<Idea[]>(
          ['ideas', updatedIdea.canvasId],
          (old) => {
            return old
              ? old.map(idea => idea.id === updatedIdea.id ? updatedIdea : idea)
              : [updatedIdea];
          }
        );
      }
    }
  );
  
  const updateGenerationParams = (params: Partial<IdeaGenerationParams>) => {
    setGenerationParams(prev => ({
      ...prev,
      ...params
    }));
  };
  
  const generateIdea = () => {
    return generateIdeaMutation.mutate(generationParams);
  };
  
  const regenerateIdea = (ideaId: string) => {
    return regenerateIdeaMutation.mutate(ideaId);
  };
  
  return {
    generationParams,
    updateGenerationParams,
    generateIdea,
    regenerateIdea,
    isGenerating: generateIdeaMutation.isLoading,
    isRegenerating: regenerateIdeaMutation.isLoading,
    error: generateIdeaMutation.error || regenerateIdeaMutation.error
  };
};
```

## Error Handling

The UI implements a robust error handling system:

### 1. Error Boundaries

```typescript
class ErrorBoundary extends React.Component<
  { fallback: React.ReactNode; children: React.ReactNode },
  { hasError: boolean, error: Error | null }
> {
  constructor(props: { fallback: React.ReactNode; children: React.ReactNode }) {
    super(props);
    this.state = { hasError: false, error: null };
  }
  
  static getDerivedStateFromError(error: Error) {
    return { hasError: true, error };
  }
  
  componentDidCatch(error: Error, errorInfo: React.ErrorInfo) {
    // Log error to monitoring service
    errorLoggingService.logError(error, errorInfo);
  }
  
  render() {
    if (this.state.hasError) {
      // Render fallback UI
      return this.props.fallback;
    }
    
    return this.props.children;
  }
}
```

### 2. API Error Handling

```typescript
// Error handling middleware
export const apiErrorMiddleware = async <T>(
  request: () => Promise<T>
): Promise<Result<T, ApiError>> => {
  try {
    const response = await request();
    return Result.ok(response);
  } catch (error) {
    // Convert to typed error
    const apiError = new ApiError(
      error.status || 500,
      error.message || 'Unknown error',
      error.data
    );
    
    // Log error
    errorLoggingService.logApiError(apiError);
    
    return Result.fail(apiError);
  }
};

// Usage in component
const IdeaDetailPage: React.FC<IdeaDetailPageProps> = ({ ideaId }) => {
  const { data, error, isLoading, refetch } = useIdea(ideaId);
  
  if (isLoading) {
    return <Loading />;
  }
  
  if (error) {
    return (
      <ErrorDisplay
        error={error}
        onRetry={refetch}
        message="Failed to load idea details"
      />
    );
  }
  
  // Render idea details
  return <IdeaDetailView idea={data} />;
};
```

### 3. Form Validation

```typescript
// Form validation
const validateIdeaForm = (values: IdeaFormState): IdeaFormErrors => {
  const errors: IdeaFormErrors = {};
  
  if (!values.title.trim()) {
    errors.title = 'Title is required';
  } else if (values.title.length > 100) {
    errors.title = 'Title cannot exceed 100 characters';
  }
  
  if (!values.description.trim()) {
    errors.description = 'Description is required';
  }
  
  if (!values.problemStatement.trim()) {
    errors.problemStatement = 'Problem statement is required';
  }
  
  if (!values.targetAudience.trim()) {
    errors.targetAudience = 'Target audience is required';
  }
  
  if (!values.uniqueValue.trim()) {
    errors.uniqueValue = 'Unique value proposition is required';
  }
  
  return errors;
};

// Form component with validation
const IdeaForm: React.FC<IdeaFormProps> = ({ onSubmit, initialValues }) => {
  // ... form state and handlers
  
  return (
    <form onSubmit={handleSubmit}>
      <FormField
        label="Title"
        name="title"
        value={formState.title}
        onChange={handleChange}
        error={errors.title}
        required
      />
      
      <FormField
        label="Description"
        name="description"
        value={formState.description}
        onChange={handleChange}
        error={errors.description}
        multiline
        required
      />
      
      {/* ... other fields */}
      
      <Button type="submit" label="Save" variant="primary" />
    </form>
  );
};
```

## Progressive Loading

The UI implements progressive loading patterns to provide feedback during long-running operations:

### 1. Skeleton Screens

```typescript
const IdeaCardSkeleton: React.FC = () => {
  return (
    <div className="idea-card idea-card--skeleton">
      <div className="idea-card__title skeleton" />
      <div className="idea-card__description skeleton" />
      <div className="idea-card__meta">
        <div className="idea-card__author skeleton" />
        <div className="idea-card__date skeleton" />
      </div>
    </div>
  );
};

const IdeaList: React.FC<IdeaListProps> = ({ canvasId }) => {
  const { data: ideas, isLoading } = useIdeas(canvasId);
  
  if (isLoading) {
    return (
      <div className="idea-list">
        {Array.from({ length: 6 }).map((_, index) => (
          <IdeaCardSkeleton key={index} />
        ))}
      </div>
    );
  }
  
  return (
    <div className="idea-list">
      {ideas.map(idea => (
        <IdeaCard key={idea.id} idea={idea} />
      ))}
    </div>
  );
};
```

### 2. Progress Indicators

```typescript
const IdeaGenerationButton: React.FC<IdeaGenerationButtonProps> = ({
  onClick,
  isGenerating
}) => {
  return (
    <Button
      variant="primary"
      label={isGenerating ? 'Generating...' : 'Generate Idea'}
      onClick={onClick}
      disabled={isGenerating}
      isLoading={isGenerating}
      loadingLabel="AI is working on your idea"
    />
  );
};
```

### 3. Streaming Updates

```typescript
const IdeaGenerationStream: React.FC<IdeaGenerationStreamProps> = ({
  params,
  canvasId
}) => {
  const [partialIdea, setPartialIdea] = useState<Partial<Idea> | null>(null);
  const [isComplete, setIsComplete] = useState(false);
  const [error, setError] = useState<Error | null>(null);
  
  useEffect(() => {
    const subscription = ideaService
      .generateIdeaStream(params, canvasId)
      .subscribe({
        next: (partialResult) => {
          setPartialIdea(prev => ({
            ...prev,
            ...partialResult
          }));
        },
        error: (err) => {
          setError(err);
        },
        complete: () => {
          setIsComplete(true);
        }
      });
    
    return () => subscription.unsubscribe();
  }, [params, canvasId]);
  
  if (error) {
    return (
      <ErrorDisplay
        error={error}
        message="Error generating idea"
        retry={() => {
          setError(null);
          setPartialIdea(null);
          setIsComplete(false);
        }}
      />
    );
  }
  
  return (
    <div className="idea-generation-stream">
      <StreamingIdeaCard
        partialIdea={partialIdea}
        isComplete={isComplete}
      />
      
      {!isComplete && (
        <ProgressIndicator
          message="AI is generating your idea"
          subtext="This may take a few seconds..."
        />
      )}
      
      {isComplete && (
        <Button
          variant="primary"
          label="Save Idea"
          onClick={() => saveIdea(partialIdea!)}
        />
      )}
    </div>
  );
};
```

## Component Integration Examples

### 1. Canvas Management

```typescript
// Canvas selection component
const CanvasSelector: React.FC = () => {
  const { data: canvases, isLoading } = useCanvases();
  const setCurrentCanvas = useStore(state => state.setCurrentCanvas);
  const currentCanvasId = useStore(state => state.currentCanvas?.id);
  
  if (isLoading) {
    return <SkeletonLoader count={3} height={40} />;
  }
  
  return (
    <DropdownSelect
      value={currentCanvasId}
      onChange={canvasId => setCurrentCanvas(canvasId)}
      options={canvases.map(canvas => ({
        value: canvas.id,
        label: canvas.name
      }))}
      placeholder="Select a canvas"
    />
  );
};

// Canvas dashboard page
const CanvasDashboardPage: React.FC = () => {
  const { data: canvases, isLoading, error, refetch } = useCanvases();
  const navigate = useNavigate();
  
  if (isLoading) {
    return <PageLoader />;
  }
  
  if (error) {
    return (
      <ErrorPageState
        title="Failed to load canvases"
        message="We couldn't load your canvases. Please try again."
        action={{ label: 'Retry', onClick: refetch }}
      />
    );
  }
  
  return (
    <PageLayout title="Your Canvases">
      <CanvasGrid>
        {canvases.map(canvas => (
          <CanvasCard
            key={canvas.id}
            canvas={canvas}
            onClick={() => navigate(`/canvas/${canvas.id}`)}
          />
        ))}
        <NewCanvasCard onClick={() => navigate('/canvas/new')} />
      </CanvasGrid>
    </PageLayout>
  );
};
```

### 2. Idea Generation

```typescript
// Idea generation form
const IdeaGenerationForm: React.FC<IdeaGenerationFormProps> = ({ canvasId }) => {
  const {
    generationParams,
    updateGenerationParams,
    generateIdea,
    isGenerating,
    error
  } = useIdeaGeneration(canvasId);
  
  return (
    <Card>
      <CardHeader title="Generate New Idea" />

================
File: docs/idea-playground/guides/CODE_STANDARDS.md
================
# Code Standards

This document outlines the coding standards and practices for the Idea Playground rebuild project. Adhering to these standards ensures code consistency, maintainability, and quality across the codebase.

## TypeScript Configuration

### Strict Type Checking

We use TypeScript with strict mode enabled to catch type-related issues at compile time:

```json
// tsconfig.json
{
  "compilerOptions": {
    "strict": true,
    "noImplicitAny": true,
    "strictNullChecks": true,
    "strictFunctionTypes": true,
    "strictBindCallApply": true,
    "strictPropertyInitialization": true,
    "noImplicitThis": true,
    "alwaysStrict": true,
    "noUncheckedIndexedAccess": true,
    "noImplicitReturns": true,
    "noFallthroughCasesInSwitch": true,
    "esModuleInterop": true,
    "skipLibCheck": true,
    "forceConsistentCasingInFileNames": true,
    "lib": ["dom", "dom.iterable", "esnext"],
    "module": "esnext",
    "moduleResolution": "node",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "jsx": "react-jsx",
    "target": "es2020"
  },
  "include": ["src/**/*"],
  "exclude": ["node_modules", "build", "dist", "**/*.spec.ts"]
}
```

## Naming Conventions

| Item | Convention | Example |
|------|------------|---------|
| Files | kebab-case for files, PascalCase for React components | `canvas-service.ts`, `IdeaCard.tsx` |
| Interfaces | PascalCase with "I" prefix for TypeScript interfaces | `ICanvasService`, `IIdeaRepository` |
| Types | PascalCase | `CanvasCreationParams`, `IdeaUpdateParams` |
| Classes | PascalCase | `Canvas`, `IdeaService` |
| Methods/Functions | camelCase | `createCanvas()`, `generateIdea()` |
| Variables | camelCase | `canvasId`, `currentUser` |
| Constants | UPPER_SNAKE_CASE | `MAX_CANVAS_COUNT`, `DEFAULT_TIMEOUT` |
| Components | PascalCase | `CanvasSelector`, `IdeaList` |
| Props | camelCase | `onSubmit`, `isLoading` |
| Enums | PascalCase with singular name | `Permission`, `ComponentType` |
| Private class members | camelCase with underscore prefix | `_id`, `_name` |

## Directory Structure

```
src/
  ├── components/                  # React components
  │   ├── canvas/                  # Canvas-related components
  │   ├── idea/                    # Idea-related components
  │   ├── variation/               # Variation-related components
  │   ├── merge/                   # Merge-related components
  │   ├── shared/                  # Shared/common components
  │   └── ui/                      # UI primitives
  │
  ├── domain/                      # Domain model
  │   ├── entities/                # Domain entities
  │   ├── value-objects/           # Value objects
  │   ├── events/                  # Domain events
  │   └── services/                # Domain services
  │
  ├── application/                 # Application services
  │   ├── commands/                # Command handlers
  │   ├── queries/                 # Query handlers
  │   └── services/                # Application services
  │
  ├── infrastructure/              # Infrastructure implementation
  │   ├── repositories/            # Repository implementations
  │   ├── ai/                      # AI service implementation
  │   ├── persistence/             # Database access
  │   └── api/                     # API client
  │
  ├── hooks/                       # Custom React hooks
  │   ├── domain/                  # Domain-specific hooks
  │   ├── ui/                      # UI-related hooks
  │   └── api/                     # API-related hooks
  │
  ├── pages/                       # Page components
  │   ├── canvas/                  # Canvas-related pages
  │   ├── idea/                    # Idea-related pages
  │   ├── variation/               # Variation-related pages
  │   └── merge/                   # Merge-related pages
  │
  ├── store/                       # Global state management
  │   ├── slices/                  # State slices 
  │   ├── selectors/               # State selectors
  │   └── middleware/              # Store middleware
  │
  ├── utils/                       # Utility functions
  │   ├── formatting/              # Formatting utilities
  │   ├── validation/              # Validation utilities
  │   └── error-handling/          # Error handling utilities
  │
  └── types/                       # TypeScript types and interfaces
      ├── domain/                  # Domain-related types
      ├── api/                     # API-related types
      └── ui/                      # UI-related types
```

## Code Organization

### Component Structure

React components should follow this structure:

```typescript
// Imports (grouped and ordered)
import React, { useState, useEffect } from 'react';
import { useQuery } from 'react-query';
// Internal imports (alphabetical order)
import { Button } from '../ui/Button';
import { Card } from '../ui/Card';
// Types
import type { Idea } from '../../types/domain/idea.types';

// Props type definition
interface IdeaCardProps {
  idea: Idea;
  onClick: () => void;
  isSelected?: boolean;
}

// Component
export const IdeaCard: React.FC<IdeaCardProps> = ({
  idea,
  onClick,
  isSelected = false
}) => {
  // State hooks
  const [isExpanded, setIsExpanded] = useState(false);
  
  // Other hooks
  const { data, isLoading } = useQuery(['idea-details', idea.id], () => 
    fetchIdeaDetails(idea.id)
  );
  
  // Effects
  useEffect(() => {
    if (isSelected) {
      setIsExpanded(true);
    }
  }, [isSelected]);
  
  // Event handlers
  const handleClick = () => {
    onClick();
  };
  
  const handleToggleExpand = (e: React.MouseEvent) => {
    e.stopPropagation();
    setIsExpanded(prev => !prev);
  };
  
  // Render methods
  const renderDetails = () => {
    if (!isExpanded) return null;
    
    return (
      <div className="idea-card__details">
        <p className="idea-card__problem">{idea.problemStatement}</p>
        <p className="idea-card__audience">{idea.targetAudience}</p>
      </div>
    );
  };
  
  // Component render
  return (
    <Card 
      className={`idea-card ${isSelected ? 'idea-card--selected' : ''}`}
      onClick={handleClick}
    >
      <div className="idea-card__header">
        <h3 className="idea-card__title">{idea.title}</h3>
        <Button 
          variant="icon"
          onClick={handleToggleExpand}
          aria-label={isExpanded ? 'Collapse' : 'Expand'}
          icon={isExpanded ? 'chevron-up' : 'chevron-down'}
        />
      </div>
      
      <p className="idea-card__description">{idea.description}</p>
      
      {renderDetails()}
    </Card>
  );
};
```

### Service Structure

Services should follow this structure:

```typescript
// Imports
import { injectable, inject } from 'inversify';
import { TYPES } from '../types';
// Types
import type { Idea } from '../types/domain/idea.types';
import type { IIdeaRepository } from '../domain/repositories/idea-repository.interface';
import type { IAIService } from '../infrastructure/ai/ai-service.interface';
import type { IEventPublisher } from '../infrastructure/events/event-publisher.interface';

// Service interface
export interface IIdeaService {
  createIdea(params: IdeaCreationParams, canvasId: string, userId: string): Promise<Idea>;
  getIdea(ideaId: string, userId: string): Promise<Idea>;
  updateIdea(ideaId: string, params: IdeaUpdateParams, userId: string): Promise<Idea>;
  deleteIdea(ideaId: string, userId: string): Promise<void>;
  listIdeas(canvasId: string, userId: string): Promise<Idea[]>;
  generateIdea(params: IdeaGenerationParams, canvasId: string, userId: string): Promise<Idea>;
}

// Service implementation
@injectable()
export class IdeaService implements IIdeaService {
  constructor(
    @inject(TYPES.IdeaRepository) private ideaRepository: IIdeaRepository,
    @inject(TYPES.CanvasRepository) private canvasRepository: ICanvasRepository,
    @inject(TYPES.AIService) private aiService: IAIService,
    @inject(TYPES.EventPublisher) private eventPublisher: IEventPublisher
  ) {}

  // Method implementations
  async createIdea(params: IdeaCreationParams, canvasId: string, userId: string): Promise<Idea> {
    // Method implementation here
  }
  
  // Additional methods...
}
```

### Domain Entity Structure

Domain entities should follow this structure:

```typescript
import { AggregateRoot } from '../core/aggregate-root';
import { DomainError } from '../errors/domain-error';
import { IdeaCreatedEvent } from '../events/idea-created.event';
import { Component } from '../value-objects/component';

export class Idea extends AggregateRoot {
  // Private properties
  private _id: string;
  private _canvasId: string;
  private _title: string;
  private _description: string;
  private _problemStatement: string;
  private _targetAudience: string;
  private _uniqueValue: string;
  private _components: Component[];
  
  // Constructor
  constructor(params: IdeaCreationParams, canvasId: string) {
    super();
    this._id = params.id || generateUUID();
    this._canvasId = canvasId;
    this._title = params.title;
    this._description = params.description;
    this._problemStatement = params.problemStatement;
    this._targetAudience = params.targetAudience;
    this._uniqueValue = params.uniqueValue;
    this._components = [];
    
    // Validate invariants
    this.validateTitle(this._title);
    
    // Register domain event
    this.registerDomainEvent(new IdeaCreatedEvent(this));
  }
  
  // Getters
  get id(): string { return this._id; }
  get canvasId(): string { return this._canvasId; }
  get title(): string { return this._title; }
  get description(): string { return this._description; }
  get problemStatement(): string { return this._problemStatement; }
  get targetAudience(): string { return this._targetAudience; }
  get uniqueValue(): string { return this._uniqueValue; }
  get components(): Component[] { return [...this._components]; }
  
  // Commands
  public update(details: IdeaUpdateParams): void {
    if (details.title) {
      this.validateTitle(details.title);
      this._title = details.title;
    }
    
    // Additional updates...
    
    this.registerDomainEvent(new IdeaUpdatedEvent(this));
  }
  
  // Private methods
  private validateTitle(title: string): void {
    if (!title || title.trim().length === 0) {
      throw new DomainError('Idea title cannot be empty');
    }
    
    if (title.length > 100) {
      throw new DomainError('Idea title cannot exceed 100 characters');
    }
  }
}
```

## Coding Conventions

### General Guidelines

- Use TypeScript for all new code
- Use ESLint and Prettier for code quality and formatting
- Follow SOLID principles
- Write single-responsibility functions and classes
- Use dependency injection
- Prefer immutability
- Use early returns to reduce nesting
- Avoid side effects in pure functions

### Error Handling

```typescript
// Error class hierarchy
export abstract class AppError extends Error {
  constructor(message: string) {
    super(message);
    this.name = this.constructor.name;
    Object.setPrototypeOf(this, AppError.prototype);
  }
}

export class DomainError extends AppError {
  constructor(message: string) {
    super(message);
    Object.setPrototypeOf(this, DomainError.prototype);
  }
}

export class ValidationError extends AppError {
  constructor(message: string) {
    super(message);
    Object.setPrototypeOf(this, ValidationError.prototype);
  }
}

export class NotFoundError extends AppError {
  constructor(entityName: string, id: string) {
    super(`${entityName} with ID ${id} not found`);
    Object.setPrototypeOf(this, NotFoundError.prototype);
  }
}

export class AuthorizationError extends AppError {
  constructor(message: string) {
    super(message);
    Object.setPrototypeOf(this, AuthorizationError.prototype);
  }
}

// Error handling in services
async getIdea(ideaId: string, userId: string): Promise<Idea> {
  // Check if idea exists
  const idea = await this.ideaRepository.findById(ideaId);
  if (!idea) {
    throw new NotFoundError('Idea', ideaId);
  }
  
  // Check if user has access
  const canvas = await this.canvasRepository.findById(idea.canvasId);
  if (!canvas) {
    throw new DomainError('Canvas not found for idea');
  }
  
  if (!canvas.canUserAccess(userId)) {
    throw new AuthorizationError('User does not have permission to access this idea');
  }
  
  return idea;
}
```

### Async Code

Use async/await for asynchronous operations:

```typescript
// Good
async function loadIdeas(canvasId: string): Promise<Idea[]> {
  try {
    const ideas = await ideaService.listIdeas(canvasId);
    return ideas;
  } catch (error) {
    logger.error('Failed to load ideas', error);
    throw error;
  }
}

// Avoid
function loadIdeas(canvasId: string): Promise<Idea[]> {
  return ideaService.listIdeas(canvasId)
    .then(ideas => {
      return ideas;
    })
    .catch(error => {
      logger.error('Failed to load ideas', error);
      throw error;
    });
}
```

### React Patterns

#### Functional Components

Use functional components with hooks:

```typescript
// Good
const IdeaCard: React.FC<IdeaCardProps> = ({ idea, onClick }) => {
  const [isExpanded, setIsExpanded] = useState(false);
  
  return (
    <div className="idea-card" onClick={onClick}>
      <h3>{idea.title}</h3>
      <button onClick={() => setIsExpanded(!isExpanded)}>
        {isExpanded ? 'Collapse' : 'Expand'}
      </button>
      
      {isExpanded && (
        <div className="idea-card__details">
          <p>{idea.description}</p>
        </div>
      )}
    </div>
  );
};

// Avoid
class IdeaCard extends React.Component<IdeaCardProps, IdeaCardState> {
  constructor(props: IdeaCardProps) {
    super(props);
    this.state = {
      isExpanded: false
    };
  }
  
  toggleExpanded = () => {
    this.setState(prevState => ({
      isExpanded: !prevState.isExpanded
    }));
  };
  
  render() {
    const { idea, onClick } = this.props;
    const { isExpanded } = this.state;
    
    return (
      <div className="idea-card" onClick={onClick}>
        <h3>{idea.title}</h3>
        <button onClick={this.toggleExpanded}>
          {isExpanded ? 'Collapse' : 'Expand'}
        </button>
        
        {isExpanded && (
          <div className="idea-card__details">
            <p>{idea.description}</p>
          </div>
        )}
      </div>
    );
  }
}
```

#### Custom Hooks

Extract reusable logic into custom hooks:

```typescript
// Custom hook for idea operations
function useIdeaOperations(canvasId: string) {
  const queryClient = useQueryClient();
  
  // Get ideas query
  const ideasQuery = useQuery(
    ['ideas', canvasId],
    () => ideaService.listIdeas(canvasId),
    { enabled: !!canvasId }
  );
  
  // Create idea mutation
  const createIdeaMutation = useMutation(
    (params: IdeaCreationParams) => ideaService.createIdea(params, canvasId),
    {
      onSuccess: (newIdea) => {
        queryClient.setQueryData<Idea[]>(['ideas', canvasId], (oldData) => {
          return oldData ? [...oldData, newIdea] : [newIdea];
        });
      }
    }
  );
  
  // Delete idea mutation
  const deleteIdeaMutation = useMutation(
    (ideaId: string) => ideaService.deleteIdea(ideaId),
    {
      onSuccess: (_, ideaId) => {
        queryClient.setQueryData<Idea[]>(['ideas', canvasId], (oldData) => {
          return oldData ? oldData.filter(idea => idea.id !== ideaId) : [];
        });
      }
    }
  );
  
  return {
    ideas: ideasQuery.data || [],
    isLoading: ideasQuery.isLoading,
    error: ideasQuery.error,
    createIdea: createIdeaMutation.mutate,
    isCreating: createIdeaMutation.isLoading,
    deleteIdea: deleteIdeaMutation.mutate,
    isDeleting: deleteIdeaMutation.isLoading,
    refetch: ideasQuery.refetch
  };
}
```

## Documentation

### Code Documentation

Use JSDoc for documenting code:

```typescript
/**
 * Generates business idea variations based on a parent idea.
 * 
 * @param ideaId - The ID of the parent idea
 * @param count - The number of variations to generate (default: 3)
 * @param userId - The ID of the user making the request
 * @returns A list of generated variations
 * @throws {NotFoundError} If the parent idea doesn't exist
 * @throws {AuthorizationError} If the user doesn't have access to the parent idea
 */
async generateVariations(
  ideaId: string, 
  count: number = 3, 
  userId: string
): Promise<Variation[]> {
  // Implementation...
}
```

### Interface Documentation

Document interfaces with JSDoc:

```typescript
/**
 * Repository interface for Canvas entity operations.
 */
export interface ICanvasRepository {
  /**
   * Saves a canvas to the repository.
   * 
   * @param canvas - The canvas to save
   */
  save(canvas: Canvas): Promise<void>;
  
  /**
   * Finds a canvas by its ID.
   * 
   * @param id - The canvas ID
   * @returns The canvas if found, null otherwise
   */
  findById(id: string): Promise<Canvas | null>;
  
  /**
   * Finds all canvases owned by a user.
   * 
   * @param ownerId - The owner's user ID
   * @returns An array of canvases
   */
  findByOwner(ownerId: string): Promise<Canvas[]>;
  
  /**
   * Finds all canvases shared with a user.
   * 
   * @param userId - The user ID
   * @returns An array of canvases
   */
  findShared(userId: string): Promise<Canvas[]>;
  
  /**
   * Deletes a canvas from the repository.
   * 
   * @param id - The canvas ID
   */
  delete(id: string): Promise<void>;
}
```

## Performance Guidelines

1. **Memoization**: Use `useMemo` and `useCallback` for expensive computations and callbacks
2. **Virtualization**: Use virtualized lists for large datasets
3. **Lazy Loading**: Lazy load components and data
4. **Code Splitting**: Split code into smaller chunks
5. **Debounce/Throttle**: Limit the frequency of expensive operations

## Linting and Formatting

We use ESLint with a custom configuration:

```json
// .eslintrc.json
{
  "parser": "@typescript-eslint/parser",
  "extends": [
    "eslint:recommended",
    "plugin:@typescript-eslint/recommended",
    "plugin:react/recommended",
    "plugin:react-hooks/recommended",
    "plugin:jsx-a11y/recommended",
    "prettier"
  ],
  "plugins": [
    "@typescript-eslint",
    "react",
    "react-hooks",
    "jsx-a11y",
    "prettier"
  ],
  "rules": {
    "no-console": ["warn", { "allow": ["warn", "error"] }],
    "no-alert": "error",
    "@typescript-eslint/explicit-function-return-type": "off",
    "@typescript-eslint/no-explicit-any": "error",
    "@typescript-eslint/no-unused-vars": ["error", { "argsIgnorePattern": "^_" }],
    "react/prop-types": "off",
    "react-hooks/rules-of-hooks": "error",
    "react-hooks/exhaustive-deps": "warn",
    "prettier/prettier": "error"
  },
  "settings": {
    "react": {
      "version": "detect"
    }
  }
}
```

We use Prettier for code formatting:

```json
// .prettierrc
{
  "printWidth": 100,
  "tabWidth": 2,
  "useTabs": false,
  "semi": true,
  "singleQuote": true,
  "trailingComma": "es5",
  "bracketSpacing": true,
  "jsxBracketSameLine": false,
  "arrowParens": "avoid"
}
```

## Git Workflow

### Branch Naming

- `feature/description`: New features
- `fix/description`: Bug fixes
- `refactor/description`: Code refactoring
- `docs/description`: Documentation updates
- `test/description`: Test additions or modifications
- `chore/description`: Routine tasks, maintenance, etc.

### Commit Messages

Follow the conventional commit format:

```
<type>(<scope>): <description>

[optional body]

[optional footer(s)]
```

Types:
- `feat`: New feature
- `fix`: Bug fix
- `docs`: Documentation changes
- `style`: Code style changes (formatting, etc.)
- `refactor`: Code refactoring
- `test`: Test additions or modifications
- `chore`: Routine tasks, maintenance, etc.

Example:
```
feat(canvas): add canvas sharing functionality

This commit adds the ability for users to share canvases with team members.
- Added collaboration service
- Implemented invite functionality
- Added permission system

Closes #123
```

## Security Guidelines

1. **Input Validation**: Validate all user input
2. **Authentication**: Use JWT for authentication
3. **Authorization**: Implement proper access control
4. **Data Protection**: Encrypt sensitive data
5. **CSRF Protection**: Implement CSRF tokens
6. **Content Security Policy**: Define CSP headers
7. **Error Handling**: Don't expose sensitive information in error messages

## Accessibility Guidelines

1. Use semantic HTML elements
2. Provide alternative text for images
3. Ensure sufficient color contrast
4. Support keyboard navigation
5. Add ARIA attributes where necessary
6. Test with screen readers

## Testing Guidelines

See the [Testing Strategy](./TESTING_STRATEGY.md) document for detailed testing guidelines.

## Continuous Integration

Our CI pipeline runs these checks on every PR:

1. Linting
2. Type checking
3. Unit tests
4. Integration tests
5. Build verification
6. Bundle size analysis

## Development Environment Setup

See the [Contributing Guide](./CONTRIBUTING.md) for detailed instructions on setting up the development environment.

================
File: docs/idea-playground/guides/TESTING_STRATEGY.md
================
# Testing Strategy

This document outlines the comprehensive testing approach for the Idea Playground rebuild, ensuring quality, reliability, and performance of the system.

## Testing Philosophy

Our testing strategy follows these key principles:

1. **Shift Left**: Find defects as early as possible in the development lifecycle
2. **Automation First**: Automate tests wherever possible for consistency and efficiency
3. **Pyramid Approach**: Focus on a solid foundation of unit tests, supported by integration and E2E tests
4. **Risk-Based**: Prioritize testing of critical paths and high-risk components
5. **Continuous Testing**: Run tests automatically as part of the CI/CD pipeline

## Testing Pyramid

```mermaid
graph TD
    E2E[End-to-End Tests] --> Integration[Integration Tests]
    Integration --> Unit[Unit Tests]
    
    classDef e2e fill:#f9f,stroke:#333,stroke-width:2px;
    classDef int fill:#bbf,stroke:#333,stroke-width:2px;
    classDef unit fill:#bfb,stroke:#333,stroke-width:2px;
    
    class E2E e2e;
    class Integration int;
    class Unit unit;
```

Our testing pyramid consists of:
- **Many Unit Tests**: Fast, focused tests that verify individual components and functions
- **Some Integration Tests**: Tests that verify interactions between components
- **Few End-to-End Tests**: Tests that verify complete user flows

## Unit Testing

### Approach

Unit tests focus on testing individual components, functions, and classes in isolation:

```typescript
// Example unit test for a domain entity
describe('Canvas', () => {
  it('should create a canvas with valid parameters', () => {
    // Arrange
    const params: CanvasCreationParams = {
      name: 'Test Canvas',
      description: 'A test canvas',
      tags: ['test', 'canvas']
    };
    const ownerId = '123';
    
    // Act
    const canvas = new Canvas(params, ownerId);
    
    // Assert
    expect(canvas.name).toBe('Test Canvas');
    expect(canvas.description).toBe('A test canvas');
    expect(canvas.tags).toEqual(['test', 'canvas']);
    expect(canvas.ownerId).toBe('123');
  });
  
  it('should throw an error if name is empty', () => {
    // Arrange
    const params: CanvasCreationParams = {
      name: '',
      description: 'A test canvas',
      tags: ['test', 'canvas']
    };
    const ownerId = '123';
    
    // Act & Assert
    expect(() => new Canvas(params, ownerId)).toThrow('Canvas name cannot be empty');
  });
  
  it('should publish CanvasCreatedEvent when created', () => {
    // Arrange
    const params: CanvasCreationParams = {
      name: 'Test Canvas',
      description: 'A test canvas',
      tags: ['test', 'canvas']
    };
    const ownerId = '123';
    
    // Act
    const canvas = new Canvas(params, ownerId);
    
    // Assert
    expect(canvas.domainEvents).toHaveLength(1);
    expect(canvas.domainEvents[0]).toBeInstanceOf(CanvasCreatedEvent);
    expect((canvas.domainEvents[0] as CanvasCreatedEvent).canvas).toBe(canvas);
  });
});
```

### Unit Testing Targets

1. **Domain Entities**: Test business rules, invariants, and behavior
2. **Value Objects**: Test immutability and valid state
3. **Domain Services**: Test business logic and domain rules
4. **Application Services**: Test orchestration of domain services
5. **UI Components**: Test rendering and user interactions
6. **Hooks**: Test custom hook behavior and state changes
7. **Utilities**: Test helper functions and utilities

### Mocking Strategy

We use Jest's mocking capabilities to isolate units under test:

```typescript
// Example of mocking dependencies
jest.mock('../repositories/canvasRepository');
jest.mock('../services/aiService');

const mockCanvasRepository = canvasRepository as jest.Mocked<CanvasRepository>;
const mockAIService = aiService as jest.Mocked<AIService>;

describe('IdeaService', () => {
  let ideaService: IdeaService;
  
  beforeEach(() => {
    // Setup mocks
    mockCanvasRepository.findById.mockResolvedValue(mockCanvas);
    mockAIService.generateIdea.mockResolvedValue(mockIdeaResult);
    
    // Create service with mocked dependencies
    ideaService = new IdeaServiceImpl(
      mockIdeaRepository,
      mockCanvasRepository,
      mockAIService,
      mockEventPublisher
    );
  });
  
  afterEach(() => {
    jest.clearAllMocks();
  });
  
  it('should call AI service to generate an idea', async () => {
    // Arrange
    const params: IdeaGenerationParams = {
      industry: 'Technology',
      constraints: ['B2B', 'SaaS']
    };
    const canvasId = '123';
    const userId = '456';
    
    // Act
    await ideaService.generateIdea(params, canvasId, userId);
    
    // Assert
    expect(mockAIService.generateIdea).toHaveBeenCalledWith({
      industry: 'Technology',
      constraints: ['B2B', 'SaaS'],
      targetAudience: undefined
    });
  });
});
```

## Integration Testing

### Approach

Integration tests verify the correct interaction between components:

```typescript
// Example integration test for a service with its repository
describe('IdeaService Integration', () => {
  let ideaService: IdeaService;
  let canvasRepository: CanvasRepository;
  let ideaRepository: IdeaRepository;
  let aiService: AIService;
  let eventPublisher: DomainEventPublisher;
  
  beforeEach(async () => {
    // Setup test database
    await setupTestDatabase();
    
    // Create real repositories connected to test database
    canvasRepository = new SupabaseCanvasRepository(supabaseClient);
    ideaRepository = new SupabaseIdeaRepository(supabaseClient);
    
    // Mock AI service and event publisher
    aiService = mock<AIService>();
    eventPublisher = mock<DomainEventPublisher>();
    
    // Create service with real repositories and mocked services
    ideaService = new IdeaServiceImpl(
      ideaRepository,
      canvasRepository,
      aiService,
      eventPublisher
    );
    
    // Seed test data
    await seedTestCanvas(canvasRepository);
  });
  
  afterEach(async () => {
    await cleanupTestDatabase();
  });
  
  it('should save generated idea to repository', async () => {
    // Arrange
    const params: IdeaGenerationParams = {
      industry: 'Technology',
      constraints: ['B2B', 'SaaS']
    };
    const canvasId = 'test-canvas-id';
    const userId = 'test-user-id';
    
    // Setup mock AI service
    when(aiService.generateIdea).calledWith(anything()).mockResolvedValue({
      title: 'Test Idea',
      description: 'A test idea',
      problemStatement: 'A test problem',
      targetAudience: 'Test audience',
      uniqueValue: 'Test value'
    });
    
    // Act
    const idea = await ideaService.generateIdea(params, canvasId, userId);
    
    // Assert
    const savedIdea = await ideaRepository.findById(idea.id);
    expect(savedIdea).not.toBeNull();
    expect(savedIdea?.title).toBe('Test Idea');
    expect(savedIdea?.canvasId).toBe(canvasId);
  });
});
```

### Integration Testing Targets

1. **Services + Repositories**: Test services with real repositories
2. **Services + External APIs**: Test integration with external systems
3. **UI Components + State Management**: Test components with state management
4. **API Endpoints + Services**: Test API endpoints with services

### Component Integration Testing

For React components, we use React Testing Library to test component integration:

```typescript
// Example component integration test
describe('IdeaList', () => {
  it('should render ideas from API', async () => {
    // Arrange
    const mockIdeas = [
      {
        id: '1',
        canvasId: 'canvas-1',
        title: 'Idea 1',
        description: 'Description 1'
      },
      {
        id: '2',
        canvasId: 'canvas-1',
        title: 'Idea 2',
        description: 'Description 2'
      }
    ];
    
    // Mock hooks
    jest.mock('../../hooks/useIdeas', () => ({
      useIdeas: () => ({
        data: mockIdeas,
        isLoading: false,
        error: null
      })
    }));
    
    // Act
    const { findByText } = render(<IdeaList canvasId="canvas-1" />);
    
    // Assert
    expect(await findByText('Idea 1')).toBeInTheDocument();
    expect(await findByText('Idea 2')).toBeInTheDocument();
  });
  
  it('should show loading state', () => {
    // Arrange
    jest.mock('../../hooks/useIdeas', () => ({
      useIdeas: () => ({
        data: null,
        isLoading: true,
        error: null
      })
    }));
    
    // Act
    const { container } = render(<IdeaList canvasId="canvas-1" />);
    
    // Assert
    expect(container.querySelectorAll('.skeleton')).toHaveLength(6);
  });
});
```

## End-to-End Testing

### Approach

End-to-end tests verify complete user flows:

```typescript
// Example E2E test using Cypress
describe('Idea Generation Flow', () => {
  beforeEach(() => {
    // Login and navigate to the canvas page
    cy.login('test-user@example.com', 'password');
    cy.visit('/canvas/test-canvas-id');
    cy.contains('h1', 'Test Canvas').should('be.visible');
  });
  
  it('should generate a new idea', () => {
    // Click the generate idea button
    cy.contains('button', 'Generate Idea').click();
    
    // Fill in the generation form
    cy.get('input[name="industry"]').type('Technology');
    cy.get('div[role="combobox"]').click();
    cy.contains('li', 'B2B').click();
    cy.contains('li', 'SaaS').click();
    
    // Submit the form
    cy.contains('button', 'Generate').click();
    
    // Wait for the idea to be generated
    cy.contains('AI is generating your idea').should('be.visible');
    cy.contains('AI is generating your idea', { timeout: 20000 }).should('not.exist');
    
    // Verify the idea was created
    cy.contains('h2', /New Idea|Generated Idea/).should('be.visible');
    cy.contains('button', 'Save Idea').click();
    
    // Verify the idea appears in the list
    cy.contains('a', /New Idea|Generated Idea/).should('be.visible');
  });
});
```

### Critical User Flows

We prioritize E2E tests for these critical flows:

1. **Canvas Creation and Management**
2. **Idea Generation (Manual and AI-assisted)**
3. **Variation Generation and Selection**
4. **Idea Merging and Refinement**
5. **Collaboration and Sharing**

### E2E Testing Tools

- **Cypress**: For browser-based E2E testing
- **Playwright**: For cross-browser testing
- **Storybook**: For visual regression testing

## API Testing

### Approach

We test API endpoints with Supertest:

```typescript
// Example API test
describe('Canvas API', () => {
  let app: Express;
  let token: string;
  
  beforeAll(async () => {
    app = createTestApp();
    token = await getTestToken();
  });
  
  it('should create a canvas', async () => {
    // Arrange
    const canvasData = {
      name: 'Test Canvas',
      description: 'A test canvas',
      tags: ['test', 'canvas']
    };
    
    // Act
    const response = await request(app)
      .post('/api/canvas')
      .set('Authorization', `Bearer ${token}`)
      .send(canvasData);
    
    // Assert
    expect(response.status).toBe(201);
    expect(response.body.name).toBe('Test Canvas');
    expect(response.body.id).toBeDefined();
  });
  
  it('should return 400 for invalid canvas data', async () => {
    // Arrange
    const canvasData = {
      // Missing name
      description: 'A test canvas',
      tags: ['test', 'canvas']
    };
    
    // Act
    const response = await request(app)
      .post('/api/canvas')
      .set('Authorization', `Bearer ${token}`)
      .send(canvasData);
    
    // Assert
    expect(response.status).toBe(400);
    expect(response.body.error).toContain('name');
  });
});
```

### API Testing Coverage

1. **Input Validation**: Test validation of request bodies
2. **Authentication**: Test authentication and authorization
3. **Error Handling**: Test error responses
4. **Success Paths**: Test successful operations
5. **Edge Cases**: Test boundary conditions and special cases

## Performance Testing

### Approach

Performance tests verify system performance under various conditions:

```typescript
// Example performance test using K6
export default function() {
  // Load test configuration
  const BASE_URL = __ENV.BASE_URL || 'http://localhost:3000';
  const USERS = parseInt(__ENV.USERS || '10');
  const DURATION = __ENV.DURATION || '30s';
  
  // Test options
  export let options = {
    vus: USERS,
    duration: DURATION,
    thresholds: {
      'http_req_duration': ['p(95)<500'], // 95% of requests must complete below 500ms
      'http_req_failed': ['rate<0.01'],    // Less than 1% of requests should fail
    },
  };
  
  // Test scenario
  group('Canvas API', function() {
    // Get canvases
    let getCanvasesRes = http.get(`${BASE_URL}/api/canvas`, {
      headers: { Authorization: `Bearer ${getToken()}` },
    });
    check(getCanvasesRes, {
      'status is 200': (r) => r.status === 200,
      'response time < 200ms': (r) => r.timings.duration < 200,
    });
    sleep(1);
    
    // Create canvas
    let createCanvasRes = http.post(`${BASE_URL}/api/canvas`, {
      headers: { 
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${getToken()}`
      },
      body: JSON.stringify({
        name: `Test Canvas ${__VU}`,
        description: 'Performance test canvas',
        tags: ['test', 'performance']
      }),
    });
    check(createCanvasRes, {
      'status is 201': (r) => r.status === 201,
      'response time < 300ms': (r) => r.timings.duration < 300,
    });
    sleep(1);
  });
}
```

### Performance Test Types

1. **Load Testing**: Verify system behavior under expected load
2. **Stress Testing**: Verify system behavior under extreme load
3. **Endurance Testing**: Verify system behavior over time
4. **Spike Testing**: Verify system behavior with sudden load increases

### Performance Metrics

We track these key performance metrics:

1. **Response Time**: Time to complete a request
2. **Throughput**: Number of requests per second
3. **Error Rate**: Percentage of failed requests
4. **Resource Utilization**: CPU, memory, network usage
5. **Time to First Byte (TTFB)**: Time to start receiving response

## AI Behavior Testing

### Approach

AI behavior tests verify the correctness and quality of AI-generated content:

```typescript
// Example AI behavior test
describe('AI Idea Generation', () => {
  let aiService: AIService;
  
  beforeEach(() => {
    aiService = new AIServiceImpl(openaiClient, promptManager);
  });
  
  it('should generate an idea with required fields', async () => {
    // Arrange
    const params: IdeaGenerationParams = {
      industry: 'Technology',
      constraints: ['B2B', 'SaaS']
    };
    
    // Act
    const result = await aiService.generateIdea(params);
    
    // Assert
    expect(result.title).toBeDefined();
    expect(result.title.length).toBeGreaterThan(0);
    expect(result.description).toBeDefined();
    expect(result.description.length).toBeGreaterThan(0);
    expect(result.problemStatement).toBeDefined();
    expect(result.targetAudience).toBeDefined();
    expect(result.uniqueValue).toBeDefined();
  });
  
  it('should generate an idea relevant to specified industry', async () => {
    // Arrange
    const params: IdeaGenerationParams = {
      industry: 'Healthcare',
      constraints: []
    };
    
    // Act
    const result = await aiService.generateIdea(params);
    
    // Assert
    expect(result.title.toLowerCase()).toMatch(/health|care|medical|patient|doctor|clinic|hospital|wellness/);
    expect(result.description.toLowerCase()).toMatch(/health|care|medical|patient|doctor|clinic|hospital|wellness/);
  });
  
  it('should handle API errors gracefully', async () => {
    // Arrange
    jest.spyOn(openaiClient, 'createCompletion').mockRejectedValue(new Error('API error'));
    
    // Act & Assert
    await expect(aiService.generateIdea({
      industry: 'Technology',
      constraints: []
    })).rejects.toThrow('Failed to generate idea');
  });
});
```

### AI Quality Metrics

We evaluate AI-generated content based on:

1. **Completeness**: All required fields are present
2. **Relevance**: Content is relevant to the input parameters
3. **Coherence**: Content is logical and well-structured
4. **Creativity**: Content demonstrates originality and creativity
5. **Diversity**: Generated variations are sufficiently different from each other

## Test Data Management

### Test Data Strategy

1. **Isolated Test Data**: Each test should use its own data to avoid test interdependence
2. **Data Factories**: Use factory functions to generate test data
3. **Seeded Randomness**: Use seeded random generators for deterministic but varied data
4. **Clean Up**: Tests should clean up after themselves

### Test Data Factories

```typescript
// Example test data factory
export const createTestCanvas = (overrides: Partial<CanvasCreationParams> = {}): Canvas => {
  return new Canvas({
    name: overrides.name || `Test Canvas ${faker.random.alphaNumeric(8)}`,
    description: overrides.description || faker.lorem.paragraph(),
    tags: overrides.tags || [faker.random.word(), faker.random.word()],
    ...overrides
  }, overrides.ownerId || 'test-user-id');
};

export const createTestIdea = (overrides: Partial<IdeaCreationParams> = {}): Idea => {
  return new Idea({
    title: overrides.title || `Test Idea ${faker.random.alphaNumeric(8)}`,
    description: overrides.description || faker.lorem.paragraph(),
    problemStatement: overrides.problemStatement || faker.lorem.sentence(),
    targetAudience: overrides.targetAudience || faker.commerce.department(),
    uniqueValue: overrides.uniqueValue || faker.company.catchPhrase(),
    ...overrides
  }, overrides.canvasId || 'test-canvas-id');
};
```

## Test Automation

### Continuous Integration

Our CI pipeline runs tests at different stages:

```mermaid
graph TD
    Commit[Commit] --> Lint[Lint]
    Lint --> TypeCheck[Type Check]
    TypeCheck --> UnitTest[Unit Tests]
    UnitTest --> Build[Build]
    Build --> IntegrationTest[Integration Tests]
    IntegrationTest --> E2ETest[E2E Tests]
    E2ETest --> Deploy[Deploy]
    
    style Commit fill:#f9f,stroke:#333,stroke-width:1px
    style Deploy fill:#9f9,stroke:#333,stroke-width:1px
```

### Testing Tools

1. **Jest**: Unit and integration testing
2. **React Testing Library**: Component testing
3. **Cypress**: End-to-end testing
4. **K6**: Performance testing
5. **Storybook**: Component development and visual testing

## Test Coverage

We aim for the following test coverage targets:

| Test Type | Coverage Target |
|-----------|----------------|
| Unit Tests | 80% |
| Integration Tests | 60% |
| E2E Tests | Critical paths |

## Testing Guidelines

1. **Write Tests First**: Follow test-driven development (TDD) where appropriate
2. **Keep Tests Simple**: Each test should verify one thing
3. **Organize Tests**: Group tests logically by feature or component
4. **Mock Judiciously**: Mock external dependencies, not internal ones
5. **Test Edge Cases**: Include tests for boundary conditions and error cases
6. **Keep Tests Fast**: Optimize tests for speed to enable rapid feedback

## Conclusion

This testing strategy ensures comprehensive test coverage for the Idea Playground rebuild, focusing on quality, reliability, and performance. By following this approach, we can deliver a robust system that meets user needs and maintains high standards of quality.

================
File: docs/idea-playground/AI_FEATURES_FIX.md
================
# Idea Playground AI Features Fix

## Issue Identified

The AI features in the Idea Playground stopped working due to a change in the feature flags configuration in `src/lib/store.ts`. Specifically:

- The `useRealAI` flag was set to `false` (previously `true`)
- This prevented the AI generation services from properly functioning

## Fix Applied

We've made the following changes to restore AI functionality:

1. Updated the default value of `useRealAI` to `true` in `src/lib/store.ts`
2. Kept `useMockAI` enabled as a fallback option
3. Created helper scripts to reset the LLM services

## How the Fix Works

The `sequential-generation.service.ts` is designed to only return genuine AI-generated suggestions without mock data. When `useRealAI` is disabled, it skips real AI generation but also doesn't generate any mock suggestions by design, resulting in no data being displayed.

By enabling `useRealAI` again, the generation service properly attempts to create AI-generated ideas, restoring the expected functionality.

## How to Use

If you experience issues with AI generation after a refresh or app restart, you can:

1. Run the simple fix script to apply the feature flag changes in memory:
   ```
   node scripts/fix-ai-feature-flags.js
   ```

2. Alternatively, you can check and modify the feature flags directly in the Settings UI:
   - Go to "Admin" → "Feature Flags"
   - Ensure "Use Real AI" is enabled

## Long-Term Solution

The permanent fix has been applied to the default values in the store, so this setting will persist across application restarts. The behavior has been tested and verified to work consistently.

The modular architecture still supports toggling between real AI and mock mode, but defaults to using real AI for better user experience.

================
File: docs/idea-playground/AI_STREAMING_IMPROVEMENTS.md
================
# AI Streaming Improvements for Idea Suggestions

## Overview

The suggestion generation system for the Idea Playground has been enhanced to properly display streaming suggestions in real-time. This document explains the changes made to fix issues with suggestions not populating the screen as they arrive.

## Issues Addressed

1. **Streaming Display Issues**: Suggestions were not populating the screen in real-time as they were generated
2. **Timeout Configuration**: Previous timeout settings were too short for complex AI operations
3. **Fallback Mechanism**: When AI failed to generate a suggestion, no fallback was used, leaving gaps
4. **Position Tracking**: Suggestions were not properly positioned in the UI

## Technical Changes Made

### Sequential Generation Service

The main fix was in `src/lib/services/idea-playground/ai/sequential-generation.service.ts`:

1. **Increased Timeout**: Extended from 30 seconds to 60 seconds per suggestion
2. **Position Tracking**: Added tracking of filled positions to maintain order
3. **Immediate Callback**: Modified to call the progress callback immediately when each suggestion is ready
4. **Fallback Logic**: Added automatic fallback to mock suggestions when real AI generation fails

```typescript
// Key improvements in the sequential generation code:
try {
  // Generate AI suggestion with increased timeout (60s)
  const suggestion = await Promise.race([
    ideaPathway1AIService.generateSingleSuggestion(idea, userId, i),
    timeoutPromise
  ]);
  
  // Add to results and notify immediately with correct position
  allSuggestions.push(suggestion);
  filledPositions.add(i);
  progressCallback(suggestion, false, i, boundedCount);
  
} catch (error) {
  // Generate a mock suggestion as fallback for this position
  const mockSuggestion = ideaPathway1AIService.generateMockSuggestionsPublic(idea, 1)[0];
  
  if (!filledPositions.has(i)) {
    allSuggestions.push(mockSuggestion);
    filledPositions.add(i);
    
    // Notify with the mock suggestion at the correct position
    progressCallback(mockSuggestion, true, i, boundedCount);
  }
}
```

### Suggestions UI Screen

No changes were needed to the UI components themselves since:

1. The `SuggestionsScreen` component already had the necessary display logic
2. The UI correctly displays suggestions as they come in
3. The implementation distinguishes between real AI and mock suggestions

## Testing

A test script has been added to verify the improvements:

```bash
node scripts/test-streaming-suggestions.js
```

This script confirms:
- Suggestions appear as they are generated
- All positions get filled, either with real or fallback suggestions
- Position tracking works correctly
- Mock suggestions are properly labeled

## Expected Behavior

With these improvements:

1. Suggestions will populate the screen in real-time as each one completes
2. If at least one real AI suggestion is generated, mock suggestions will only be used to fill any gaps
3. The UI will show an accurate progress indicator
4. The experience will feel more responsive and interactive

## Conclusion

The updated implementation provides a more responsive, fault-tolerant experience when generating AI suggestions. The longer timeout, proper positional tracking, and intelligent fallback system ensure that users see suggestions as they arrive and don't need to wait for all suggestions to be generated before seeing results.

================
File: docs/idea-playground/ARCHITECTURE.md
================
# Idea Playground Rebuild: Architecture

## System Architecture Overview

The Idea Playground rebuild adopts a comprehensive domain-driven architecture with clear separation of concerns, robust error handling, and optimal performance characteristics. The following diagram illustrates the high-level system architecture:

```mermaid
graph TD
    Client[Client Application] --> APIGateway[API Gateway]
    
    subgraph "Application Layer"
        APIGateway --> ApplicationServices[Application Services]
        ApplicationServices --> CommandHandlers[Command Handlers]
        ApplicationServices --> QueryHandlers[Query Handlers]
        CommandHandlers --> DomainServices[Domain Services]
        QueryHandlers --> ReadModels[Read Models]
    end
    
    subgraph "Domain Layer"
        DomainServices --> Entities[Domain Entities]
        DomainServices --> ValueObjects[Value Objects]
        DomainServices --> DomainEvents[Domain Events]
        Entities --> AggregateRoots[Aggregate Roots]
    end
    
    subgraph "Infrastructure Layer"
        DomainServices --> Repositories[Repositories]
        ApplicationServices --> AIService[AI Service]
        AIService --> AIO[AI Orchestrator]
        AIO --> AIProviders[AI Providers]
        Repositories --> Database[(Database)]
        ApplicationServices --> EventBus[Event Bus]
        ApplicationServices --> Caching[Caching]
        ApplicationServices --> Logging[Logging]
    end
```

## Component Architecture

### Client Application

The client application is a React-based single-page application (SPA) with the following key architectural components:

1. **State Management**: Uses Zustand for global state management with slice pattern for modularity
2. **API Communication**: React Query for data fetching, caching, and synchronization
3. **UI Components**: Composite component pattern with composition for flexibility
4. **Routing**: React Router with typed routes for navigation and deep linking
5. **Error Boundary**: Hierarchical error boundaries for isolated error handling

```mermaid
graph TD
    ReactApp[React Application] --> GlobalState[Global State - Zustand]
    ReactApp --> Routing[React Router]
    ReactApp --> ApiLayer[API Layer - React Query]
    ReactApp --> UIComponents[UI Components]
    
    UIComponents --> CanvasComponents[Canvas Components]
    UIComponents --> IdeaComponents[Idea Components]
    UIComponents --> VariationComponents[Variation Components]
    UIComponents --> RefinementComponents[Refinement Components]
    UIComponents --> SharedComponents[Shared Components]
    
    GlobalState --> UserSlice[User Slice]
    GlobalState --> CanvasSlice[Canvas Slice]
    GlobalState --> IdeaSlice[Idea Slice]
    GlobalState --> UISlice[UI Slice]
    
    ApiLayer --> ApiHooks[Custom API Hooks]
    ApiLayer --> QueryCache[Query Cache]
    ApiLayer --> MutationHandlers[Mutation Handlers]
```

### Application Layer

The application layer coordinates use cases by delegating to domain services:

1. **Command Handlers**: Process user actions that modify state
2. **Query Handlers**: Process read operations optimized for specific views
3. **Application Services**: Orchestrate use cases spanning multiple domain services
4. **DTOs**: Data Transfer Objects for API communication

```mermaid
graph TD
    AppServices[Application Services] --> CanvasAppService[Canvas Application Service]
    AppServices --> IdeaAppService[Idea Application Service]
    AppServices --> VariationAppService[Variation Application Service]
    AppServices --> MergeAppService[Merge Application Service]
    AppServices --> RefinementAppService[Refinement Application Service]
    
    CanvasAppService --> CanvasCommands[Canvas Commands]
    CanvasAppService --> CanvasQueries[Canvas Queries]
    
    IdeaAppService --> IdeaCommands[Idea Commands]
    IdeaAppService --> IdeaQueries[Idea Queries]
    
    VariationAppService --> VariationCommands[Variation Commands]
    VariationAppService --> VariationQueries[Variation Queries]
    
    MergeAppService --> MergeCommands[Merge Commands]
    MergeAppService --> MergeQueries[Merge Queries]
    
    RefinementAppService --> RefinementCommands[Refinement Commands]
    RefinementAppService --> RefinementQueries[Refinement Queries]
```

### Domain Layer

The domain layer contains the business logic and domain rules:

1. **Aggregate Roots**: Canvas, Idea, Variation, MergedIdea (enforce invariants)
2. **Entities**: Objects with identity and lifecycle
3. **Value Objects**: Immutable objects without identity
4. **Domain Events**: Represent significant state changes in the domain
5. **Domain Services**: Implement domain logic that doesn't fit naturally in entities

```mermaid
graph TD
    subgraph "Domain Model"
        Canvas[Canvas] --> Idea[Idea]
        Idea --> Variation[Variation]
        Variation --> MergedIdea[Merged Idea]
        
        Canvas --> User[User]
        Canvas --> Collaborator[Collaborator]
        Idea --> Comment[Comment]
        Idea --> Component[Component]
        Variation --> SWOT[SWOT Analysis]
        MergedIdea --> Source[Source Relationship]
    end
    
    subgraph "Domain Services"
        CanvasService[Canvas Service]
        IdeaService[Idea Service]
        VariationService[Variation Service]
        MergeService[Merge Service]
        RefinementService[Refinement Service]
        ValidationService[Validation Service]
    end
    
    subgraph "Domain Events"
        CanvasCreated[Canvas Created]
        IdeaGenerated[Idea Generated]
        VariationsCreated[Variations Created]
        IdeasMerged[Ideas Merged]
        IdeaRefined[Idea Refined]
    end
    
    Canvas -.-> CanvasCreated
    Idea -.-> IdeaGenerated
    Variation -.-> VariationsCreated
    MergedIdea -.-> IdeasMerged
```

### Infrastructure Layer

The infrastructure layer provides technical capabilities:

1. **Repositories**: Data access abstractions for domain objects
2. **AI Service**: API client and orchestration for AI operations
3. **Event Bus**: Publish-subscribe mechanism for domain events
4. **Caching**: Performance optimization for frequently accessed data
5. **Logging**: Comprehensive logging for monitoring and debugging

#### AI Service Architecture

The AI Service is a critical component with specialized architecture:

```mermaid
graph TD
    AIService[AI Service] --> AIOrchestrator[AI Orchestrator]
    AIOrchestrator --> PromptManager[Prompt Manager]
    AIOrchestrator --> ResponseProcessor[Response Processor]
    AIOrchestrator --> ErrorHandler[Error Handler]
    
    PromptManager --> PromptTemplates[(Prompt Templates)]
    PromptManager --> ContextEnrichment[Context Enrichment]
    PromptManager --> TokenOptimization[Token Optimization]
    
    ResponseProcessor --> SchemaValidation[Schema Validation]
    ResponseProcessor --> ResponseParsing[Response Parsing]
    ResponseProcessor --> ResponseTransformation[Response Transformation]
    
    ErrorHandler --> RetryMechanism[Retry Mechanism]
    ErrorHandler --> Fallbacks[Fallbacks]
    ErrorHandler --> ErrorReporting[Error Reporting]
    
    AIOrchestrator --> OpenAI[OpenAI Provider]
    AIOrchestrator -.-> AlternateProvider[Alternate Provider]
```

## Database Schema

```mermaid
erDiagram
    users ||--o{ canvases : owns
    users ||--o{ collaborators : is
    canvases ||--o{ ideas : contains
    canvases ||--o{ collaborators : has
    ideas ||--o{ variations : generates
    variations ||--o{ merged_ideas : contributes-to
    ideas ||--o{ components : has
    ideas ||--o{ comments : receives
    
    users {
        uuid id PK
        string email
        string name
        timestamp created_at
        timestamp updated_at
    }
    
    canvases {
        uuid id PK
        uuid owner_id FK
        string name
        string description
        string[] tags
        timestamp created_at
        timestamp updated_at
    }
    
    collaborators {
        uuid canvas_id FK
        uuid user_id FK
        string permission_level
        timestamp created_at
    }
    
    ideas {
        uuid id PK
        uuid canvas_id FK
        string title
        text description
        text problem_statement
        text target_audience
        text unique_value
        jsonb metadata
        timestamp created_at
        timestamp updated_at
    }
    
    variations {
        uuid id PK
        uuid parent_idea_id FK
        string title
        text description
        text problem_statement
        text target_audience
        text unique_value
        jsonb swot
        boolean is_selected
        timestamp created_at
        timestamp updated_at
    }
    
    merged_ideas {
        uuid id PK
        uuid canvas_id FK
        string title
        text description
        text problem_statement
        text target_audience
        text unique_value
        jsonb swot
        boolean is_selected
        timestamp created_at
        timestamp updated_at
    }
    
    merge_sources {
        uuid merged_idea_id FK
        uuid variation_id FK
        timestamp created_at
    }
    
    components {
        uuid id PK
        uuid idea_id FK
        string type
        string name
        jsonb content
        timestamp created_at
        timestamp updated_at
    }
    
    comments {
        uuid id PK
        uuid idea_id FK
        uuid user_id FK
        text content
        timestamp created_at
        timestamp updated_at
    }
```

## Integration Points

### 1. Authentication System

The Idea Playground integrates with the existing authentication system:

- JWT token-based authentication
- Role-based access control
- Session management

### 2. OpenAI Integration

Integration with OpenAI's API for AI-powered features:

- GPT-4 for idea generation and refinement
- Function calling for structured output
- Input validation and output parsing
- Context management for coherent AI operations

### 3. Event System

Event-based architecture for loose coupling:

- Domain events published through event bus
- Subscribers consume events for side effects
- Cross-domain communication via events

### 4. Logging System

Integration with the comprehensive logging system:

- Structured logging for all operations
- Error tracking and reporting
- Performance monitoring
- AI interaction logging

## Data Flow

### 1. Idea Generation Flow

```mermaid
sequenceDiagram
    participant User
    participant UI as UI Components
    participant AppService as Application Service
    participant DomainService as Domain Service
    participant AI as AI Service
    participant DB as Database
    
    User->>UI: Request idea generation
    UI->>AppService: GenerateIdea command
    AppService->>DomainService: Create idea draft
    DomainService->>DB: Save draft
    DB-->>DomainService: Draft saved
    DomainService-->>AppService: Return draft
    AppService->>AI: Request idea generation
    
    AI->>AI: Process request
    AI-->>AppService: Generated content
    
    AppService->>DomainService: Update idea
    DomainService->>DB: Save complete idea
    DB-->>DomainService: Idea saved
    DomainService->>DomainService: Publish IdeaCreated event
    DomainService-->>AppService: Return complete idea
    AppService-->>UI: Return result
    UI-->>User: Display generated idea
```

### 2. Variation Generation Flow

```mermaid
sequenceDiagram
    participant User
    participant UI as UI Components
    participant AppService as Application Service
    participant VarService as Variation Service
    participant IdeaService as Idea Service
    participant AI as AI Service
    participant DB as Database
    
    User->>UI: Request variations
    UI->>AppService: GenerateVariations command
    AppService->>IdeaService: Get original idea
    IdeaService->>DB: Fetch idea
    DB-->>IdeaService: Idea data
    IdeaService-->>AppService: Return idea
    
    AppService->>AI: Generate variations
    AI->>AI: Process request
    AI-->>AppService: Generated variations
    
    loop For Each Variation
        AppService->>VarService: Create variation
        VarService->>DB: Save variation
        DB-->>VarService: Variation saved
    end
    
    VarService->>VarService: Publish VariationsCreated event
    VarService-->>AppService: Return all variations
    AppService-->>UI: Return results
    UI-->>User: Display variations
```

## Performance Considerations

1. **Lazy Loading**: Components and data loaded on-demand
2. **Caching Strategy**:
   - Short-term cache for AI results
   - Long-term cache for reference data
   - Cache invalidation on updates
3. **Pagination**:
   - Client-side pagination for small datasets
   - Server-side pagination for large datasets
4. **Connection Pooling**:
   - Database connection pooling
   - API client pooling for external services

## Security Architecture

1. **Authentication**:
   - JWT-based authentication
   - Short-lived access tokens
   - Refresh token rotation

2. **Authorization**:
   - Role-based access control
   - Object-level permissions
   - Permission checks in application services

3. **Data Protection**:
   - Data encryption at rest
   - Secure API communication (HTTPS)
   - Input validation and output sanitization

4. **API Security**:
   - Rate limiting
   - Request validation
   - CSRF protection

## Error Handling Strategy

1. **Domain Errors**:
   - Rich domain error types
   - Error classification by severity
   - Error translation for UI

2. **Infrastructure Errors**:
   - Retry mechanisms for transient failures
   - Circuit breakers for external services
   - Fallback mechanisms for degraded operation

3. **UI Error Handling**:
   - Component-level error boundaries
   - Contextual error messages
   - Recovery options for users

## Monitoring and Observability

1. **Performance Metrics**:
   - Response times
   - Error rates
   - Resource utilization

2. **Business Metrics**:
   - User engagement
   - Feature usage
   - Conversion rates

3. **AI Operation Metrics**:
   - Token usage
   - Response quality
   - Error classification

================
File: docs/idea-playground/BACKEND_SERVICES.md
================
# Backend Services Implementation

This document provides implementation details for the backend services that power the Idea Playground system.

## Service Architecture

The backend services follow a modular architecture that makes it easy to swap components, add new features, and maintain the codebase. 

```mermaid
graph TD
    subgraph "Core Services"
        IdS[IdeaService]
        IPS[IdeaProtectionService]
        ITS[InteractionTrackingService]
        ISS[IdeaSimilarityService]
    end
    
    subgraph "LLM Adapters"
        OAA[OpenAIAdapter]
        HFA[HuggingFaceAdapter]
        LocalA[LocalModelAdapter]
    end
    
    subgraph "Context Providers"
        BCP[BaseContextProvider]
        CCP[CompanyContextProvider]
        ACP[AbstractionContextProvider]
    end
    
    subgraph "Orchestration"
        Orch[LLMOrchestrator]
    end
    
    IdS --> IPS
    IdS --> ITS
    IdS --> ISS
    
    ISS --> Orch
    
    Orch --> OAA
    Orch --> HFA
    Orch --> LocalA
    
    Orch --> BCP
    Orch --> CCP
    Orch --> ACP
```

## Core Services

### 1. Idea Protection Service

The IdeaProtectionService is responsible for managing protection levels for ideas and controlling access based on these protection settings.

```typescript
// src/lib/services/idea-playground/idea-protection.service.ts
import { supabase } from '../../supabase';
import { IdeaProtectionLevel, IdeaProtectionSettings } from '../../types/idea-protection.types';
import { IdeaPlaygroundIdea } from '../../types/idea-playground.types';

export class IdeaProtectionService {
  /**
   * Set protection level for an idea
   */
  async setProtectionLevel(
    ideaId: string,
    protectionLevel: IdeaProtectionLevel,
    userId: string,
    options: Partial<IdeaProtectionSettings> = {}
  ): Promise<void> {
    try {
      // First check if protection settings already exist
      const { data: existingSettings } = await supabase
        .from('idea_protection_settings')
        .select('id')
        .eq('idea_id', ideaId)
        .single();
        
      const excludeFromTraining = 
        protectionLevel === IdeaProtectionLevel.PROTECTED || 
        protectionLevel === IdeaProtectionLevel.PROPRIETARY;
        
      const excludeFromSimilaritySearch = 
        protectionLevel === IdeaProtectionLevel.PROPRIETARY;
      
      // Default settings based on protection level
      const settings: Partial<IdeaProtectionSettings> = {
        ideaId,
        protectionLevel,
        ownerUserId: userId,
        excludeFromTraining,
        excludeFromSimilaritySearch,
        obfuscationLevel: protectionLevel === IdeaProtectionLevel.PROPRIETARY ? 'complete' : 'none',
        ...options
      };
      
      if (existingSettings) {
        // Update existing settings
        await supabase
          .from('idea_protection_settings')
          .update(settings)
          .eq('id', existingSettings.id);
      } else {
        // Create new settings
        await supabase
          .from('idea_protection_settings')
          .insert([settings]);
      }
      
      // Also update the idea table for quick access
      await supabase
        .from('ideas')
        .update({ 
          protection_level: protectionLevel,
          owner_user_id: userId
        })
        .eq('id', ideaId);
        
    } catch (error) {
      console.error('Error setting protection level:', error);
      throw error;
    }
  }
  
  /**
   * Mark an idea as implemented/proprietary
   */
  async markAsImplemented(
    ideaId: string, 
    userId: string,
    implementation: {
      companyName: string;
      implementationDate?: string;
      legalStatus?: 'pending' | 'patent-filed' | 'trademark-registered' | 'incorporated';
      teamAccess?: string[];
    }
  ): Promise<void> {
    try {
      await this.setProtectionLevel(
        ideaId,
        IdeaProtectionLevel.PROPRIETARY,
        userId,
        {
          companyName: implementation.companyName,
          implementationDate: implementation.implementationDate || new Date().toISOString(),
          legalStatus: implementation.legalStatus || 'pending',
          teamAccess: implementation.teamAccess || [],
          excludeFromTraining: true,
          excludeFromSimilaritySearch: true,
          obfuscationLevel: 'complete'
        }
      );
      
      // Log the implementation
      await supabase
        .from('idea_implementations')
        .insert([{
          idea_id: ideaId,
          user_id: userId,
          company_name: implementation.companyName,
          implemented_at: implementation.implementationDate || new Date().toISOString(),
          legal_status: implementation.legalStatus || 'pending'
        }]);
        
    } catch (error) {
      console.error('Error marking idea as implemented:', error);
      throw error;
    }
  }
  
  /**
   * Check if current user has access to an idea
   */
  async hasAccessToIdea(
    ideaId: string,
    userId: string,
    requiredAccess: 'view' | 'edit' | 'admin' = 'view'
  ): Promise<boolean> {
    try {
      // First get the protection settings
      const { data: protection } = await supabase
        .from('idea_protection_settings')
        .select('*')
        .eq('idea_id', ideaId)
        .single();
        
      // If no protection settings or public, anyone can view
      if (!protection || protection.protection_level === IdeaProtectionLevel.PUBLIC) {
        return requiredAccess === 'view';
      }
      
      // Owner has all access
      if (protection.owner_user_id === userId) {
        return true;
      }
      
      // Team access gives view and edit
      if (protection.team_access && protection.team_access.includes(userId)) {
        return requiredAccess !== 'admin';
      }
      
      // Viewer access only gives view
      if (protection.viewer_access && protection.viewer_access.includes(userId)) {
        return requiredAccess === 'view';
      }
      
      // No access
      return false;
    } catch (error) {
      console.error('Error checking idea access:', error);
      return false;
    }
  }
  
  /**
   * Filter ideas for training exclusion
   */
  async filterIdeasForTraining(ideas: IdeaPlaygroundIdea[]): Promise<IdeaPlaygroundIdea[]> {
    try {
      if (!ideas || ideas.length === 0) return [];
      
      // Get IDs of all ideas to check
      const ideaIds = ideas.map(idea => idea.id);
      
      // Find settings for all ideas
      const { data: protectionSettings } = await supabase
        .from('idea_protection_settings')
        .select('idea_id, exclude_from_training')
        .in('idea_id', ideaIds)
        .eq('exclude_from_training', true);
        
      if (!protectionSettings || protectionSettings.length === 0) {
        return ideas; // No exclusions
      }
      
      // Build a set of excluded idea IDs for fast lookup
      const excludedIdeaIds = new Set(
        protectionSettings.map(setting => setting.idea_id)
      );
      
      // Filter out excluded ideas
      return ideas.filter(idea => !excludedIdeaIds.has(idea.id));
    } catch (error) {
      console.error('Error filtering ideas for training:', error);
      // In case of error, be conservative and exclude all
      return [];
    }
  }
  
  /**
   * Get protection settings for an idea
   */
  async getProtectionSettings(ideaId: string): Promise<IdeaProtectionSettings | null> {
    try {
      const { data, error } = await supabase
        .from('idea_protection_settings')
        .select('*')
        .eq('idea_id', ideaId)
        .single();
        
      if (error) return null;
      return data;
    } catch (error) {
      console.error('Error getting protection settings:', error);
      return null;
    }
  }
}
```

### 2. Interaction Tracking Service

The InteractionTrackingService records all user interactions with ideas, creating a comprehensive feedback collection mechanism for AI model training.

```typescript
// src/lib/services/idea-playground/interaction-tracking.service.ts
import { supabase } from '../../supabase';
import { IdeaInteraction, IdeaInteractionType } from '../../types/idea-feedback.types';
import { IdeaPlaygroundIdea } from '../../types/idea-playground.types';

export class IdeaInteractionTrackingService {
  /**
   * Track an interaction with an idea
   */
  async trackInteraction(interaction: Omit<IdeaInteraction, 'id' | 'timestamp'>): Promise<string> {
    try {
      const timestamp = new Date().toISOString();
      
      const { data, error } = await supabase
        .from('idea_interactions')
        .insert([{
          ...interaction,
          timestamp
        }])
        .select('id');
        
      if (error) throw error;
      
      // Update idea training status based on interaction
      await this.updateIdeaTrainingStatusFromInteraction(
        interaction.ideaId,
        interaction.interactionType
      );
      
      return data?.[0]?.id;
    } catch (error) {
      console.error('Error tracking idea interaction:', error);
      throw error;
    }
  }
  
  /**
   * Track idea editing
   */
  async trackIdeaEdit(
    userId: string,
    previousVersion: IdeaPlaygroundIdea,
    newVersion: IdeaPlaygroundIdea
  ): Promise<string> {
    return this.trackInteraction({
      ideaId: previousVersion.id,
      userId,
      interactionType: IdeaInteractionType.EDITED,
      previousVersion,
      newVersion
    });
  }
  
  /**
   * Track idea merge
   */
  async trackIdeaMerge(
    userId: string,
    resultingIdeaId: string,
    mergedIdeaIds: string[]
  ): Promise<string> {
    return this.trackInteraction({
      ideaId: resultingIdeaId,
      userId,
      interactionType: IdeaInteractionType.MERGED,
      mergedWithIdeaIds: mergedIdeaIds
    });
  }
  
  /**
   * Track explicit rating
   */
  async trackRating(
    userId: string,
    ideaId: string,
    rating: number,
    comment?: string
  ): Promise<string> {
    // Determine rating type
    let interactionType: IdeaInteractionType;
    if (rating >= 4) {
      interactionType = IdeaInteractionType.POSITIVE_RATING;
    } else if (rating <= 2) {
      interactionType = IdeaInteractionType.NEGATIVE_RATING;
    } else {
      interactionType = IdeaInteractionType.NEUTRAL_RATING;
    }
    
    return this.trackInteraction({
      ideaId,
      userId,
      interactionType,
      ratingValue: rating,
      comment
    });
  }
  
  /**
   * Track idea dismissal
   */
  async trackDismissal(
    userId: string,
    ideaId: string,
    reason?: string
  ): Promise<string> {
    return this.trackInteraction({
      ideaId,
      userId,
      interactionType: IdeaInteractionType.DISMISSED,
      reason
    });
  }
  
  /**
   * Update idea training status based on interaction
   */
  private async updateIdeaTrainingStatusFromInteraction(
    ideaId: string,
    interactionType: IdeaInteractionType
  ): Promise<void> {
    try {
      let trainingStatus: string | null = null;
      
      // Map interaction types to training statuses
      switch (interactionType) {
        case IdeaInteractionType.POSITIVE_RATING:
        case IdeaInteractionType.SAVED:
        case IdeaInteractionType.IMPLEMENTED:
        case IdeaInteractionType.SHARED:
        case IdeaInteractionType.EXPORTED:
          trainingStatus = 'positive_example';
          break;
          
        case IdeaInteractionType.NEGATIVE_RATING:
        case IdeaInteractionType.DISMISSED:
        case IdeaInteractionType.LOW_ENGAGEMENT:
          trainingStatus = 'negative_example';
          break;
          
        case IdeaInteractionType.EDITED:
        case IdeaInteractionType.REFINED:
          trainingStatus = 'modified_example';
          break;
          
        case IdeaInteractionType.MERGED:
          trainingStatus = 'merged_example';
          break;
          
        // For neutral interactions, no change in training status
        default:
          return;
      }
      
      if (trainingStatus) {
        await supabase
          .from('ideas')
          .update({ training_status: trainingStatus })
          .eq('id', ideaId);
      }
    } catch (error) {
      console.error('Error updating idea training status:', error);
      // Non-critical error, just log it
    }
  }
  
  /**
   * Get all interactions for an idea
   */
  async getIdeaInteractions(ideaId: string): Promise<IdeaInteraction[]> {
    try {
      const { data, error } = await supabase
        .from('idea_interactions')
        .select('*')
        .eq('ideaId', ideaId)
        .order('timestamp', { ascending: true });
        
      if (error) throw error;
      
      return data || [];
    } catch (error) {
      console.error('Error getting idea interactions:', error);
      return [];
    }
  }
}
```

### 3. Idea Similarity Service

The IdeaSimilarityService handles generating embeddings for ideas and comparing them to prevent IP leakage and detect similarities between ideas.

```typescript
// src/lib/services/idea-playground/idea-similarity.service.ts
import { supabase } from '../../supabase';
import { IdeaPlaygroundIdea } from '../../types/idea-playground.types';
import { LLMOrchestrator } from './llm/orchestrator';

export class IdeaSimilarityService {
  private orchestrator: LLMOrchestrator;
  
  constructor(orchestrator: LLMOrchestrator) {
    this.orchestrator = orchestrator;
  }
  
  /**
   * Generate embeddings for an idea
   */
  async generateIdeaEmbedding(idea: IdeaPlaygroundIdea): Promise<number[]> {
    // Create a combined text representation of the idea
    const ideaText = `
      ${idea.title}
      ${idea.description}
      ${idea.problem_statement}
      ${idea.solution_concept}
      ${typeof idea.target_audience === 'string' ? idea.target_audience : idea.target_audience.join(', ')}
      ${idea.unique_value}
      ${idea.business_model}
    `;
    
    // Generate embeddings
    return await this.orchestrator.generateEmbedding(ideaText);
  }
  
  /**
   * Store embeddings for an idea
   */
  async storeIdeaEmbedding(ideaId: string, embedding: number[]): Promise<void> {
    try {
      await supabase
        .from('ideas')
        .update({ embedding })
        .eq('id', ideaId);
    } catch (error) {
      console.error('Error storing idea embedding:', error);
      throw error;
    }
  }
  
  /**
   * Check if a new idea is too similar to proprietary ideas
   */
  async checkForProprietarySimilarity(
    newIdea: IdeaPlaygroundIdea, 
    similarityThreshold: number = 0.85
  ): Promise<{ tooSimilar: boolean; similarityScore?: number; similarToIdeaId?: string }> {
    try {
      // Get embedding for the new idea
      const newIdeaEmbedding = await this.generateIdeaEmbedding(newIdea);
      
      // Get proprietary ideas
      const { data: proprietaryIdeas } = await supabase
        .from('ideas')
        .select('id, title, description, problem_statement, solution_concept, target_audience, unique_value, business_model, embedding')
        .eq('protection_level', 'proprietary')
        .not('embedding', 'is', null);
        
      if (!proprietaryIdeas || proprietaryIdeas.length === 0) {
        return { tooSimilar: false };
      }
      
      // Calculate similarity with each proprietary idea
      let highestSimilarity = 0;
      let mostSimilarIdeaId = '';
      
      for (const proprietaryIdea of proprietaryIdeas) {
        if (!proprietaryIdea.embedding) continue;
        
        // Calculate cosine similarity
        const similarity = this.calculateCosineSimilarity(
          newIdeaEmbedding,
          proprietaryIdea.embedding
        );
        
        if (similarity > highestSimilarity) {
          highestSimilarity = similarity;
          mostSimilarIdeaId = proprietaryIdea.id;
        }
      }
      
      // Check if similarity is above threshold
      if (highestSimilarity > similarityThreshold) {
        return {
          tooSimilar: true,
          similarityScore: highestSimilarity,
          similarToIdeaId: mostSimilarIdeaId
        };
      }
      
      return { tooSimilar: false };
    } catch (error) {
      console.error('Error checking proprietary similarity:', error);
      // In case of error, be conservative
      return { tooSimilar: true };
    }
  }
  
  /**
   * Calculate cosine similarity between two embedding vectors
   */
  private calculateCosineSimilarity(vecA: number[], vecB: number[]): number {
    if (vecA.length !== vecB.length) {
      throw new Error('Vectors must have the same length');
    }
    
    let dotProduct = 0;
    let normA = 0;
    let normB = 0;
    
    for (let i = 0; i < vecA.length; i++) {
      dotProduct += vecA[i] * vecB[i];
      normA += vecA[i] * vecA[i];
      normB += vecB[i] * vecB[i];
    }
    
    normA = Math.sqrt(normA);
    normB = Math.sqrt(normB);
    
    // Prevent division by zero
    if (normA === 0 || normB === 0) {
      return 0;
    }
    
    return dotProduct / (normA * normB);
  }
  
  /**
   * Find similar ideas for inspiration
   */
  async findSimilarIdeas(
    idea: IdeaPlaygroundIdea,
    limit: number = 5,
    excludeProprietaryIdeas: boolean = true
  ): Promise<{ id: string; title: string; similarity: number }[]> {
    try {
      // Generate embedding for the input idea
      const embedding = await this.generateIdeaEmbedding(idea);
      
      // Query the database for similar ideas
      // This uses a raw SQL query with the pgvector extension
      // The actual implementation would depend on how vector similarity is implemented in your Supabase instance
      
      let query = supabase
        .from('ideas')
        .select('id, title, embedding');
      
      if (excludeProprietaryIdeas) {
        query = query.neq('protection_level', 'proprietary');
      }
      
      query = query.not('embedding', 'is', null);
      
      const { data: potentialMatches, error } = await query;
      
      if (error || !potentialMatches) {
        console.error('Error finding similar ideas:', error);
        return [];
      }
      
      // Calculate similarities
      const similarIdeas = potentialMatches
        .map(match => ({
          id: match.id,
          title: match.title,
          similarity: this.calculateCosineSimilarity(embedding, match.embedding)
        }))
        .filter(match => match.id !== idea.id) // Exclude the original idea
        .sort((a, b) => b.similarity - a.similarity) // Sort by similarity (descending)
        .slice(0, limit); // Take only the top matches
      
      return similarIdeas;
    } catch (error) {
      console.error('Error finding similar ideas:', error);
      return [];
    }
  }
}
```

## LLM Adapter Framework

The LLM Adapter framework provides a flexible abstraction layer for different AI models, making it easy to switch between providers or use multiple models.

### Orchestrator

```typescript
// src/lib/services/idea-playground/llm/orchestrator.ts
import { OpenAIAdapter } from './adapters/openai.adapter';
import { HuggingFaceAdapter } from './adapters/huggingface.adapter';
import { LocalModelAdapter } from './adapters/local-model.adapter';
import { LLMAdapter } from './adapters/interface';
import { ContextManager } from './context/context-manager';
import { BaseContextProvider } from './context/base-context.provider';
import { CompanyContextProvider } from './context/company-context.provider';
import { AbstractionContextProvider } from './context/abstraction-context.provider';

export class LLMOrchestrator {
  private adapters: Record<string, LLMAdapter>;
  private contextManager: ContextManager;
  private activeAdapter: string;
  
  constructor() {
    // Initialize adapters
    this.adapters = {
      openai: new OpenAIAdapter(),
      huggingface: new HuggingFaceAdapter(),
      local: new LocalModelAdapter()
    };
    
    // Set default adapter
    this.activeAdapter = 'openai';
    
    // Initialize context providers
    this.contextManager = new ContextManager([
      new BaseContextProvider(),
      new CompanyContextProvider(),
      new AbstractionContextProvider()
    ]);
  }
  
  /**
   * Switch the active LLM adapter
   */
  setActiveAdapter(adapterId: string): void {
    if (!this.adapters[adapterId]) {
      throw new Error(`Adapter ${adapterId} not found`);
    }
    this.activeAdapter = adapterId;
  }
  
  /**
   * Get the active adapter
   */
  getActiveAdapter(): LLMAdapter {
    return this.adapters[this.activeAdapter];
  }
  
  /**
   * Generate idea using the active LLM adapter
   */
  async generateIdea(prompt: string, options?: any): Promise<any> {
    const adapter = this.getActiveAdapter();
    
    // Enhance prompt with context
    const enhancedPrompt = await this.contextManager.enhancePrompt(prompt);
    
    // Generate idea
    return adapter.generateText(enhancedPrompt, options);
  }
  
  /**
   * Generate embedding for text using the active LLM adapter
   */
  async generateEmbedding(text: string): Promise<number[]> {
    return this.getActiveAdapter().generateEmbedding(text);
  }
  
  /**
   * Generate variations of an idea
   */
  async generateIdeaVariations(originalIdea: any, count: number = 3): Promise<any[]> {
    const adapter = this.getActiveAdapter();
    return adapter.generateIdeaVariations(originalIdea, count);
  }
  
  /**
   * Refine an idea based on feedback
   */
  async refineIdea(idea: any, feedback: string): Promise<any> {
    const adapter = this.getActiveAdapter();
    return adapter.refineIdea(idea, feedback);
  }
}
```

### LLM Adapter Interface

```typescript
// src/lib/services/idea-playground/llm/adapters/interface.ts
export interface LLMAdapter {
  /**
   * Generate text from a prompt
   */
  generateText(prompt: string, options?: any): Promise<string>;
  
  /**
   * Generate embeddings for a text
   */
  generateEmbedding(text: string): Promise<number[]>;
  
  /**
   * Generate variations of an idea
   */
  generateIdeaVariations(originalIdea: any, count: number): Promise<any[]>;
  
  /**
   * Refine an idea based on feedback
   */
  refineIdea(idea: any, feedback: string): Promise<any>;
  
  /**
   * Check if the adapter is available
   */
  isAvailable(): Promise<boolean>;
}
```

### OpenAI Adapter Implementation

```typescript
// src/lib/services/idea-playground/llm/adapters/openai.adapter.ts
import { Configuration, OpenAIApi } from 'openai';
import { LLMAdapter } from './interface';

export class OpenAIAdapter implements LLMAdapter {
  private openai: OpenAIApi;
  private embeddingModel: string = 'text-embedding-ada-002';
  private textModel: string = 'gpt-4';
  
  constructor() {
    const configuration = new Configuration({
      apiKey: process.env.OPENAI_API_KEY
    });
    this.openai = new OpenAIApi(configuration);
  }
  
  async generateText(prompt: string, options: any = {}): Promise<string> {
    try {
      const response = await this.openai.createChatCompletion({
        model: options.model || this.textModel,
        messages: [{ role: 'user', content: prompt }],
        temperature: options.temperature ?? 0.7,
        max_tokens: options.maxTokens ?? 1000
      });
      
      return response.data.choices[0].message?.content || '';
    } catch (error) {
      console.error('OpenAI text generation error:', error);
      throw error;
    }
  }
  
  async generateEmbedding(text: string): Promise<number[]> {
    try {
      const response = await this.openai.createEmbedding({
        model: this.embeddingModel,
        input: text
      });
      
      return response.data.data[0].embedding;
    } catch (error) {
      console.error('OpenAI embedding generation error:', error);
      throw error;
    }
  }
  
  async generateIdeaVariations(originalIdea: any, count: number): Promise<any[]> {
    const prompt = `
      Generate ${count} variations of the following business idea:
      
      Title: ${originalIdea.title}
      Description: ${originalIdea.description}
      Problem Statement: ${originalIdea.problem_statement}
      
      Each variation should have a different angle, target market, or implementation approach.
      Provide the result as a JSON array of ideas, each with the following properties:
      title, description, problem_statement, solution_concept, target_audience, unique_value.
    `;
    
    try {
      const jsonResponse = await this.generateText(prompt, {
        temperature: 0.8
      });
      
      // Parse JSON response
      return JSON.parse(jsonResponse);
    } catch (error) {
      console.error('Error generating idea variations:', error);
      return [];
    }
  }
  
  async refineIdea(idea: any, feedback: string): Promise<any> {
    const prompt = `
      Refine the following business idea based on this feedback:
      
      ORIGINAL IDEA:
      Title: ${idea.title}
      Description: ${idea.description}
      Problem Statement: ${idea.problem_statement}
      Solution Concept: ${idea.solution_concept}
      Target Audience: ${typeof idea.target_audience === 'string' ? idea.target_audience : idea.target_audience.join(', ')}
      Unique Value: ${idea.unique_value}
      
      FEEDBACK:
      ${feedback}
      
      Provide the refined idea as a JSON object with the same properties as the original.
      Make sure to address all the feedback points.
    `;
    
    try {
      const jsonResponse = await this.generateText(prompt, {
        temperature: 0.5
      });
      
      // Parse JSON response
      return JSON.parse(jsonResponse);
    } catch (error) {
      console.error('Error refining idea:', error);
      return idea; // Return original idea on error
    }
  }
  
  async isAvailable(): Promise<boolean> {
    try {
      // Simple test call to check if API is available
      await this.openai.createEmbedding({
        model: this.embeddingModel,
        input: 'test'
      });
      return true;
    } catch (error) {
      console.error('OpenAI API is not available:', error);
      return false;
    }
  }
}
```

## Context Providers

The Context Provider framework enriches prompts with relevant information before they're sent to LLM adapters.

### Context Provider Interface

```typescript
// src/lib/services/idea-playground/llm/context/interface.ts
export interface ContextProvider {
  /**
   * Enhance a prompt with additional context
   */
  enhancePrompt(prompt: string): Promise<string>;
  
  /**
   * Get the name of the context provider
   */
  getName(): string;
  
  /**
   * Check if this context provider should be used for a given prompt
   */
  shouldEnhance(prompt: string): boolean;
}
```

### Context Manager

```typescript
// src/lib/services/idea-playground/llm/context/context-manager.ts
import { ContextProvider } from './interface';

export class ContextManager {
  private providers: ContextProvider[];
  
  constructor(providers: ContextProvider[] = []) {
    this.providers = providers;
  }
  
  /**
   * Add a context provider
   */
  addProvider(provider: ContextProvider): void {
    this.providers.push(provider);
  }
  
  /**
   * Remove a context provider by name
   */
  removeProvider(name: string): void {
    this.providers = this.providers.filter(p => p.getName() !== name);
  }
  
  /**
   * Enhance a prompt with all applicable context providers
   */
  async enhancePrompt(prompt: string): Promise<string> {
    let enhancedPrompt = prompt;
    
    for (const provider of this.providers) {
      if (provider.shouldEnhance(prompt)) {
        enhancedPrompt = await provider.enhancePrompt(enhancedPrompt);
      }
    }
    
    return enhancedPrompt;
  }
}
```

## Service Factory

The ServiceFactory provides a centralized way to access all the services needed for the Idea Playground functionality.

```typescript
// src/lib/services/idea-playground/index.ts
import {

================
File: docs/idea-playground/CONTINUOUS_LEARNING_WITH_IP_PROTECTION.md
================
# Comprehensive Implementation Guide: AI-Powered Business Idea Generation with Continuous Learning and IP Protection

## Table of Contents
1. [System Overview](#1-system-overview)
2. [Database Schema Implementation](#2-database-schema-implementation)
3. [Backend Services Implementation](#3-backend-services-implementation)
4. [Frontend Components](#4-frontend-components)
5. [Hugging Face Setup](#5-hugging-face-setup)
6. [Training Pipeline Implementation](#6-training-pipeline-implementation)
7. [IP Protection Layer](#7-ip-protection-layer)
8. [Testing Framework](#8-testing-framework)
9. [Deployment Strategy](#9-deployment-strategy)
10. [Monitoring and Maintenance](#10-monitoring-and-maintenance)

---

## 1. System Overview

### 1.1 Architecture Diagram

```mermaid
graph TD
    subgraph "Frontend Application"
        UI[UI Components]
        Hooks[React Hooks]
        Contexts[React Contexts]
    end
    
    subgraph "Service Layer"
        IdS[Idea Services]
        FbS[Feedback Services]
        IPS[IP Protection Service]
        TraS[Training Services]
    end
    
    subgraph "LLM Adapters"
        OAA[OpenAI Adapter]
        HFA[Hugging Face Adapter]
        LocalA[Local Model Adapter]
    end
    
    subgraph "Database"
        Ideas[Ideas Table]
        Feedback[Feedback Table]
        Protection[Protection Settings]
        Interactions[User Interactions]
    end
    
    subgraph "Training Infrastructure"
        HFS[Hugging Face Spaces]
        TrainP[Training Pipeline]
        ModelR[Model Repository]
    end
    
    UI --> Hooks
    Hooks --> Contexts
    Contexts --> IdS
    Contexts --> FbS
    Contexts --> IPS
    IdS --> OAA
    IdS --> HFA
    IdS --> LocalA
    FbS --> Feedback
    IPS --> Protection
    IdS --> Ideas
    TraS --> Ideas
    TraS --> Feedback
    TraS --> Interactions
    TraS --> TrainP
    TrainP --> ModelR
    ModelR --> HFS
    HFS --> LocalA
```

### 1.2 High-Level System Components

1. **Idea Generation System**
   - Core idea generation capabilities
   - Modular architecture with swappable LLM adapters
   - Abstraction layers for different AI models

2. **Feedback Collection Framework**
   - Comprehensive tracking of all user interactions
   - Classification of feedback into multiple categories
   - Storage of metadata for training signal extraction

3. **IP Protection Layer**
   - Protection levels for different idea stages
   - Owner/team access controls
   - Training exclusion mechanisms
   - Similarity detection to prevent IP leakage

4. **Training Pipeline**
   - Data collection and preparation
   - Filtering and obfuscation of sensitive data
   - Training supervision with continuous learning
   - Model versioning and deployment

5. **Hugging Face Integration**
   - Model hosting and serving
   - Training infrastructure
   - Model registry and versioning

---

## 2. Database Schema Implementation

### 2.1 Supabase Migration Files

#### 2.1.1 Core Schema Modifications

Create the following migration file to add the necessary tables and relationships for capturing all types of feedback and IP protection:

```sql
-- supabase/migrations/20250318000000_idea_playground_extended.sql

-- First, ensure the UUID extension is available
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";

-- Add new columns to the ideas table for training and protection
ALTER TABLE ideas ADD COLUMN IF NOT EXISTS training_status TEXT;
ALTER TABLE ideas ADD COLUMN IF NOT EXISTS protection_level TEXT DEFAULT 'public';
ALTER TABLE ideas ADD COLUMN IF NOT EXISTS owner_user_id UUID REFERENCES auth.users(id);
ALTER TABLE ideas ADD COLUMN IF NOT EXISTS last_trained_at TIMESTAMP WITH TIME ZONE;
ALTER TABLE ideas ADD COLUMN IF NOT EXISTS embedding VECTOR(1536); -- For similarity search

-- Create a comprehensive interaction tracking table
CREATE TABLE IF NOT EXISTS idea_interactions (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  idea_id UUID REFERENCES ideas(id) ON DELETE CASCADE,
  user_id UUID REFERENCES auth.users(id),
  interaction_type TEXT NOT NULL, -- 'positive_rating', 'negative_rating', 'saved', 'dismissed', 'edited', 'merged', etc.
  timestamp TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  
  -- For ratings
  rating_value INTEGER,
  
  -- For modifications
  previous_version JSONB,
  new_version JSONB,
  
  -- For merges
  merged_with_idea_ids UUID[],
  
  -- For dismissals or explicit feedback
  reason TEXT,
  comment TEXT
);

-- Add indexes for faster queries
CREATE INDEX IF NOT EXISTS idea_interactions_idea_id_idx ON idea_interactions(idea_id);
CREATE INDEX IF NOT EXISTS idea_interactions_user_id_idx ON idea_interactions(user_id);
CREATE INDEX IF NOT EXISTS idea_interactions_type_idx ON idea_interactions(interaction_type);

-- Protection settings table
CREATE TABLE IF NOT EXISTS idea_protection_settings (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  idea_id UUID REFERENCES ideas(id) ON DELETE CASCADE,
  protection_level TEXT NOT NULL DEFAULT 'public',
  updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  
  -- Proprietary metadata
  company_name TEXT,
  implementation_date TIMESTAMP WITH TIME ZONE,
  legal_status TEXT,
  
  -- Access controls
  owner_user_id UUID NOT NULL REFERENCES auth.users(id),
  team_access UUID[] DEFAULT '{}',
  viewer_access UUID[] DEFAULT '{}',
  
  -- Training controls
  exclude_from_training BOOLEAN DEFAULT FALSE,
  exclude_from_similarity_search BOOLEAN DEFAULT FALSE,
  obfuscation_level TEXT DEFAULT 'none',
  
  UNIQUE(idea_id)
);

-- Training data tracking table
CREATE TABLE IF NOT EXISTS training_sessions (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  started_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  completed_at TIMESTAMP WITH TIME ZONE,
  model_version TEXT,
  training_data_count INTEGER,
  validation_data_count INTEGER,
  metrics JSONB,
  status TEXT DEFAULT 'pending'
);

-- Ideas used in training
CREATE TABLE IF NOT EXISTS training_ideas (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  training_session_id UUID REFERENCES training_sessions(id) ON DELETE CASCADE,
  idea_id UUID REFERENCES ideas(id) ON DELETE CASCADE,
  included_as TEXT, -- 'positive', 'negative', 'modified', 'merged'
  obfuscated BOOLEAN DEFAULT FALSE
);

-- Create the implementation tracking table
CREATE TABLE IF NOT EXISTS idea_implementations (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  idea_id UUID REFERENCES ideas(id) ON DELETE CASCADE,
  user_id UUID REFERENCES auth.users(id),
  company_name TEXT NOT NULL,
  implemented_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  legal_status TEXT DEFAULT 'pending',
  
  -- Additional details
  implementation_details JSONB,
  business_model TEXT,
  market_segment TEXT,
  funding_status TEXT
);
```

#### 2.1.2 Row-Level Security Policies

Create another migration file to implement the necessary RLS policies:

```sql
-- supabase/migrations/20250318000100_idea_playground_security.sql

-- Enable Row Level Security on all tables
ALTER TABLE idea_interactions ENABLE ROW LEVEL SECURITY;
ALTER TABLE idea_protection_settings ENABLE ROW LEVEL SECURITY;
ALTER TABLE training_sessions ENABLE ROW LEVEL SECURITY;
ALTER TABLE training_ideas ENABLE ROW LEVEL SECURITY;
ALTER TABLE idea_implementations ENABLE ROW LEVEL SECURITY;

-- Idea Interactions - Basic policies
CREATE POLICY "Users can see their own interactions"
  ON idea_interactions
  FOR SELECT
  USING (auth.uid() = user_id);
  
CREATE POLICY "Users can create their own interactions"
  ON idea_interactions
  FOR INSERT
  WITH CHECK (auth.uid() = user_id);
  
CREATE POLICY "Users can update their own interactions"
  ON idea_interactions
  FOR UPDATE
  USING (auth.uid() = user_id);

-- Protection Settings - Access control
CREATE POLICY "Owners can view protection settings"
  ON idea_protection_settings
  FOR SELECT
  USING (auth.uid() = owner_user_id);
  
CREATE POLICY "Team members can view protection settings"
  ON idea_protection_settings
  FOR SELECT
  USING (auth.uid() = ANY(team_access));
  
CREATE POLICY "Owners can edit protection settings"
  ON idea_protection_settings
  FOR ALL
  USING (auth.uid() = owner_user_id);

-- Idea Implementations - IP protection
CREATE POLICY "Only owners can view implementations"
  ON idea_implementations
  FOR SELECT
  USING (auth.uid() = user_id);
  
CREATE POLICY "Only owners can create implementations"
  ON idea_implementations
  FOR INSERT
  WITH CHECK (auth.uid() = user_id);

-- Training data access - Admin only
CREATE POLICY "Only admins can view training sessions"
  ON training_sessions
  FOR SELECT
  USING (auth.uid() IN (SELECT user_id FROM user_roles WHERE role = 'admin'));
  
CREATE POLICY "Only admins can manage training sessions"
  ON training_sessions
  FOR ALL
  USING (auth.uid() IN (SELECT user_id FROM user_roles WHERE role = 'admin'));
```

### 2.2 TypeScript Type Definitions

Create type definitions for all the new database entities:

```typescript
// src/lib/types/idea-protection.types.ts
export enum IdeaProtectionLevel {
  PUBLIC = 'public',           // Default, can be shared and used for training
  PRIVATE = 'private',         // Only visible to creator, but can be used for anonymized training
  PROTECTED = 'protected',     // Private and excluded from training entirely
  PROPRIETARY = 'proprietary'  // Implemented as product/company with legal protection
}

export enum LegalStatus {
  PENDING = 'pending',
  PATENT_FILED = 'patent-filed',
  TRADEMARK_REGISTERED = 'trademark-registered',
  INCORPORATED = 'incorporated'
}

export interface IdeaProtectionSettings {
  id: string;
  ideaId: string;
  protectionLevel: IdeaProtectionLevel;
  updatedAt: string;
  
  // For proprietary ideas
  companyName?: string;
  implementationDate?: string;
  legalStatus?: LegalStatus;
  
  // Access controls
  ownerUserId: string;
  teamAccess?: string[];
  viewerAccess?: string[];
  
  // For training exclusion
  excludeFromTraining: boolean;
  excludeFromSimilaritySearch: boolean;
  obfuscationLevel: 'none' | 'basic' | 'complete';
}

export interface IdeaImplementation {
  id: string;
  ideaId: string;
  userId: string;
  companyName: string;
  implementedAt: string;
  legalStatus: LegalStatus;
  implementationDetails?: any;
  businessModel?: string;
  marketSegment?: string;
  fundingStatus?: string;
}
```

```typescript
// src/lib/types/idea-feedback.types.ts
export enum IdeaInteractionType {
  // Direct feedback
  POSITIVE_RATING = 'positive_rating',
  NEGATIVE_RATING = 'negative_rating',
  NEUTRAL_RATING = 'neutral_rating',
  
  // Actions
  SAVED = 'saved',
  DISMISSED = 'dismissed',
  
  // Modifications
  EDITED = 'edited',
  MERGED = 'merged',
  REFINED = 'refined',
  
  // Business actions
  IMPLEMENTED = 'implemented',
  SHARED = 'shared',
  EXPORTED = 'exported',
  
  // Engagement metrics
  VIEWED_DETAILS = 'viewed_details',
  HIGH_ENGAGEMENT = 'high_engagement',
  LOW_ENGAGEMENT = 'low_engagement'
}

export interface IdeaInteraction {
  id: string;
  ideaId: string;
  userId: string;
  interactionType: IdeaInteractionType;
  timestamp: string;
  
  // For ratings
  ratingValue?: number;
  
  // For modifications
  previousVersion?: any;
  newVersion?: any;
  
  // For merges
  mergedWithIdeaIds?: string[];
  
  // For dismissals or explicit feedback
  reason?: string;
  comment?: string;
}
```

```typescript
// src/lib/types/training.types.ts
export enum TrainingStatus {
  PENDING = 'pending',
  PROCESSING = 'processing',
  COMPLETED = 'completed',
  FAILED = 'failed'
}

export interface TrainingSession {
  id: string;
  startedAt: string;
  completedAt?: string;
  modelVersion: string;
  trainingDataCount: number;
  validationDataCount: number;
  metrics?: any;
  status: TrainingStatus;
}

export interface TrainingIdea {
  id: string;
  trainingSessionId: string;
  ideaId: string;
  includedAs: 'positive' | 'negative' | 'modified' | 'merged';
  obfuscated: boolean;
}
```

---

## 3. Backend Services Implementation

### 3.1 IP Protection Service

```typescript
// src/lib/services/idea-playground/idea-protection.service.ts
import { supabase } from '../../supabase';
import { IdeaProtectionLevel, IdeaProtectionSettings } from '../../types/idea-protection.types';
import { IdeaPlaygroundIdea } from '../../types/idea-playground.types';

export class IdeaProtectionService {
  /**
   * Set protection level for an idea
   */
  async setProtectionLevel(
    ideaId: string,
    protectionLevel: IdeaProtectionLevel,
    userId: string,
    options: Partial<IdeaProtectionSettings> = {}
  ): Promise<void> {
    try {
      // First check if protection settings already exist
      const { data: existingSettings } = await supabase
        .from('idea_protection_settings')
        .select('id')
        .eq('idea_id', ideaId)
        .single();
        
      const excludeFromTraining = 
        protectionLevel === IdeaProtectionLevel.PROTECTED || 
        protectionLevel === IdeaProtectionLevel.PROPRIETARY;
        
      const excludeFromSimilaritySearch = 
        protectionLevel === IdeaProtectionLevel.PROPRIETARY;
      
      // Default settings based on protection level
      const settings: Partial<IdeaProtectionSettings> = {
        ideaId,
        protectionLevel,
        ownerUserId: userId,
        excludeFromTraining,
        excludeFromSimilaritySearch,
        obfuscationLevel: protectionLevel === IdeaProtectionLevel.PROPRIETARY ? 'complete' : 'none',
        ...options
      };
      
      if (existingSettings) {
        // Update existing settings
        await supabase
          .from('idea_protection_settings')
          .update(settings)
          .eq('id', existingSettings.id);
      } else {
        // Create new settings
        await supabase
          .from('idea_protection_settings')
          .insert([settings]);
      }
      
      // Also update the idea table for quick access
      await supabase
        .from('ideas')
        .update({ 
          protection_level: protectionLevel,
          owner_user_id: userId
        })
        .eq('id', ideaId);
        
    } catch (error) {
      console.error('Error setting protection level:', error);
      throw error;
    }
  }
  
  /**
   * Mark an idea as implemented/proprietary
   */
  async markAsImplemented(
    ideaId: string, 
    userId: string,
    implementation: {
      companyName: string;
      implementationDate?: string;
      legalStatus?: 'pending' | 'patent-filed' | 'trademark-registered' | 'incorporated';
      teamAccess?: string[];
    }
  ): Promise<void> {
    try {
      await this.setProtectionLevel(
        ideaId,
        IdeaProtectionLevel.PROPRIETARY,
        userId,
        {
          companyName: implementation.companyName,
          implementationDate: implementation.implementationDate || new Date().toISOString(),
          legalStatus: implementation.legalStatus || 'pending',
          teamAccess: implementation.teamAccess || [],
          excludeFromTraining: true,
          excludeFromSimilaritySearch: true,
          obfuscationLevel: 'complete'
        }
      );
      
      // Log the implementation
      await supabase
        .from('idea_implementations')
        .insert([{
          idea_id: ideaId,
          user_id: userId,
          company_name: implementation.companyName,
          implemented_at: implementation.implementationDate || new Date().toISOString(),
          legal_status: implementation.legalStatus || 'pending'
        }]);
        
    } catch (error) {
      console.error('Error marking idea as implemented:', error);
      throw error;
    }
  }
  
  /**
   * Check if current user has access to an idea
   */
  async hasAccessToIdea(
    ideaId: string,
    userId: string,
    requiredAccess: 'view' | 'edit' | 'admin' = 'view'
  ): Promise<boolean> {
    try {
      // First get the protection settings
      const { data: protection } = await supabase
        .from('idea_protection_settings')
        .select('*')
        .eq('idea_id', ideaId)
        .single();
        
      // If no protection settings or public, anyone can view
      if (!protection || protection.protection_level === IdeaProtectionLevel.PUBLIC) {
        return requiredAccess === 'view';
      }
      
      // Owner has all access
      if (protection.owner_user_id === userId) {
        return true;
      }
      
      // Team access gives view and edit
      if (protection.team_access && protection.team_access.includes(userId)) {
        return requiredAccess !== 'admin';
      }
      
      // Viewer access only gives view
      if (protection.viewer_access && protection.viewer_access.includes(userId)) {
        return requiredAccess === 'view';
      }
      
      // No access
      return false;
    } catch (error) {
      console.error('Error checking idea access:', error);
      return false;
    }
  }
  
  /**
   * Filter ideas for training exclusion
   */
  async filterIdeasForTraining(ideas: IdeaPlaygroundIdea[]): Promise<IdeaPlaygroundIdea[]> {
    try {
      if (!ideas || ideas.length === 0) return [];
      
      // Get IDs of all ideas to check
      const ideaIds = ideas.map(idea => idea.id);
      
      // Find settings for all ideas
      const { data: protectionSettings } = await supabase
        .from('idea_protection_settings')
        .select('idea_id, exclude_from_training')
        .in('idea_id', ideaIds)
        .eq('exclude_from_training', true);
        
      if (!protectionSettings || protectionSettings.length === 0) {
        return ideas; // No exclusions
      }
      
      // Build a set of excluded idea IDs for fast lookup
      const excludedIdeaIds = new Set(
        protectionSettings.map(setting => setting.idea_id)
      );
      
      // Filter out excluded ideas
      return ideas.filter(idea => !excludedIdeaIds.has(idea.id));
    } catch (error) {
      console.error('Error filtering ideas for training:', error);
      // In case of error, be conservative and exclude all
      return [];
    }
  }
  
  /**
   * Get protection settings for an idea
   */
  async getProtectionSettings(ideaId: string): Promise<IdeaProtectionSettings | null> {
    try {
      const { data, error } = await supabase
        .from('idea_protection_settings')
        .select('*')
        .eq('idea_id', ideaId)
        .single();
        
      if (error) return null;
      return data;
    } catch (error) {
      console.error('Error getting protection settings:', error);
      return null;
    }
  }
}
```

### 3.2 Interaction Tracking Service

```typescript
// src/lib/services/idea-playground/interaction-tracking.service.ts
import { supabase } from '../../supabase';
import { IdeaInteraction, IdeaInteractionType } from '../../types/idea-feedback.types';
import { IdeaPlaygroundIdea } from '../../types/idea-playground.types';

export class IdeaInteractionTrackingService {
  /**
   * Track an interaction with an idea
   */
  async trackInteraction(interaction: Omit<IdeaInteraction, 'id' | 'timestamp'>): Promise<string> {
    try {
      const timestamp = new Date().toISOString();
      
      const { data, error } = await supabase
        .from('idea_interactions')
        .insert([{
          ...interaction,
          timestamp
        }])
        .select('id');
        
      if (error) throw error;
      
      // Update idea training status based on interaction
      await this.updateIdeaTrainingStatusFromInteraction(
        interaction.ideaId,
        interaction.interactionType
      );
      
      return data?.[0]?.id;
    } catch (error) {
      console.error('Error tracking idea interaction:', error);
      throw error;
    }
  }
  
  /**
   * Track idea editing
   */
  async trackIdeaEdit(
    userId: string,
    previousVersion: IdeaPlaygroundIdea,
    newVersion: IdeaPlaygroundIdea
  ): Promise<string> {
    return this.trackInteraction({
      ideaId: previousVersion.id,
      userId,
      interactionType: IdeaInteractionType.EDITED,
      previousVersion,
      newVersion
    });
  }
  
  /**
   * Track idea merge
   */
  async trackIdeaMerge(
    userId: string,
    resultingIdeaId: string,
    mergedIdeaIds: string[]
  ): Promise<string> {
    return this.trackInteraction({
      ideaId: resultingIdeaId,
      userId,
      interactionType: IdeaInteractionType.MERGED,
      mergedWithIdeaIds: mergedIdeaIds
    });
  }
  
  /**
   * Track explicit rating
   */
  async trackRating(
    userId: string,
    ideaId: string,
    rating: number,
    comment?: string
  ): Promise<string> {
    // Determine rating type
    let interactionType: IdeaInteractionType;
    if (rating >= 4) {
      interactionType = IdeaInteractionType.POSITIVE_RATING;
    } else if (rating <= 2) {
      interactionType = IdeaInteractionType.NEGATIVE_RATING;
    } else {
      interactionType = IdeaInteractionType.NEUTRAL_RATING;
    }
    
    return this.trackInteraction({
      ideaId,
      userId,
      interactionType,
      ratingValue: rating,
      comment
    });
  }
  
  /**
   * Track idea dismissal
   */
  async trackDismissal(
    userId: string,
    ideaId: string,
    reason?: string
  ): Promise<string> {
    return this.trackInteraction({
      ideaId,
      userId,
      interactionType: IdeaInteractionType.DISMISSED,
      reason
    });
  }
  
  /**
   * Update idea training status based on interaction
   */
  private async updateIdeaTrainingStatusFromInteraction(
    ideaId: string,
    interactionType: IdeaInteractionType
  ): Promise<void> {
    try {
      let trainingStatus: string | null = null;
      
      // Map interaction types to training statuses
      switch (interactionType) {
        case IdeaInteractionType.POSITIVE_RATING:
        case IdeaInteractionType.SAVED:
        case IdeaInteractionType.IMPLEMENTED:
        case IdeaInteractionType.SHARED:
        case IdeaInteractionType.EXPORTED:
          trainingStatus = 'positive_example';
          break;
          
        case IdeaInteractionType.NEGATIVE_RATING:
        case IdeaInteractionType.DISMISSED:
        case IdeaInteractionType.LOW_ENGAGEMENT:
          trainingStatus = 'negative_example';
          break;
          
        case IdeaInteractionType.EDITED:
        case IdeaInteractionType.REFINED:
          trainingStatus = 'modified_example';
          break;
          
        case IdeaInteractionType.MERGED:
          trainingStatus = 'merged_example';
          break;
          
        // For neutral interactions, no change in training status
        default:
          return;
      }
      
      if (trainingStatus) {
        await supabase
          .from('ideas')
          .update({ training_status: trainingStatus })
          .eq('id', ideaId);
      }
    } catch (error) {
      console.error('Error updating idea training status:', error);
      // Non-critical error, just log it
    }
  }
  
  /**
   * Get all interactions for an idea
   */
  async getIdeaInteractions(ideaId: string): Promise<IdeaInteraction[]> {
    try {
      const { data, error } = await supabase
        .from('idea_interactions')
        .select('*')
        .eq('ideaId', ideaId)
        .order('timestamp', { ascending: true });
        
      if (error) throw error;
      
      return data || [];
    } catch (error) {
      console.error('Error getting idea interactions:', error);
      return [];
    }
  }
}
```

### 3.3 Idea Similarity Service

```typescript
// src/lib/services/idea-playground/idea-similarity.service.ts
import { supabase } from '../../supabase';
import { IdeaPlaygroundIdea } from '../../types/idea-playground.types';
import { LLMOrchestrator } from './llm/orchestrator';

export class IdeaSimilarityService {
  private orchestrator: LLMOrchestrator;
  
  constructor(orchestrator: LLMOrchestrator) {
    this.orchestrator = orchestrator;
  }
  
  /**
   * Generate embeddings for an idea
   */
  async generateIdeaEmbedding(idea: IdeaPlaygroundIdea): Promise<number[]> {
    // Create a combined text representation of the idea
    const ideaText = `
      ${idea.title}
      ${idea.description}
      ${idea.problem_statement}
      ${idea.solution_concept}
      ${typeof idea.target_audience === 'string' ? idea.target_audience : idea.target_audience.join(', ')}
      ${idea.unique_value}
      ${idea.business_model}
    `;
    
    // Generate embeddings
    return await this.orchestrator.generateEmbedding(ideaText);
  }
  
  /**
   * Store embeddings for an idea
   */
  async storeIdeaEmbedding(ideaId: string, embedding: number[]): Promise<void> {
    try {
      await supabase
        .from('ideas')
        .update({ embedding })
        .eq('id', ideaId);
    } catch (error) {
      console.error('Error storing idea embedding:', error);
      throw error;
    }
  }
  
  /**
   * Check if a new idea is too similar to proprietary ideas
   */
  async checkForProprietarySimilarity(
    newIdea: IdeaPlaygroundIdea, 
    similarityThreshold: number = 0.85
  ): Promise<{ tooSimilar: boolean; similarityScore?: number; similarToIdeaId?: string }> {
    try {
      // Get embedding for the new idea
      const newIdeaEmbedding = await this.generateIdeaEmbedding(newIdea);
      
      // Get proprietary ideas
      const { data: proprietaryIdeas } = await supabase
        .from('ideas')
        .select('id, title, description, problem_statement, solution_concept, target_audience, unique_value, business_model, embedding')
        .eq('protection_level', 'proprietary')
        .not('embedding', 'is', null);
        
      if (!proprietaryIdeas || proprietaryIdeas.length === 0) {
        return { tooSimilar: false };
      }
      
      // Calculate similarity with each proprietary idea
      let highestSimilarity = 0;
      let mostSimilarIdeaId = '';
      
      for (const proprietaryIdea of proprietaryIdeas) {
        if (!proprietaryIdea.embedding) continue;
        
        // Calculate cosine similarity
        const similarity = this.calculateCosineSimilarity(
          newIdeaEmbedding,
          proprietaryIdea.embedding
        );
        
        if (similarity > highestSimilarity) {
          highestSimilarity = similarity;
          mostSimilarIdeaId = proprietaryIdea.id;
        }
      }
      
      // Check if similarity is above threshold
      if (highestSimilarity > similarityThreshold) {
        return {
          tooSimilar: true,
          similarityScore: highestSimilarity,
          similarToIdeaId: mostSimilarIdeaId
        };
      }
      
      return { tooSimilar: false };
    } catch (error) {
      console.error('Error checking proprietary similarity:', error);
      // In case of error, be conservative
      return { tooSimilar: true };
    }
  }
  
  /**
   * Calculate cosine similarity between two embedding vectors
   */
  private calculateCosineSimilarity(vecA: number[], vecB: number[]): number {
    if (vecA.length !== vecB.length) {
      throw new Error('Vectors must have the same length');
    }
    
    let dotProduct = 0;
    let normA = 0;
    let normB = 0;
    
    for (let i = 0; i < vecA.length; i++) {
      dotProduct += vecA[i] * vecB[i];
      normA += vecA[i] * vecA[i];
      normB += vecB[i] * vecB[i];
    }
    
    normA = Math.sqrt(normA);
    normB = Math.sqrt(normB);
    
    // Prevent division by zero
    if (normA === 0

================
File: docs/idea-playground/DATA_MIGRATION.md
================
# Data Migration Strategy

This document outlines the approach for migrating existing idea playground data to the new architecture. While backward compatibility is not a strict requirement due to the complete rebuild, we want to preserve valuable user data wherever possible.

## Migration Overview

```mermaid
graph LR
    OldDB[(Old Database)] --> Extractor[Data Extractor]
    Extractor --> TransformationEngine[Transformation Engine]
    TransformationEngine --> Validator[Data Validator]
    Validator --> Loader[Data Loader]
    Loader --> NewDB[(New Database)]
    Validator -.-> ErrorHandler[Error Handler]
    ErrorHandler -.-> ManualReview[Manual Review Queue]
```

The migration process follows an ETL (Extract, Transform, Load) approach with additional validation and error handling.

## 1. Data Analysis

### Current Schema Analysis

The current schema has several issues that need to be addressed:

| Issue | Description | Impact |
|-------|-------------|--------|
| Inconsistent relationships | Related data is not properly linked | Hard to trace relationships between entities |
| Denormalized data | Repeated data across multiple tables | Redundancy and update anomalies |
| Missing constraints | Lack of proper foreign keys and validation | Data integrity issues |
| Schema drift | Schema has evolved without proper migrations | Inconsistent data structures |
| Unstructured JSON | Complex data stored as unstructured JSON | Difficult to query and validate |

### Data Quality Assessment

A preliminary analysis of the data shows the following quality issues:

1. **Missing Data**: Approximately 15% of records have missing required fields
2. **Invalid References**: About 8% of records reference non-existent parent entities
3. **Duplicate Records**: Around 5% of records are duplicates
4. **Malformed JSON**: Nearly 20% of JSON data is malformed or does not match expected schema
5. **Orphaned Records**: About 10% of records are orphaned (parent records deleted)

## 2. Migration Strategy

### Phase 1: Extract

The extraction phase involves pulling data from the current database:

```typescript
interface DataExtractor {
  extractCanvases(): Promise<RawCanvasData[]>;
  extractIdeas(): Promise<RawIdeaData[]>;
  extractVariations(): Promise<RawVariationData[]>;
  extractMergedIdeas(): Promise<RawMergedIdeaData[]>;
  extractComponents(): Promise<RawComponentData[]>;
  extractCollaborators(): Promise<RawCollaboratorData[]>;
}

class SupabaseDataExtractor implements DataExtractor {
  constructor(private readonly supabaseClient: SupabaseClient) {}
  
  async extractCanvases(): Promise<RawCanvasData[]> {
    const { data, error } = await this.supabaseClient
      .from('canvases')
      .select('*');
      
    if (error) {
      throw new Error(`Failed to extract canvases: ${error.message}`);
    }
    
    return data;
  }
  
  // Similar implementations for other entity types
}
```

### Phase 2: Transform

The transformation phase converts the old data structure to the new domain model:

```typescript
interface DataTransformer {
  transformCanvas(rawCanvas: RawCanvasData): Canvas;
  transformIdea(rawIdea: RawIdeaData): Idea;
  transformVariation(rawVariation: RawVariationData): Variation;
  transformMergedIdea(rawMergedIdea: RawMergedIdeaData): MergedIdea;
  transformComponent(rawComponent: RawComponentData): Component;
  transformCollaborator(rawCollaborator: RawCollaboratorData): Collaborator;
}

class DomainModelTransformer implements DataTransformer {
  transformCanvas(rawCanvas: RawCanvasData): Canvas {
    // Mapping old schema to new domain model
    try {
      return new Canvas({
        id: rawCanvas.id,
        name: rawCanvas.name || 'Untitled Canvas',
        description: rawCanvas.description || '',
        tags: this.extractTags(rawCanvas.metadata)
      }, rawCanvas.user_id);
    } catch (error) {
      throw new TransformationError(
        'canvas',
        rawCanvas.id,
        error.message,
        rawCanvas
      );
    }
  }
  
  // Similar implementations for other entity types
  
  private extractTags(metadata: any): string[] {
    if (!metadata || !metadata.tags) {
      return [];
    }
    
    try {
      if (Array.isArray(metadata.tags)) {
        return metadata.tags;
      }
      
      if (typeof metadata.tags === 'string') {
        return metadata.tags.split(',').map(t => t.trim());
      }
      
      return [];
    } catch {
      return [];
    }
  }
}
```

### Phase 3: Validate

The validation phase ensures the transformed data meets the requirements of the new system:

```typescript
interface DataValidator {
  validateCanvas(canvas: Canvas): ValidationResult;
  validateIdea(idea: Idea): ValidationResult;
  validateVariation(variation: Variation): ValidationResult;
  validateMergedIdea(mergedIdea: MergedIdea): ValidationResult;
  validateComponent(component: Component): ValidationResult;
  validateCollaborator(collaborator: Collaborator): ValidationResult;
}

class DomainModelValidator implements DataValidator {
  validateCanvas(canvas: Canvas): ValidationResult {
    const errors: ValidationError[] = [];
    
    // Check required fields
    if (!canvas.name) {
      errors.push(new ValidationError(
        'canvas',
        canvas.id,
        'Name is required'
      ));
    }
    
    // Check invariants
    if (canvas.name && canvas.name.length > 100) {
      errors.push(new ValidationError(
        'canvas',
        canvas.id,
        'Name cannot exceed 100 characters'
      ));
    }
    
    return {
      isValid: errors.length === 0,
      errors
    };
  }
  
  // Similar implementations for other entity types
}
```

### Phase 4: Load

The loading phase persists the validated data to the new database:

```typescript
interface DataLoader {
  loadCanvas(canvas: Canvas): Promise<void>;
  loadIdea(idea: Idea): Promise<void>;
  loadVariation(variation: Variation): Promise<void>;
  loadMergedIdea(mergedIdea: MergedIdea): Promise<void>;
  loadComponent(component: Component): Promise<void>;
  loadCollaborator(collaborator: Collaborator): Promise<void>;
}

class RepositoryDataLoader implements DataLoader {
  constructor(
    private readonly canvasRepository: CanvasRepository,
    private readonly ideaRepository: IdeaRepository,
    private readonly variationRepository: VariationRepository,
    private readonly mergedIdeaRepository: MergedIdeaRepository,
    private readonly componentRepository: ComponentRepository,
    private readonly collaboratorRepository: CollaboratorRepository
  ) {}
  
  async loadCanvas(canvas: Canvas): Promise<void> {
    await this.canvasRepository.save(canvas);
  }
  
  // Similar implementations for other entity types
}
```

### Phase 5: Error Handling

Errors that occur during migration are handled through a dedicated error handling system:

```typescript
class MigrationErrorHandler {
  constructor(private readonly errorQueue: ErrorQueue) {}
  
  async handleError(error: MigrationError): Promise<void> {
    // Log the error
    console.error(`Migration error: ${error.message}`, {
      entityType: error.entityType,
      entityId: error.entityId,
      errorType: error.constructor.name,
      rawData: error.rawData
    });
    
    // Add to error queue for manual review
    await this.errorQueue.enqueue({
      entityType: error.entityType,
      entityId: error.entityId,
      error: error.message,
      timestamp: new Date(),
      rawData: error.rawData,
      status: 'pending'
    });
  }
}
```

## 3. Migration Execution Plan

### Pre-Migration Tasks

1. **Create Database Snapshot**: Before starting migration, create a full backup of the current database
2. **Prepare Staging Environment**: Set up a staging environment with both old and new schemas
3. **Develop Migration Scripts**: Implement the migration components described above
4. **Prepare Rollback Plan**: Document step-by-step instructions for rollback if needed

### Migration Execution

The migration will be executed in stages:

1. **Metadata Migration**: First migrate users, canvases, and other metadata
2. **Core Data Migration**: Migrate ideas and their direct components
3. **Derived Data Migration**: Migrate variations and merged ideas
4. **Relationship Migration**: Establish relationships between entities

```typescript
class MigrationOrchestrator {
  constructor(
    private readonly extractor: DataExtractor,
    private readonly transformer: DataTransformer,
    private readonly validator: DataValidator,
    private readonly loader: DataLoader,
    private readonly errorHandler: MigrationErrorHandler
  ) {}
  
  async migrateCanvases(): Promise<MigrationStats> {
    const stats: MigrationStats = {
      total: 0,
      successful: 0,
      failed: 0,
      skipped: 0
    };
    
    try {
      // 1. Extract canvases
      const rawCanvases = await this.extractor.extractCanvases();
      stats.total = rawCanvases.length;
      
      // 2. Process each canvas
      for (const rawCanvas of rawCanvases) {
        try {
          // 3. Transform
          const canvas = this.transformer.transformCanvas(rawCanvas);
          
          // 4. Validate
          const validationResult = this.validator.validateCanvas(canvas);
          
          if (!validationResult.isValid) {
            // Handle validation errors
            for (const error of validationResult.errors) {
              await this.errorHandler.handleError(error);
            }
            stats.failed++;
            continue;
          }
          
          // 5. Load
          await this.loader.loadCanvas(canvas);
          stats.successful++;
        } catch (error) {
          // Handle transformation or loading errors
          await this.errorHandler.handleError(new MigrationError(
            'canvas',
            rawCanvas.id,
            error.message,
            rawCanvas
          ));
          stats.failed++;
        }
      }
    } catch (error) {
      // Handle extraction errors
      console.error(`Failed to migrate canvases: ${error.message}`);
      throw error;
    }
    
    return stats;
  }
  
  // Similar implementations for other entity types
}
```

### Post-Migration Tasks

1. **Validation**: Run validation queries to ensure data integrity
2. **Reconciliation**: Compare record counts and key metrics between old and new systems
3. **Manual Review**: Process error queue items that require manual intervention
4. **Performance Testing**: Verify system performance with migrated data
5. **User Acceptance Testing**: Have key stakeholders verify their data

## 4. Data Mapping

### Canvas Mapping

| Old Field | New Field | Transformation |
|-----------|-----------|----------------|
| id | id | Direct mapping |
| user_id | ownerId | Direct mapping |
| name | name | Direct mapping (default to 'Untitled Canvas' if null) |
| description | description | Direct mapping (default to empty string if null) |
| metadata.tags | tags | Convert to array if string, extract if embedded in JSON |
| created_at | createdAt | Direct mapping |
| updated_at | updatedAt | Direct mapping |

### Idea Mapping

| Old Field | New Field | Transformation |
|-----------|-----------|----------------|
| id | id | Direct mapping |
| canvas_id | canvasId | Direct mapping |
| title | title | Direct mapping (default to 'Untitled Idea' if null) |
| description | description | Direct mapping (default to empty string if null) |
| problem | problemStatement | Direct mapping (default to empty string if null) |
| audience | targetAudience | Direct mapping (default to empty string if null) |
| value_prop | uniqueValue | Direct mapping (default to empty string if null) |
| content | Extracted to components | Parse JSON and extract components |
| created_at | createdAt | Direct mapping |
| updated_at | updatedAt | Direct mapping |

Similar mappings will be created for variations, merged ideas, components, and collaborators.

## 5. Rollback Plan

In case of migration failures, the following rollback procedure will be followed:

1. **Stop Migration**: Immediately halt the migration process
2. **Assess Impact**: Determine the extent of the failure and affected data
3. **Restore Backup**: If severe, restore the database from the pre-migration snapshot
4. **Targeted Fixes**: For minor issues, apply targeted fixes to the migrated data
5. **Retry Migration**: If appropriate, fix the migration script and retry with improved error handling

## 6. Testing Strategy

### Unit Testing

Each component of the migration system will be unit tested:

- Extractors will be tested with mock database responses
- Transformers will be tested with various input data scenarios
- Validators will be tested with valid and invalid data
- Loaders will be tested with mock repositories

### Integration Testing

Integration tests will verify the end-to-end migration process:

1. **Sample Data Test**: Run migration on a small representative sample
2. **Edge Case Test**: Run migration on specially crafted edge cases
3. **Volume Test**: Run migration on a volume similar to production

### Validation Testing

After migration, validation tests will verify:

1. **Record Count Validation**: Compare entity counts between old and new systems
2. **Data Integrity Validation**: Verify key fields and relationships
3. **Business Logic Validation**: Test that business operations work on migrated data

## 7. Timeline and Resource Requirements

| Stage | Duration | Resources |
|-------|----------|-----------|
| Analysis | 1 day | Data Analyst |
| Script Development | 2 days | Backend Developer |
| Testing | 1 day | QA Engineer |
| Execution | 1 day | DevOps Engineer, Backend Developer |
| Validation | 1 day | QA Engineer, Data Analyst |
| Total | 6 days | |

## 8. Risks and Mitigation

| Risk | Impact | Likelihood | Mitigation |
|------|--------|------------|------------|
| Data loss | High | Low | Comprehensive backup and testing |
| Performance issues | Medium | Medium | Batch processing, optimize queries |
| Inconsistent data | Medium | High | Robust validation and error handling |
| Migration failure | High | Low | Detailed rollback plan and monitoring |
| Downtime | Medium | Medium | Schedule migration during off-peak hours |

## 9. Communication Plan

| Stakeholder | Method | Frequency | Content |
|-------------|--------|-----------|---------|
| Development Team | Stand-up meetings | Daily | Technical progress |
| Project Manager | Status reports | Daily | Progress, issues, timeline |
| End Users | Email announcement | Before & after | Downtime notice, feature changes |
| Executive Sponsor | Executive summary | End of migration | Results, issues, next steps |

## 10. Conclusion

This data migration plan provides a comprehensive approach to transferring existing idea playground data to the new architecture. By following this plan, we can ensure a smooth transition with minimal data loss and disruption to users.

================
File: docs/idea-playground/DATABASE_SCHEMA.md
================
# Database Schema and Type Definitions

This document details the database schema for the Idea Playground system with continuous learning and IP protection features.

## Table Structure

### Core Tables

- **ideas** - Base table for storing business ideas (existing table extended)
- **idea_interactions** - User interactions with ideas (ratings, edits, etc.)
- **idea_protection_settings** - IP protection configuration
- **training_sessions** - AI model training sessions 
- **training_ideas** - Ideas used in training sessions
- **idea_implementations** - Business ideas implemented as products/companies

## Supabase Migration Files

### Core Schema Modifications

```sql
-- supabase/migrations/20250318000000_idea_playground_extended.sql

-- First, ensure the UUID extension is available
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";

-- Add new columns to the ideas table for training and protection
ALTER TABLE ideas ADD COLUMN IF NOT EXISTS training_status TEXT;
ALTER TABLE ideas ADD COLUMN IF NOT EXISTS protection_level TEXT DEFAULT 'public';
ALTER TABLE ideas ADD COLUMN IF NOT EXISTS owner_user_id UUID REFERENCES auth.users(id);
ALTER TABLE ideas ADD COLUMN IF NOT EXISTS last_trained_at TIMESTAMP WITH TIME ZONE;
ALTER TABLE ideas ADD COLUMN IF NOT EXISTS embedding VECTOR(1536); -- For similarity search

-- Create a comprehensive interaction tracking table
CREATE TABLE IF NOT EXISTS idea_interactions (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  idea_id UUID REFERENCES ideas(id) ON DELETE CASCADE,
  user_id UUID REFERENCES auth.users(id),
  interaction_type TEXT NOT NULL, -- 'positive_rating', 'negative_rating', 'saved', 'dismissed', 'edited', 'merged', etc.
  timestamp TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  
  -- For ratings
  rating_value INTEGER,
  
  -- For modifications
  previous_version JSONB,
  new_version JSONB,
  
  -- For merges
  merged_with_idea_ids UUID[],
  
  -- For dismissals or explicit feedback
  reason TEXT,
  comment TEXT
);

-- Add indexes for faster queries
CREATE INDEX IF NOT EXISTS idea_interactions_idea_id_idx ON idea_interactions(idea_id);
CREATE INDEX IF NOT EXISTS idea_interactions_user_id_idx ON idea_interactions(user_id);
CREATE INDEX IF NOT EXISTS idea_interactions_type_idx ON idea_interactions(interaction_type);

-- Protection settings table
CREATE TABLE IF NOT EXISTS idea_protection_settings (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  idea_id UUID REFERENCES ideas(id) ON DELETE CASCADE,
  protection_level TEXT NOT NULL DEFAULT 'public',
  updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  
  -- Proprietary metadata
  company_name TEXT,
  implementation_date TIMESTAMP WITH TIME ZONE,
  legal_status TEXT,
  
  -- Access controls
  owner_user_id UUID NOT NULL REFERENCES auth.users(id),
  team_access UUID[] DEFAULT '{}',
  viewer_access UUID[] DEFAULT '{}',
  
  -- Training controls
  exclude_from_training BOOLEAN DEFAULT FALSE,
  exclude_from_similarity_search BOOLEAN DEFAULT FALSE,
  obfuscation_level TEXT DEFAULT 'none',
  
  UNIQUE(idea_id)
);

-- Training data tracking table
CREATE TABLE IF NOT EXISTS training_sessions (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  started_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  completed_at TIMESTAMP WITH TIME ZONE,
  model_version TEXT,
  training_data_count INTEGER,
  validation_data_count INTEGER,
  metrics JSONB,
  status TEXT DEFAULT 'pending'
);

-- Ideas used in training
CREATE TABLE IF NOT EXISTS training_ideas (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  training_session_id UUID REFERENCES training_sessions(id) ON DELETE CASCADE,
  idea_id UUID REFERENCES ideas(id) ON DELETE CASCADE,
  included_as TEXT, -- 'positive', 'negative', 'modified', 'merged'
  obfuscated BOOLEAN DEFAULT FALSE
);

-- Create the implementation tracking table
CREATE TABLE IF NOT EXISTS idea_implementations (
  id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
  idea_id UUID REFERENCES ideas(id) ON DELETE CASCADE,
  user_id UUID REFERENCES auth.users(id),
  company_name TEXT NOT NULL,
  implemented_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
  legal_status TEXT DEFAULT 'pending',
  
  -- Additional details
  implementation_details JSONB,
  business_model TEXT,
  market_segment TEXT,
  funding_status TEXT
);
```

### Row-Level Security Policies

```sql
-- supabase/migrations/20250318000100_idea_playground_security.sql

-- Enable Row Level Security on all tables
ALTER TABLE idea_interactions ENABLE ROW LEVEL SECURITY;
ALTER TABLE idea_protection_settings ENABLE ROW LEVEL SECURITY;
ALTER TABLE training_sessions ENABLE ROW LEVEL SECURITY;
ALTER TABLE training_ideas ENABLE ROW LEVEL SECURITY;
ALTER TABLE idea_implementations ENABLE ROW LEVEL SECURITY;

-- Idea Interactions - Basic policies
CREATE POLICY "Users can see their own interactions"
  ON idea_interactions
  FOR SELECT
  USING (auth.uid() = user_id);
  
CREATE POLICY "Users can create their own interactions"
  ON idea_interactions
  FOR INSERT
  WITH CHECK (auth.uid() = user_id);
  
CREATE POLICY "Users can update their own interactions"
  ON idea_interactions
  FOR UPDATE
  USING (auth.uid() = user_id);

-- Protection Settings - Access control
CREATE POLICY "Owners can view protection settings"
  ON idea_protection_settings
  FOR SELECT
  USING (auth.uid() = owner_user_id);
  
CREATE POLICY "Team members can view protection settings"
  ON idea_protection_settings
  FOR SELECT
  USING (auth.uid() = ANY(team_access));
  
CREATE POLICY "Owners can edit protection settings"
  ON idea_protection_settings
  FOR ALL
  USING (auth.uid() = owner_user_id);

-- Idea Implementations - IP protection
CREATE POLICY "Only owners can view implementations"
  ON idea_implementations
  FOR SELECT
  USING (auth.uid() = user_id);
  
CREATE POLICY "Only owners can create implementations"
  ON idea_implementations
  FOR INSERT
  WITH CHECK (auth.uid() = user_id);

-- Training data access - Admin only
CREATE POLICY "Only admins can view training sessions"
  ON training_sessions
  FOR SELECT
  USING (auth.uid() IN (SELECT user_id FROM user_roles WHERE role = 'admin'));
  
CREATE POLICY "Only admins can manage training sessions"
  ON training_sessions
  FOR ALL
  USING (auth.uid() IN (SELECT user_id FROM user_roles WHERE role = 'admin'));
```

## TypeScript Type Definitions

